<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>10_机器学习_集成学习 | 春和景明的记事本</title><meta name="author" content="Scenery"><meta name="copyright" content="Scenery"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 核心思想集成学习的核心思想可以精炼为一句话：“集思广益，博采众长”。 集成学习的根本目标是，通过将多个学习器（“基学习器”或“专家模型”）的“智慧”结合起来，以获得比任何单个学习器都更强大、更稳定的综合性能。 要成功实现这一目标，需要解决两个核心问题：  如何保证参与集成的专家们“各有所长”？"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://vernalscenery.github.io/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '10_机器学习_集成学习',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-05 14:17:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="春和景明的记事本"><span class="site-name">春和景明的记事本</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">10_机器学习_集成学习</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-26T01:58:53.000Z" title="发表于 2025-07-26 09:58:53">2025-07-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T06:17:15.000Z" title="更新于 2025-08-05 14:17:15">2025-08-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>21分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="10_机器学习_集成学习"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="1-核心思想"><a href="#1-核心思想" class="headerlink" title="1 核心思想"></a>1 核心思想</h2><p>集成学习的核心思想可以精炼为一句话：<strong>“集思广益，博采众长”</strong>。</p>
<p>集成学习的根本目标是，通过将多个学习器（“基学习器”或“专家模型”）的“智慧”结合起来，以获得比任何单个学习器都更强大、更稳定的综合性能。</p>
<p>要成功实现这一目标，需要解决两个核心问题：</p>
<ol>
<li><strong>如何保证参与集成的专家们“各有所长”？</strong> —— 这就是<strong>多样性 (Diversity)</strong> 的问题。</li>
<li><strong>如何将这些专家的意见有效地结合起来？</strong> —— 这就是<strong>集成策略</strong>的问题。</li>
</ol>
<h4 id="1-1-1-核心前提：保证模型的多样性"><a href="#1-1-1-核心前提：保证模型的多样性" class="headerlink" title="1.1.1 核心前提：保证模型的多样性"></a>1.1.1 核心前提：保证模型的多样性</h4><p>要让集成学习有效，关键在于<strong>保证参与集成的各个模型具有差异性</strong>（即拥有<strong>不同的决策边界</strong>）。</p>
<p>创造多样性的主要途径有三种：</p>
<ul>
<li><strong>(1) 数据层面</strong>: 让每个模型学习<strong>不同的数据集</strong>。这是最核心、最常用的方法。</li>
<li><strong>(2) 参数层面</strong>: 使用<strong>不同的超参数</strong>来训练同一种模型。</li>
<li><strong>(3) 模型层面</strong>: 直接选用<strong>完全不同的模型</strong>来组合。</li>
</ul>
<h4 id="1-1-2-核心策略：如何组织专家模型"><a href="#1-1-2-核心策略：如何组织专家模型" class="headerlink" title="1.1.2 核心策略：如何组织专家模型"></a>1.1.2 核心策略：如何组织专家模型</h4><p>根据“专家的构成”和“专家的工作方式”，我们可以将主流的集成策略分为以下几类：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726142553.png" alt="image.png|659"></p>
<p><span style="color: #badc58; font-weight: 550;">维度一：专家的构成 (同质 vs. 异质)</span></p>
<ul>
<li><strong>同质集成 (Homogeneous)</strong>: 所有基学习器都<strong>是同一种算法</strong>（例如，全部是决策树）。这是最常见的情况，如 <strong>Bagging</strong> 和 <strong>Boosting</strong>。</li>
<li><strong>异质集成 (Heterogeneous)</strong>: 基学习器由<strong>不同类型的算法</strong>构成（例如，一个 SVM、一个逻辑回归、一个 K 近邻）。如 <strong>Voting</strong> 和 <strong>Stacking</strong>。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">维度二：专家的工作方式 (并行 vs. 串行 vs. 混联)</span></p>
<p><span style="color: #9c88ff; font-weight: 550;">1、并行方法 (Parallel)</span></p>
<p>降低方差 (Variance)：这种策略的目标是构建一个<strong>稳定且泛化能力强</strong>的集成模型。</p>
<ul>
<li><strong>思想</strong>: 通过将多个<strong>相互独立</strong>的弱学习器（基模型）的预测结果进行平均或投票，可以有效地抵消掉单个模型的随机误差（即方差），从而提高模型的稳定性和泛化能力，有效防止过拟合。</li>
<li><strong>类比</strong>: 这就像一个由多位背景不同的专家组成的<strong>委员会</strong>。单个专家可能会有偏见，但委员会通过集体<strong>民主投票</strong>做出的决策会更加稳健、可靠。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726142955.png" alt="image.png|536"></p>
<p><strong>Bagging</strong> 像是在组建一个<strong>并行的专家委员会</strong>，追求的是群体的<strong>稳定与稳健</strong>（降低方差）。</p>
<p><span style="color: #9c88ff; font-weight: 550;">2、串行方法 (Serial)</span></p>
<p>降低偏差 (Bias)： 这种策略的目标是构建一个<strong>高度精确、拟合能力强</strong>的集成模型。</p>
<ul>
<li><strong>思想</strong>: 采用<strong>串行</strong>的方式，让一系列模型接力学习。每一个新模型都专注于弥补前一个模型的不足和错误，从而不断减少整体的系统性误差。通过不断迭代修正错误来<strong>降低偏差</strong>，将一群“弱学习器”提升为一个“强学习器”。</li>
<li><strong>类比</strong>: 这就像一个学生在不断<strong>订正错题</strong>。他先做一遍作业，然后专门针对做错的题目进行强化练习，反复弥补短板，最终成为学霸。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726143013.png" alt="|695"></p>
<p><strong>Boosting</strong> 则像是在培养一个<strong>串行的精英序列</strong>，追求的是个体的<strong>极致与精准</strong>（降低偏差）。</p>
<p><span style="color: #9c88ff; font-weight: 550;">3、混联/分层方法 (Hybrid/Hierarchical)</span></p>
<p>同时降低偏差与方差：这种策略的目标是找到模型间<strong>最佳的组合方式</strong>，试图同时在偏差和方差上都取得收益。</p>
<ul>
<li><strong>思想</strong>: 它不再采用简单的投票或加权，而是引入一个“<strong>元学习器</strong> (Meta-Learner)”，这个学习器的任务就是专门学习如何最好地结合前面所有基学习器的预测结果。</li>
<li><strong>类比</strong>: 这就像一个项目团队里，既有各个领域的专家（基学习器），还有一个非常聪明的<strong>项目经理</strong>（元学习器）。专家们各自提交自己的方案，而项目经理不亲自解决原始问题，他的工作是分析所有专家的方案，学习“<strong>在什么情况下该听谁的</strong>”，以及如何将这些方案整合起来，形成一份最终的、远超任何单个方案的完美报告。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726145409.png" alt="image.png"></p>
<p><strong>Stacking</strong> 则引入了一个<strong>分层的管理架构</strong>，通过一个“项目经理”来学习如何最智慧地融合专家意见，力求达到<strong>稳定与精准兼得</strong>的最终效果。</p>
<h2 id="2-并行策略"><a href="#2-并行策略" class="headerlink" title="2 并行策略"></a>2 并行策略</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726103225.png" alt="image.png|590"></p>
<p>进行集成学习的关键是参与集成的模型有不同的决策边界，要做到这点无外乎就下面几种办法：</p>
<ul>
<li><strong>(1) 数据层面</strong>: 让每个模型学习<strong>不同的数据集</strong>。这是最核心、最常用的方法。</li>
<li><strong>(2) 参数层面</strong>: 使用<strong>不同的超参数</strong>来训练同一种模型。</li>
<li><strong>(3) 模型层面</strong>: 直接选用<strong>完全不同的模型</strong>来组合。</li>
</ul>
<h3 id="2-1-Bagging-算法"><a href="#2-1-Bagging-算法" class="headerlink" title="2.1 Bagging 算法"></a>2.1 Bagging 算法</h3><p>我们先从数据层面进行考虑， 假设我们从总体中抽样得到如下的数据集：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726150623.png" alt="image.png|543"></p>
<p>有两种数据抽取方式：</p>
<ul>
<li>有放回</li>
<li>无放回</li>
</ul>
<p>我们选择<strong>有放回</strong>的方式，在统计中又叫作<strong>自助法（Bootstrap）</strong>，具体步骤如下：</p>
<ol>
<li>随机从该数据集中抽取 1 个样本，然后放回</li>
<li>将（1）重复 n 次（和原数据集的大小一样），就可以得到 1 个新的数据集；</li>
<li>将（2）重复 T 次（根据你的需要），就可以得到 T 个新的数据集：</li>
</ol>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151048.png" alt="image.png"></p>
<p>将 Bootstrap 用于集成学习，就是 Bagging 算法：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151205.png" alt="image.png"></p>
<p>具体的做法就是在现有数据集的基础上，通过 Bootstrap 生成 T 个数据集，然后分别训练出 T 个模型，最后用一人一票的方式投出集成模型：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151221.png" alt="image.png|520"></p>
<p>每个模型都是<strong>分开</strong>训练的，所以这是<strong>并行</strong>的集成学习算法。</p>
<p><span style="color: #badc58; font-weight: 550;">OOB (out-of-bag)</span></p>
<p>假设数据集中有 n 个点，每一次抽取，某样本被抽到的概率为 $\frac{1}{n}$，而没被抽到的概率就是 $1-\frac{1}{n}$：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151339.png" alt="image.png"></p>
<p>抽了 n 次还没被抽中的概率是 $\left(1-\frac{1}{n}\right)^n$，其极限为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151319.png" alt="image.png"></p>
<p>也就是说从理论上讲，使用 Bootstrap 生成数据集时（也就是训练集），大概会有 $36.8%$ 的数据不会被选中，正好拿来作为测试集。</p>
<p>并且因为会有 $36.8%$ 的数据不会被选中，故通过 Bootstrap 生成数据集时，也可以<strong>过滤一些异常数据</strong>，如下图</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151631.png" alt="image.png|559"></p>
<h3 id="2-2-随机森林"><a href="#2-2-随机森林" class="headerlink" title="2.2 随机森林"></a>2.2 随机森林</h3><p>如果 Bagging 算法都采用决策树的话，最终得到的集成模型就称为<strong>决策森林</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151918.png" alt="image.png|435"></p>
<p> 一般来说，决策森林中的决策树要求计算到最深，也就是要求每个节点尽量只包含一种类别，这种决策树也称为<strong>最纯的决策树</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726152806.png" alt="image.png|496"></p>
<ul>
<li>如果我们在这些相似的数据集上只训练“简单”的决策树（比如限制树的深度），那么它们学到的决策边界将会非常相似。</li>
<li>因此我们会<strong>故意</strong>让每一棵树长到最纯，以此来<strong>最大化它们之间的差异性</strong>；然后我们再<strong>利用</strong>集成的平均/投票机制来<strong>消除</strong>这些复杂树各自的过拟合风险。</li>
</ul>
<p>在决策森林的基础上，<strong>随机选择一些特征</strong>来构造决策树，这样的算法就称为<strong>随机森林</strong>（Random Forest）：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726151937.png" alt="image.png|611"></p>
<p>具体来说，随机森林的构建过程包含两层<strong>随机性</strong>：</p>
<ol>
<li><strong>数据的随机性（来自 Bagging）</strong>: 通过 <strong>Bootstrap</strong> 方法有放回地随机抽样，创建出多个不同的训练数据集。</li>
<li><strong>特征的随机性（RF 的核心改进）</strong>: 在构建每一棵决策树的每一个节点时，并<strong>不是从所有特征</strong>中选择最优的进行分裂，而是先<strong>随机抽取一部分特征</strong>（即“随机选择特征”），再从这个子集中选出最优特征。</li>
</ol>
<p>最终，将所有这样构造出来的决策树组合起来，通过“<strong>一人一票</strong>” （投票）的方式得出最终结果，这个模型就是随机森林。</p>
<p><span style="color: #badc58; font-weight: 550;">随机选择特征带来的三大好处</span></p>
<ol>
<li><strong>保证决策树的多样性</strong>: 这是最主要的好处。即使在数据层面（Bagging）引入了随机性，但如果某些特征特别强大，每棵树可能还是会优先选择这些强特征，导致树的结构趋同。通过限制每次只能从部分特征里选择，随机森林<strong>强制</strong>每棵树使用不同的特征组合，从而确保了树之间的<strong>差异性</strong>，这对于集成学习至关重要。</li>
<li><strong>自动探索特征组合</strong>: 因为特征是随机选择的，模型能够自动地、无偏见地探索各种可能的特征组合，用户<strong>无需再费心去手动分析</strong>应该优先使用哪些特征。</li>
<li><strong>如何选择随机特征的数量 (K)</strong>:<ul>
<li><strong>权衡</strong>: <code>K</code> 的选择是一个权衡过程。<code>K</code> 太小，可能导致单棵树性能太弱；<code>K</code> 太大，树之间的差异性就会减小，失去了随机选择的意义。</li>
<li><strong>经验法则</strong>: 一个常用的启发式建议是，如果总共有 <code>d</code> 个特征，可以尝试设置 $K = \sqrt{d}$。例如，100 个特征中，每次随机选择 10 个。</li>
<li><strong>最终标准</strong>: 最佳的 <code>K</code> 值仍需通过交叉验证等方式根据模型最终的泛化能力来确定。</li>
</ul>
</li>
</ol>
<p><span style="color: #badc58; font-weight: 550;">投票策略</span></p>
<ul>
<li>硬投票 (Hard Voting) - 基于<strong>票数</strong>预测<br>每个分类器预测出一个最终的类别标签（例如“类别 0”或“类别 1”），然后统计所有分类器预测的票数，哪个类别的票数最多，最终结果就是哪个类别。</li>
<li>软投票 (Soft Voting) - 基于<strong>概率</strong>预测<br>它不再看最终的类别标签，而是去平均所有分类器对<strong>每个类别预测的概率</strong>，然后选择平均概率最高的那个类别作为最终结果。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726154932.png" alt="image.png|505"></p>
<p>除了上面两种投票策略，还可以对输出的决策树再进行学习，从而决定每个模型的投票权重。</p>
<p>因为学习分了两层，第一层学习获得决策树，第二层学习获得投票权重，所以该算法称为 <strong>Stacking</strong>：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726155210.png" alt="image.png"></p>
<ul>
<li><strong>工作方式</strong>：它引入了一个<strong>第二层的模型（元学习器）</strong>。这个元学习器的任务不是直接对原始数据进行预测，而是学习如何根据第一层所有基学习器的预测结果，来做出最终的、最准确的决策。</li>
</ul>
<p>Stacking 肯定不光可以和随机森林配合，实际上可以和很多算法进行配合，这里不再赘述。</p>
<p><span style="color: #badc58; font-weight: 550;">总结</span></p>
<p>在实践中，总是可以先试试随机森林算法，这是因为它有不少方便之处：</p>
<ul>
<li>可以不用事先去处理异常数据，因为 Bootstrap 本身就有一定的异常数据过滤的作用</li>
<li>不用特别去挑选特征，因为算法本身就在随机选择特征，各种特征组合都可能出现</li>
<li>因为有投票机制，也不是那么容易过拟合</li>
</ul>
<h3 id="2-3-极端随机树（Extra-Trees）"><a href="#2-3-极端随机树（Extra-Trees）" class="headerlink" title="2.3 极端随机树（Extra-Trees）"></a>2.3 极端随机树（Extra-Trees）</h3><p><strong>Extra-Trees</strong> 通过在节点分裂时引入更强的随机性，并使用整个数据集，通常能以<strong>更快的训练速度</strong>换来一个<strong>方差更低、泛化能力可能更强</strong>的模型。</p>
<p>Extra-Trees 为了将“随机”发挥到极致，在两个关键点上与随机森林不同：</p>
<ul>
<li><strong>节点分裂的策略 (最主要的区别)</strong><ul>
<li><strong>随机森林 (Random Forest)<strong>：在随机选择的特征子集中，会</strong>寻找一个最优分裂点</strong>（例如，让基尼不纯度下降最多的那个点）。这是一个经过计算和优化的选择。</li>
<li><strong>极端随机树 (Extra-Trees)<strong>：它放弃了寻找最优分裂点这个步骤。而是在随机选择的特征子集中，为每个特征</strong>随机地生成一个分裂阈值</strong>，然后从这些随机生成的分裂点中选择一个最好的。这使得它的随机性更强。</li>
</ul>
</li>
<li><strong>数据集的使用方式</strong><ul>
<li>**随机森林 (Random Forest)**：通常使用 <strong>Bootstrap</strong> 对原始数据进行有放回抽样，为每棵树创建不同的训练子集。</li>
<li><strong>极端随机树 (Extra-Trees)<strong>：通常</strong>直接使用全部的原始训练数据</strong>来训练每一棵树，不再进行数据抽样。它完全依靠节点分裂的强随机性来保证树与树之间的差异性。</li>
</ul>
</li>
</ul>
<p>基于以上的区别，Extra-Trees 表现出以下特点：</p>
<ul>
<li><p><strong>优点</strong></p>
<ul>
<li><strong>更快的训练速度</strong>: 因为它省去了计算最优分裂点的步骤（这在计算上非常耗时），所以训练速度通常比随机森林更快。</li>
<li><strong>通常能提供更好的泛化能力（更低的方差）</strong>: 更强的随机性注入使得每棵树的差异变得更大。将这些差异巨大的树集成起来，其方差会进一步降低，从而有效抑制过拟合，模型的泛化能力可能更强。</li>
</ul>
</li>
<li><p><strong>缺点</strong></p>
<ul>
<li><strong>可能会有更高的偏差</strong>: 由于分裂点是随机选择的，而不是最优的，单棵树的性能可能会比随机森林中的单棵树要弱。这在某些情况下可能会导致最终模型的偏差（bias）略微增大。</li>
</ul>
</li>
</ul>
<h2 id="3-串联策略"><a href="#3-串联策略" class="headerlink" title="3 串联策略"></a>3 串联策略</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726104534.png" alt="image.png|607"></p>
<h3 id="3-1-Boosting"><a href="#3-1-Boosting" class="headerlink" title="3.1 Boosting"></a>3.1 Boosting</h3><p>Boosting 的核心是<strong>按顺序、一个接一个地</strong>生成新模型。每个新模型的构建都依赖于前一个模型的结果，因此它们无法并行训练。</p>
<p>整个过程是一个<strong>迭代学习、不断修正错误</strong>的过程。</p>
<ul>
<li>第一个模型做出初步分类，但分错了一些样本。</li>
<li>第二个模型在训练时会<strong>更加关注</strong>第一个模型分错的样本（图中被分错的样本点被放大了），并试图修正这些错误。</li>
<li>第三个模型又会更加关注前两个模型综合起来仍然分错的样本，继续修正</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726160036.png" alt="image.png|706"></p>
<p>每个模型根据其表现（通常是分类的准确率）被赋予一个权重 $a_m$，通过<strong>加权平均</strong>进行组合</p>
<p>权重 $\alpha_m$ 是一个需要<strong>手动调整的超参数</strong>，如果有 T 个模型，那么就有 T 个超参数 $\alpha_m$ 需要设置，这在实践中是非常困难的，因为我们不知道该如何为每个模型设置最优的权重。</p>
<p>这个问题也引出了后续更先进的 Boosting 算法（如 <strong>AdaBoost</strong>），它们的核心改进之一就是设计了一种能够<strong>自动计算</strong>出最优权重 $\alpha_m$ 的方法</p>
<h3 id="3-2-AdaBoost"><a href="#3-2-AdaBoost" class="headerlink" title="3.2 AdaBoost"></a>3.2 AdaBoost</h3><p>AdaBoost 是“自适应提升”的缩写，是 Boosting 家族中最著名、最经典的算法之一。它的“自适应”体现在能够<strong>自动地</strong>学习和调整<strong>样本权重</strong>和<strong>模型权重</strong>。</p>
<p>AdaBoost 的核心思想是，通过一个迭代的过程，每一轮都：</p>
<ol>
<li><strong>提升（Boost）</strong> 那些被前一轮模型错误分类的样本的权重。</li>
<li><strong>降低</strong>那些被正确分类的样本的权重。</li>
<li>根据当前轮模型的表现（错误率），为其**计算一个权重 <code>α</code>**，表现越好的模型权重越大。</li>
</ol>
<p>这样，后续的模型就会更加关注那些“难学的”样本，而最终的预测结果则会综合所有模型的意见，并让“学得好”的模型有更大的话语权。</p>
<p><span style="color: #badc58; font-weight: 550;">一个生动的类比：备考复习</span></p>
<ul>
<li><strong>初始化权重</strong>: 考试前，老师认为所有知识点同等重要。</li>
<li><strong>第一轮模拟考 (模型 1)</strong>: 老师出了一份覆盖所有知识点的卷子让学生做。</li>
<li><strong>分析与调整 (计算α，更新 D)</strong>: 考完后，老师发现“难题”（大部分学生做错的题）和“学霸”（做对大部分题的学生）的价值很高。<ul>
<li><strong>更新样本权重</strong>: 老师把“难题”标记出来，告诉学生下一轮复习要重点关注这些题。</li>
<li><strong>计算模型权重</strong>: 老师认为这次模拟考的区分度很高，能有效识别出学生的薄弱环节，所以给这次考试的成绩赋予了很高的参考权重 <code>α₁</code>。</li>
</ul>
</li>
<li><strong>第二轮模拟考 (模型 2)</strong>: 老师又出了一份卷子，其中“难题”的比重增加了。学生们也在此期间重点复习了错题。</li>
<li><strong>最终成绩</strong>: 学生的最终能力评估，不是简单地把所有模拟考成绩平均，而是把那些更有价值、更能反映真实水平的考试（权重 <code>α</code> 高的）成绩加权后得出的综合评分。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">AdaBoost 的优缺点</span></p>
<ul>
<li><strong>优点</strong>:<ul>
<li>理论基础扎实，不易过拟合（当基模型简单时）。</li>
<li>实现简单，无需做特征筛选。</li>
<li>精度非常高。</li>
</ul>
</li>
<li><strong>缺点</strong>:<ul>
<li>对<strong>异常值（Outliers）和噪声数据</strong>非常敏感，因为算法会不断地提升这些“难啃的骨头”的权重，可能导致模型跑偏。</li>
<li>基模型必须是弱学习器，如果基模型很复杂，容易导致过拟合。</li>
</ul>
</li>
</ul>
<blockquote>
<p> 具体数学推导就不推了，比较复杂</p>
</blockquote>
<h3 id="3-3-Gradient-Boosting"><a href="#3-3-Gradient-Boosting" class="headerlink" title="3.3 Gradient Boosting"></a>3.3 Gradient Boosting</h3><blockquote>
<p> 本质是一种残差拟合的串行算法</p>
</blockquote>
<p>Gradient Boosting 和 AdaBoost 一样，都是一种<strong>串行的、不断迭代</strong>的集成方法。它的核心思想不是去调整样本的权重，而是让每一个新模型都去<strong>拟合前面所有模型预测结果的“残差”（Residuals）</strong>，即“真实值 - 当前预测值”的差值。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250726103749.png" alt="image.png|1048"></p>
<ul>
<li><strong>步骤 1：初始预测</strong>: 首先，模型会有一个初始的、非常简单的预测结果 <code>F₀</code>（通常是所有样本目标值的平均值）。这个初始预测肯定不准，会产生第一轮的残差 <code>r₁ = y - F₀</code>。</li>
<li><strong>步骤 2：训练第一个模型</strong>: 接下来，训练第一个决策树模型 <code>γ₁</code>，但它的目标**不是去预测 <code>y</code><strong>，而是去</strong>拟合第一轮的残差 <code>r₁</code>**。</li>
<li><strong>步骤 3：更新预测，产生新残差</strong>: 将第一个树的预测结果 <code>γ₁</code> 加到初始预测上，得到新的集成预测 <code>F₁ = F₀ + γ₁</code>。这个新预测会比初始预测更准一些，但仍然有误差，于是产生了第二轮的残差 <code>r₂ = y - F₁</code>。</li>
<li><strong>步骤 4：迭代循环</strong>: 不断重复这个过程，下一个模型 <code>γ₂</code> 去拟合残差 <code>r₂</code>，然后更新预测 <code>F₂ = F₁ + γ₂</code>，再产生新残差 <code>r₃</code>… 以此类推。</li>
</ul>
<p>经过多轮迭代后，最终的预测结果是<strong>所有模型的预测值之和</strong>，即： <code>F_n = F₀ + γ₁ + γ₂ + ... + γ_n</code></p>
<p>每一棵新树的加入，都是对整体预测的一次<strong>精细调整和修正</strong>，使得集成模型的预测结果一步步地逼近真实值。</p>
<h2 id="4-代码实现"><a href="#4-代码实现" class="headerlink" title="4 代码实现"></a>4 代码实现</h2><h3 id="4-1-VotingClassifier：异质投票"><a href="#4-1-VotingClassifier：异质投票" class="headerlink" title="4.1 VotingClassifier：异质投票"></a>4.1 VotingClassifier：异质投票</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 从 scikit-learn 中导入所需的模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment"># K近邻分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression   <span class="comment"># 逻辑回归分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB          <span class="comment"># 高斯朴素贝叶斯分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> VotingClassifier     <span class="comment"># 投票集成学习分类器</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建一个包含多个基分类器实例的列表</span></span><br><span class="line"><span class="comment"># 这些是我们将要集成的“专家”模型</span></span><br><span class="line">clf = [  </span><br><span class="line">    KNeighborsClassifier(n_neighbors=<span class="number">3</span>),  <span class="comment"># 初始化一个K值为3的KNN模型</span></span><br><span class="line">    LogisticRegression(),                 <span class="comment"># 初始化一个逻辑回归模型</span></span><br><span class="line">    GaussianNB()                          <span class="comment"># 初始化一个高斯朴素贝叶斯模型</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 创建投票分类器实例</span></span><br><span class="line">vclf = VotingClassifier(  </span><br><span class="line">    <span class="comment"># &#x27;estimators&#x27; 参数接收一个元组列表，每个元组包含(名称, 模型实例)</span></span><br><span class="line">    estimators=[  </span><br><span class="line">        (<span class="string">&#x27;knn&#x27;</span>, clf[<span class="number">0</span>]),  </span><br><span class="line">        (<span class="string">&#x27;lr&#x27;</span>, clf[<span class="number">1</span>]),  </span><br><span class="line">        (<span class="string">&#x27;gnb&#x27;</span>, clf[<span class="number">2</span>])  </span><br><span class="line">    ],  </span><br><span class="line">    <span class="comment"># &#x27;voting&#x27; 参数设置为 &#x27;hard&#x27;，表示采用“硬投票”策略</span></span><br><span class="line">    <span class="comment"># 即少数服从多数，按票数决定最终分类结果</span></span><br><span class="line">    voting=<span class="string">&#x27;hard&#x27;</span>,  </span><br><span class="line">    <span class="comment"># &#x27;n_jobs&#x27; 设置为 -1，表示使用所有可用的CPU核心进行并行计算，加速训练</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>  </span><br><span class="line">)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 在训练集上训练投票集成模型</span></span><br><span class="line">vclf.fit(x_train, y_train)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 在测试集上评估模型的准确率</span></span><br><span class="line">vclf.score(x_test, y_test)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>estimators</code>: 这是一个列表，用来<strong>注册</strong>所有参与投票的“专家”模型。列表中的每一项都是一个元组 <code>(&#39;名称&#39;, 模型实例)</code>，例如 <code>(&#39;knn&#39;, KNeighborsClassifier())</code>。</li>
<li><code>voting</code>: 用来指定投票策略，<code>&#39;hard&#39;</code> 或 <code>&#39;soft&#39;</code>。</li>
<li><code>weights</code>: (可选) 你可以为每个模型手动设置一个投票权重。</li>
<li><code>n_jobs</code>: (可选) 设置为 <code>-1</code> 可以使用所有 CPU 核心来并行训练各个基模型，提高效率。</li>
</ul>
<h3 id="4-2-BaggingClassifier：同质并行"><a href="#4-2-BaggingClassifier：同质并行" class="headerlink" title="4.2 BaggingClassifier：同质并行"></a>4.2 BaggingClassifier：同质并行</h3><p><code>BaggingClassifier</code> 的主要目标是通过组合多个<strong>相同类型</strong>的模型来<strong>降低模型的方差（variance）</strong>，从而提高模型的稳定性和泛化能力，有效防止过拟合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier  </span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line">bagging = BaggingClassifier(  </span><br><span class="line">    <span class="comment"># 指定使用的基学习器，这里我们选择决策树</span></span><br><span class="line">    base_estimator=DecisionTreeClassifier(),  </span><br><span class="line">    <span class="comment"># 指定创建100个基学习器（决策树）实例</span></span><br><span class="line">    n_estimators=<span class="number">100</span>,  </span><br><span class="line">    <span class="comment"># 设置为True，表示使用Bootstrap（自助采样法）来创建训练子集</span></span><br><span class="line">    bootstrap=<span class="literal">True</span>,  </span><br><span class="line">    <span class="comment"># 指定每个自助采样的子集最多包含500个样本</span></span><br><span class="line">    max_samples=<span class="number">500</span>,  </span><br><span class="line">    <span class="comment"># 设置为-1，表示使用所有可用的CPU核心进行并行训练，以加速</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>,  </span><br><span class="line">    <span class="comment"># 设置随机种子，以保证每次运行代码时的随机结果（如抽样）都一致，便于复现</span></span><br><span class="line">    random_state=<span class="number">20</span>  </span><br><span class="line">)</span><br><span class="line">bagging.fit(x_train, y_train)  </span><br><span class="line">bagging.score(x_test, y_test)</span><br></pre></td></tr></table></figure>

<p><span style="color: #badc58; font-weight: 550;">OOB</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 创建 Bagging 分类器实例，并配置更复杂的参数</span></span><br><span class="line">bagging = BaggingClassifier(</span><br><span class="line">    base_estimator=DecisionTreeClassifier(),</span><br><span class="line">    n_estimators=<span class="number">100</span>,</span><br><span class="line">    bootstrap=<span class="literal">True</span>,</span><br><span class="line">    max_samples=<span class="number">500</span>,</span><br><span class="line">    <span class="comment"># 设置为True，表示使用“袋外样本”(Out-of-Bag)来评估模型性能</span></span><br><span class="line">    <span class="comment"># 袋外样本是在自助采样中未被抽中的数据，可作为天然的验证集</span></span><br><span class="line">    oob_score=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 设置为True，表示对特征也进行随机抽样</span></span><br><span class="line">    <span class="comment"># 这是随机森林中“特征随机性”的核心思想</span></span><br><span class="line">    bootstrap_features=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># 与&#x27;bootstrap_features&#x27;配合使用，表示每次抽样只选择1个特征</span></span><br><span class="line">    max_features=<span class="number">1</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span>,</span><br><span class="line">    random_state=<span class="number">20</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 在整个数据集x和y上训练模型</span></span><br><span class="line">bagging.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 查看袋外(Out-of-Bag)评估分数</span></span><br><span class="line"><span class="comment"># 这是一个在训练过程中自动计算出的模型性能指标，无需使用独立的测试集</span></span><br><span class="line">bagging.oob_score_</span><br></pre></td></tr></table></figure>

<ul>
<li><code>base_estimator</code>: 指定要使用的基模型实例，例如 <code>DecisionTreeClassifier()</code>。如果不指定，默认就是决策树。</li>
<li><code>n_estimators</code>: 指定要创建的基模型的数量，即“克隆军团”的大小（例如，100）。</li>
<li><code>bootstrap</code>: 布尔值，<code>True</code>（默认）表示使用自助采样法。</li>
<li><code>oob_score</code>: 布尔值，是否使用“袋外样本 (Out-of-Bag)”来评估模型性能。袋外样本就是那些在自助采样中从未被抽中的数据，可以作为天然的验证集，非常有用。</li>
<li><code>n_jobs</code>: 设置为 <code>-1</code> 可以并行训练所有基模型，大大提高效率。</li>
</ul>
<h3 id="4-3-RandomForestClassifier、ExtraTreesClassifier"><a href="#4-3-RandomForestClassifier、ExtraTreesClassifier" class="headerlink" title="4.3 RandomForestClassifier、ExtraTreesClassifier"></a>4.3 RandomForestClassifier、ExtraTreesClassifier</h3><p><span style="color: #badc58; font-weight: 550;">RandomForestClassifier</span></p>
<p>随机森林可以被看作是 <code>BaggingClassifier</code> 的一个<strong>升级和特化版</strong>。它在 Bagging 的基础上，又增加了一层随机性，这也是它“随机”之名的由来。</p>
<ol>
<li><strong>行抽样（数据的随机性）</strong>: 与 <code>BaggingClassifier</code> 完全一样，随机森林通过**自助采样法 (Bootstrap)**，为每一棵要训练的决策树创建一个略有不同的训练数据子集。这保证了每棵树的“成长环境”不同。</li>
<li><strong>列抽样（特征的随机性）</strong>: 这是随机森林与普通 Bagging<strong>最核心的区别</strong>。在构建每一棵决策树的过程中，当需要在某个节点进行分裂时，它并<strong>不是从所有特征中寻找最优分裂点</strong>，而是先<strong>随机选择一部分特征</strong>，然后再从这个特征子集中找到最好的分裂点。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 从 scikit-learn 中导入随机森林分类器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建随机森林分类器实例</span></span><br><span class="line">rf_clf = RandomForestClassifier(</span><br><span class="line">    <span class="comment"># n_estimators: 指定森林中决策树的数量，这里是100棵</span></span><br><span class="line">    n_estimators=<span class="number">100</span>,</span><br><span class="line">    <span class="comment"># max_samples: 指定每棵树训练时，通过Bootstrap采样的最大样本数</span></span><br><span class="line">    <span class="comment"># 这里每棵树最多从原始数据中有放回地抽取500个样本进行训练</span></span><br><span class="line">    max_samples=<span class="number">500</span>,</span><br><span class="line">    <span class="comment"># max_leaf_nodes: 每棵决策树的最大叶子节点数</span></span><br><span class="line">    <span class="comment"># 这是一种控制单棵树复杂度、防止过拟合的剪枝方法</span></span><br><span class="line">    max_leaf_nodes=<span class="number">16</span>,</span><br><span class="line">    <span class="comment"># oob_score: 设置为True，表示使用“袋外样本”(Out-of-Bag)来评估模型性能</span></span><br><span class="line">    <span class="comment"># 这是一个非常有用的内置验证功能</span></span><br><span class="line">    oob_score=<span class="literal">True</span>,</span><br><span class="line">    <span class="comment"># n_jobs: 设置为-1，使用所有CPU核心并行训练这100棵树，以加速计算</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>,</span><br><span class="line">    <span class="comment"># random_state: 设置随机种子，保证每次运行代码的结果一致，便于复现</span></span><br><span class="line">    random_state=<span class="number">20</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 在数据集x和y上训练随机森林模型</span></span><br><span class="line">rf_clf.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 查看在训练过程中计算出的袋外(Out-of-Bag)评估分数</span></span><br><span class="line"><span class="comment"># oob_score_ 属性存储了模型的泛化能力估计值，无需使用单独的测试集</span></span><br><span class="line">rf_clf.oob_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 提取特征的重要性</span></span><br><span class="line"><span class="comment"># 这是一个在模型训练完成后才可用的属性。</span></span><br><span class="line"><span class="comment"># 它返回一个数组，其中每个值代表对应特征对模型预测的贡献度（重要性分数）。</span></span><br><span class="line"><span class="comment"># 分数是根据该特征在所有树中平均降低不纯度的能力来计算的，所有分数之和为1。</span></span><br><span class="line">rf_clf.feature_importances_</span><br></pre></td></tr></table></figure>

<ul>
<li><code>n_estimators</code>: 森林中树的数量。</li>
<li><code>criterion</code>: 衡量分裂质量的指标，可以是 <code>&#39;gini&#39;</code>（基尼不纯度）或 <code>&#39;entropy&#39;</code>（信息增益）。</li>
<li><code>max_depth</code>: 树的最大深度，用于控制树的复杂度。</li>
<li><code>max_features</code>: 在节点分裂时，随机选择的特征子集的大小。这是控制“特征随机性”强度的关键参数。</li>
<li><code>bootstrap</code>: 是否启用自助采样法（行抽样）。默认为 <code>True</code>。</li>
<li><code>oob_score</code>: 是否使用袋外样本进行评估。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">ExtraTreesClassifier</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> ExtraTreesClassifier</span><br><span class="line">et_clf = ExtraTreesClassifier(</span><br><span class="line">    n_estimators=<span class="number">100</span>, </span><br><span class="line">    max_samples=<span class="number">500</span>,</span><br><span class="line">    bootstrap=<span class="literal">True</span>, </span><br><span class="line">    oob_score=<span class="literal">True</span>, </span><br><span class="line">    criterion=<span class="string">&#x27;gini&#x27;</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span>,random_state=<span class="number">20</span></span><br><span class="line">)</span><br><span class="line">et_clf.fit(x,y)</span><br><span class="line">et_clf.oob_score_</span><br></pre></td></tr></table></figure>

<ul>
<li><code>criterion</code>: 衡量分裂质量的指标（’gini’ 或 ‘entropy’）。</li>
</ul>
<table>
<thead>
<tr>
<th>特性</th>
<th>RandomForestClassifier</th>
<th>ExtraTreesClassifier (极端随机树)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>节点分裂策略</strong></td>
<td><strong>寻找最优分裂点</strong>：在随机选择的特征子集中，会<strong>计算并找到一个最优的分割阈值</strong>（如让基尼不纯度下降最多的那个点）。</td>
<td><strong>随机选择分裂点</strong>：在随机选择的特征子集中，它<strong>不会去计算最优值</strong>，而是为每个特征<strong>随机生成一个分割阈值</strong>，然后从这些随机点中选一个最好的。</td>
</tr>
<tr>
<td><strong>数据使用方式</strong></td>
<td>**使用自助采样 (Bootstrap)**：通常对原始数据进行有放回的抽样来为每棵树创建训练集。</td>
<td><strong>使用全部原始数据</strong>：传统上，它直接使用<strong>全部的训练数据</strong>来训练每一棵树，不再进行数据采样。（注：<code>scikit-learn</code> 的实现很灵活，也允许设置 <code>bootstrap=True</code>）。</td>
</tr>
</tbody></table>
<h3 id="4-4-AdaBoostClassifier、GradientBoostingClassifier"><a href="#4-4-AdaBoostClassifier、GradientBoostingClassifier" class="headerlink" title="4.4 AdaBoostClassifier、GradientBoostingClassifier"></a>4.4 AdaBoostClassifier、GradientBoostingClassifier</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier  </span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier  </span><br><span class="line">ada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_leaf_nodes=<span class="number">16</span>), n_estimators=<span class="number">100</span>)  </span><br><span class="line">ada_clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier  </span><br><span class="line">gb_clf = GradientBoostingClassifier(n_estimators=<span class="number">100</span>)  </span><br><span class="line">gb_clf.fit(x_train,y_train)</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://VernalScenery.github.io">Scenery</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://vernalscenery.github.io/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/">https://vernalscenery.github.io/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://VernalScenery.github.io" target="_blank">春和景明的记事本</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/./img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">12_机器学习_降维</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="09_机器学习_朴素贝叶斯"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">09_机器学习_朴素贝叶斯</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/" title="00_机器学习_KNN算法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-04</div><div class="title">00_机器学习_KNN算法</div></div></a></div><div><a href="/2025/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E8%A7%88%E3%80%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="00_机器学习_概览、环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-03</div><div class="title">00_机器学习_概览、环境搭建</div></div></a></div><div><a href="/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="02_机器学习_线性回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="title">02_机器学习_线性回归</div></div></a></div><div><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">01_机器学习_感知机</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="04_机器学习_基础概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">04_机器学习_基础概念</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="03_机器学习_逻辑回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">03_机器学习_逻辑回归</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Scenery</div><div class="author-info__description">今天不想跑，所以才去跑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chjm0121" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1595718686@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">1 核心思想</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-1-%E6%A0%B8%E5%BF%83%E5%89%8D%E6%8F%90%EF%BC%9A%E4%BF%9D%E8%AF%81%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="toc-text">1.1.1 核心前提：保证模型的多样性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-2-%E6%A0%B8%E5%BF%83%E7%AD%96%E7%95%A5%EF%BC%9A%E5%A6%82%E4%BD%95%E7%BB%84%E7%BB%87%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B"><span class="toc-text">1.1.2 核心策略：如何组织专家模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%B9%B6%E8%A1%8C%E7%AD%96%E7%95%A5"><span class="toc-text">2 并行策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Bagging-%E7%AE%97%E6%B3%95"><span class="toc-text">2.1 Bagging 算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97"><span class="toc-text">2.2 随机森林</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%9E%81%E7%AB%AF%E9%9A%8F%E6%9C%BA%E6%A0%91%EF%BC%88Extra-Trees%EF%BC%89"><span class="toc-text">2.3 极端随机树（Extra-Trees）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%B8%B2%E8%81%94%E7%AD%96%E7%95%A5"><span class="toc-text">3 串联策略</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Boosting"><span class="toc-text">3.1 Boosting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-AdaBoost"><span class="toc-text">3.2 AdaBoost</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Gradient-Boosting"><span class="toc-text">3.3 Gradient Boosting</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">4 代码实现</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-VotingClassifier%EF%BC%9A%E5%BC%82%E8%B4%A8%E6%8A%95%E7%A5%A8"><span class="toc-text">4.1 VotingClassifier：异质投票</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-BaggingClassifier%EF%BC%9A%E5%90%8C%E8%B4%A8%E5%B9%B6%E8%A1%8C"><span class="toc-text">4.2 BaggingClassifier：同质并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-RandomForestClassifier%E3%80%81ExtraTreesClassifier"><span class="toc-text">4.3 RandomForestClassifier、ExtraTreesClassifier</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-AdaBoostClassifier%E3%80%81GradientBoostingClassifier"><span class="toc-text">4.4 AdaBoostClassifier、GradientBoostingClassifier</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/05/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/02_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_Numpy/" title="02_数据分析_Numpy">02_数据分析_Numpy</a><time datetime="2025-08-04T21:11:38.000Z" title="发表于 2025-08-05 05:11:38">2025-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/05/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/01_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_python%E8%AF%AD%E6%B3%95/" title="01_数据分析_python语法">01_数据分析_python语法</a><time datetime="2025-08-04T18:44:45.000Z" title="发表于 2025-08-05 02:44:45">2025-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/13_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" title="13_机器学习_概率图模型">13_机器学习_概率图模型</a><time datetime="2025-07-29T00:29:02.000Z" title="发表于 2025-07-29 08:29:02">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/11_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%81%9A%E7%B1%BB/" title="11_机器学习_聚类">11_机器学习_聚类</a><time datetime="2025-07-28T00:19:53.000Z" title="发表于 2025-07-28 08:19:53">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维">12_机器学习_降维</a><time datetime="2025-07-27T20:48:42.000Z" title="发表于 2025-07-28 04:48:42">2025-07-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./img/1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Scenery</div><div class="footer_custom_text"><div>波澜不惊</div><div class="footer-div"><img class="footer-icon" src="./img/备案图标.png"><a class="footer-a" target="_blank" rel="noopener" href="http://beian.miit.gov.cn/">皖ICP备2021016944号-1</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>