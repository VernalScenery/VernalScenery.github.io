<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>03_机器学习_逻辑回归 | 春和景明的记事本</title><meta name="author" content="Scenery"><meta name="copyright" content="Scenery"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="线性逻辑回归1 引入  名为“回归”，实为“分类”  感知机和逻辑回归都是线性二分类，对于数据集 D，两者都会给出决策边界： 感知机认为决策边界就是类别的分界线，下面这个负类点 ${\times}$ 因为在决策边界正类的这一侧，所以确实就是分错了：   非黑即白  逻辑回归则将超平面映射为了概率，具"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://vernalscenery.github.io/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '03_机器学习_逻辑回归',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-05 14:17:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="春和景明的记事本"><span class="site-name">春和景明的记事本</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">03_机器学习_逻辑回归</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-22T00:24:11.000Z" title="发表于 2025-07-22 08:24:11">2025-07-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T06:17:15.000Z" title="更新于 2025-08-05 14:17:15">2025-08-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>26分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="03_机器学习_逻辑回归"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="线性逻辑回归"><a href="#线性逻辑回归" class="headerlink" title="线性逻辑回归"></a>线性逻辑回归</h1><h2 id="1-引入"><a href="#1-引入" class="headerlink" title="1 引入"></a>1 引入</h2><blockquote>
<p> 名为“回归”，实为“分类”</p>
</blockquote>
<p>感知机和逻辑回归都是<strong>线性二分类</strong>，对于数据集 D，两者都会给出决策边界：</p>
<p><strong>感知机</strong>认为决策边界就是类别的分界线，下面这个负类点 ${\times}$ 因为在决策边界正类的这一侧，所以确实就是分错了：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722083324.png" alt="image.png"></p>
<blockquote>
<p>非黑即白</p>
</blockquote>
<p><strong>逻辑回归</strong>则将超平面映射为了概率，具体描述如下：</p>
<ul>
<li>决策边界，即超平面 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=0$，其上的点发放信用卡的概率为 50 %</li>
<li>在决策边界正类这一侧，与决策边界平行的超平面，比如 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=1$ 上的点发放信用卡的概率会大于 50 %；更远的超平面 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=2$ 上的点发放信用卡的概率更大</li>
<li>而在决策边界负类这一侧，比如 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=-1$、$\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=-2$ 附近的点发放信用卡的概率是低于 50 % 的</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722083506.png" alt="image.png"></p>
<p>总的来说他就是一个概率分布</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722083708.png" alt="image.png"></p>
<h2 id="2-Sigmoid-函数"><a href="#2-Sigmoid-函数" class="headerlink" title="2 Sigmoid 函数"></a>2 Sigmoid 函数</h2><p>因此根据逻辑回归的思路，我们需要将某超平面 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=a$ 映射为概率，这就得引入一个新的函数。</p>
<p>由下列公式定义的函数称为 <strong>Sigmoid</strong> 函数：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722084018.png" alt="image.png"></p>
<p>该函数图像如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722084028.png" alt="image.png"></p>
<p>令 $z=\boldsymbol{w}^\mathrm{T}\boldsymbol{x}$，将之作为自变量代入 Sigmoid 函数，实际上就是将该函数换元了，横坐标变为了 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}$：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722084059.png" alt="image.png"></p>
<p>换元后的函数其实就是将某超平面 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=a$ 映射为概率。比如：</p>
<ul>
<li>逻辑回归的决策边界为超平面 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=0$，有 $S(\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=0)=0.5$，即该超平面（也就是决策边界）上的点发卡的概率为 0.5；</li>
<li>$\boldsymbol{w}^\mathrm{T}\boldsymbol{x}\approx 0.4$，它为决策边界正类一侧的超平面，其上的点的发卡概率为 0.6</li>
<li>而 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}\approx -0.4$，它为决策边界负类一侧的超平面，其上的点的发卡概率就为 0.4</li>
</ul>
<p>用图表示即为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722084201.png" alt="image.png"></p>
<p>所以，在 <strong>Sigmoid</strong> 函数的帮助下，实现了逻辑回归的思路。</p>
<h2 id="3-损失函数"><a href="#3-损失函数" class="headerlink" title="3 损失函数"></a>3 损失函数</h2><h3 id="3-1-从模型的核心假设开始（概率视角）"><a href="#3-1-从模型的核心假设开始（概率视角）" class="headerlink" title="3.1 从模型的核心假设开始（概率视角）"></a>3.1 从模型的核心假设开始（概率视角）</h3><p>我们首先要明确逻辑回归的目标：它不是简单地输出 -1 或 +1，而是要<strong>预测一个概率</strong>。具体来说，它要建模的是**条件概率 <code>P(y|x)</code>**，即在给定输入特征 <code>x</code> 的情况下，类别 <code>y</code> 发生的概率。逻辑回归的核心，就是直接去建模和学习 $P(Y|X)$ 这个条件概率分布。</p>
<blockquote>
<p> 条件分布描述的是，在“某个事件 B 已经发生”这个前提条件下，另一个事件 A 发生的概率分布。<br> 如果我们已经<strong>知道</strong> B 是真的，那么 A 发生的可能性有多大</p>
</blockquote>
<p><strong>预测为正类的概率</strong>：我们用 Sigmoid 函数来将线性得分 <code>wᵀx</code> 转换为一个 0 到 1 之间的概率值，我们把它记为 <code>p̂</code>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722092604.png" alt="image.png"></p>
<p>该条件分布 $P(y=1|x,w)$ 的图像如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722090100.png" alt="image.png"></p>
<p><strong>预测为负类的概率</strong>：既然只有两种可能，那么负类的概率就是：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722092609.png" alt="image.png"></p>
<p>该条件分布 $P(y=0|x,w)$ 的图像如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722090328.png" alt="image.png"></p>
<h3 id="3-2-用一个统一的公式来表示概率"><a href="#3-2-用一个统一的公式来表示概率" class="headerlink" title="3.2 用一个统一的公式来表示概率"></a>3.2 用一个统一的公式来表示概率</h3><p>为了数学上的方便，我们可以用一个非常巧妙的公式，把上面两种情况合并成一个表达式。对于单个样本 <code>(xᵢ, yᵢ)</code>，其预测正确的概率可以写作：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722092943.png" alt="image.png"></p>
<p>我们来验证一下这个公式：</p>
<ul>
<li><strong>如果真实标签 <code>yᵢ = 1</code></strong>: 公式变为 <code>(p̂ᵢ)¹ * (1-p̂ᵢ)⁰ = p̂ᵢ</code>，正确。</li>
<li><strong>如果真实标签 <code>yᵢ = 0</code></strong>: 公式变为 <code>(p̂ᵢ)⁰ * (1-p̂ᵢ)¹ = 1-p̂ᵢ</code>，正确。</li>
</ul>
<p>这个统一的公式就是我们模型对于单个样本的<strong>似然（Likelihood）</strong>。</p>
<h3 id="3-3-引入核心思想——最大似然估计-MLE"><a href="#3-3-引入核心思想——最大似然估计-MLE" class="headerlink" title="3.3 引入核心思想——最大似然估计 (MLE)"></a>3.3 引入核心思想——最大似然估计 (MLE)</h3><p>现在我们的目标是：<strong>找到一组最优的参数 <code>w</code>，使得我们整个训练数据集出现的总概率最大。</strong></p>
<p>换句话说，我们要找到一个 <code>w</code>，它所定义的模型，能够让当前的训练样本（这些标签 <code>y</code>）看起来是“最可能发生”的。这就是<strong>最大似然估计</strong>的核心思想。</p>
<p>假设我们的 <code>m</code> 个训练样本是相互独立的，那么整个数据集的<strong>总似然函数 <code>L(w)</code></strong> 就是所有单个样本似然的<strong>连乘</strong>：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722093430.png" alt="image.png"></p>
<h3 id="3-4-简化问题——取对数"><a href="#3-4-简化问题——取对数" class="headerlink" title="3.4 简化问题——取对数"></a>3.4 简化问题——取对数</h3><p>直接对上面这个有很多项连乘的 <code>L(w)</code> 函数求最大值非常困难（尤其是求导时）。一个标准的数学技巧是<strong>取对数</strong>，将<strong>连乘转换为连加</strong>。</p>
<p>最大化 <code>L(w)</code> 和最大化 <code>log(L(w))</code> 的结果是等价的。我们把 <code>log(L(w))</code> 称为**对数似然函数 (Log-Likelihood)**。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722093511.png" alt="image.png"></p>
<p>根据对数运算法则 <code>log(ab) = log(a) + log(b)</code> 和 <code>log(aᵇ) = b*log(a)</code>，上式可以变为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722093521.png" alt="image.png"></p>
<p>现在，我们的目标是<strong>最大化</strong>这个对数似然函数。</p>
<h3 id="3-5-转为损失函数——取负号并平均"><a href="#3-5-转为损失函数——取负号并平均" class="headerlink" title="3.5 转为损失函数——取负号并平均"></a>3.5 转为损失函数——取负号并平均</h3><p>在机器学习中，我们习惯于<strong>最小化</strong>一个<strong>损失函数</strong>，而不是最大化一个似然函数。</p>
<p>这两个是等价的：<strong>最大化 <code>log(L(w))</code></strong> 就等同于 **最小化 <code>-log(L(w))</code>**。</p>
<p>因此，我们定义逻辑回归的损失函数 <code>J(w)</code> 为**平均负对数似然 (Average Negative Log-Likelihood)**：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722093551.png" alt="image.png"></p>
<p>至此，我们就完整地推导出了逻辑回归的损失函数——<strong>二元交叉熵损失函数</strong>。</p>
<h3 id="3-6-防止过拟合：正则化-惩罚项"><a href="#3-6-防止过拟合：正则化-惩罚项" class="headerlink" title="3.6 防止过拟合：正则化 (惩罚项)"></a>3.6 防止过拟合：正则化 (惩罚项)</h3><p>我们之前讨论的损失函数主要是衡量“模型预测值与真实值之间的差距”，通过找到最小差距，让模型尽可能地<strong>拟合训练数据</strong>。</p>
<p>但是如果<strong>一味的去追求拟合训练数据</strong>，容易导致<strong>过拟合</strong></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718160844.png" alt="image.png"></p>
<p><strong>正则化 (Regularization)</strong> 是机器学习中一个极其重要和强大的核心概念。简单来说，它的主要目的是**防止模型过拟合 (Overfitting)**。</p>
<ul>
<li><strong>核心思想</strong>：在模型试图最小化训练误差的同时，我们给它增加一个“<strong>惩罚项</strong>”，这个惩罚项会惩罚那些试图让系数变得过大的模型。</li>
</ul>
<p>这个<strong>惩罚项 (Penalty Term)</strong> 的大小与模型系数的大小正相关。现在，模型在训练时就陷入了一个“两难”的境地：</p>
<ul>
<li>一方面，它想把<strong>训练误差</strong>降到最低，这可能会驱使它去学习复杂的模型，产生很大的系数。</li>
<li>另一方面，它又想把<strong>惩罚项</strong>降到最低，这会驱使它让自己的系数尽可能地小，保持简单。</li>
</ul>
<p>正则化是一种通过向损失函数中添加“模型复杂度惩罚项”来防止过拟合的技术。它强迫模型在“拟合数据”和“保持简单”之间做出权衡，从而提升模型在未知数据上的表现（即泛化能力）。</p>
<h3 id="3-7-损失函数的求解：梯度下降"><a href="#3-7-损失函数的求解：梯度下降" class="headerlink" title="3.7 损失函数的求解：梯度下降"></a>3.7 损失函数的求解：梯度下降</h3><p>梯度下降法（Gradient Descent）的思路就像在山顶放了一个球，一松手它就会顺着山坡最陡峭的地方滚落到谷底：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722094533.png" alt="image.png"></p>
<p>将逻辑回归的损失函数看作这座山，就可以用滚球的方式找到最小值。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722094647.png" alt="image.png"></p>
<p>按照上述步骤 ，根据收敛的证明，只要 $f(\boldsymbol{x})$ 符合条件，那么选择合适的学习率 $\eta$ 和迭代上限 epochs ，最终会有：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722094758.png" alt="image.png"></p>
<p>也就是 $f(\boldsymbol{x_k})$ 为凸函数 f(x) 的最小值（或附近）。</p>
<p><span style="color: #badc58; font-weight: 550;">注意：</span>学习率是一个<strong>极其敏感且重要</strong>的超参数，<strong>太小则收敛慢，太大则不稳定甚至不收敛</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721150933.png" alt="image.png"></p>
<h2 id="4-二分类问题"><a href="#4-二分类问题" class="headerlink" title="4 二分类问题"></a>4 二分类问题</h2><p>我们来看看感知机、线性回归以及逻辑回归的假设空间：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722090728.png" alt="image.png"></p>
<p>感知机的 $h(\boldsymbol{x})$ 输出的是某点 $\boldsymbol{x}$ 的<strong>分类</strong>，即输出为 +1 或 -1，该输出是离散的，所以称为 分类。而线性回归和逻辑回归的 $h(\boldsymbol{x})$ 输出是连续的，因此都称为 回归。</p>
<p>那为什么说逻辑回归是分类算法呢？是这样的，如果以 $h(\boldsymbol{x})=0.5$ 作为分类线，那么有：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722091357.png" alt="image.png"></p>
<p>比如有两个点 $\boldsymbol{x_1}$ 和 $\boldsymbol{x_2}$，</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722090955.png" alt="image.png"></p>
<p>那么我们就说 $\boldsymbol{x_1}$ 是正类 ${\circ}$，$\boldsymbol{x_2}$ 是负类 ${\times}$</p>
<p>所以将逻辑回归用于分类时，其决策边界为超平面 $\boldsymbol{w}^\mathrm{T}\boldsymbol{x}=0$，可以将点分为正类 ${\circ}$ 和负类 ${\times}$，因此它是 线性二分类。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库和模块</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 数据准备 ---</span></span><br><span class="line"><span class="comment"># 生成一个用于二元分类的合成数据集</span></span><br><span class="line"><span class="comment"># n_samples: 样本数量; n_features: 特征数量; n_classes: 类别数量</span></span><br><span class="line"><span class="comment"># n_clusters_per_class: 每个类别的簇数; random_state: 随机种子，保证结果可复现</span></span><br><span class="line">x, y = make_classification(</span><br><span class="line">    n_samples=<span class="number">200</span>,</span><br><span class="line">    n_features=<span class="number">2</span>,</span><br><span class="line">    n_redundant=<span class="number">0</span>,</span><br><span class="line">    n_classes=<span class="number">2</span>,</span><br><span class="line">    n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">    random_state=<span class="number">1024</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集分割为训练集和测试集 (70%训练, 30%测试)</span></span><br><span class="line"><span class="comment"># stratify=y 表示分层抽样，确保训练集和测试集中类别比例与原始数据相同</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">233</span>, stratify=y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 模型训练与评估 ---</span></span><br><span class="line"><span class="comment"># 创建一个逻辑回归分类器实例</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练数据来训练(拟合)模型</span></span><br><span class="line">clf.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型在训练集上的准确率 (accuracy)</span></span><br><span class="line">clf.score(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型在测试集上的准确率，以评估其泛化能力</span></span><br><span class="line">clf.score(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 模型预测 ---</span></span><br><span class="line"><span class="comment"># 对测试集进行预测，直接输出预测的类别 (0或1)</span></span><br><span class="line">clf.predict(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行概率预测，输出一个n x 2的数组</span></span><br><span class="line"><span class="comment"># 每一行代表一个样本，两列分别表示该样本属于类别0和类别1的概率</span></span><br><span class="line">clf.predict_proba(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找出每个样本预测概率最高的类别的索引</span></span><br><span class="line"><span class="comment"># axis=1表示按行操作。这行代码的结果与 clf.predict(x_test) 相同</span></span><br><span class="line">np.argmax(clf.predict_proba(x_test), axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>通过<strong>网格搜索交叉验证</strong>（Grid Search Cross-Validation），自动寻找模型的<strong>最佳超参数组合</strong>。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入用于超参数优化的网格搜索交叉验证工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义要搜索的超参数网格。这是一个字典列表，每个字典代表一组要尝试的参数组合。</span></span><br><span class="line">params = [&#123;</span><br><span class="line">    <span class="comment"># 第一组：使用&#x27;liblinear&#x27;求解器，它可以同时处理L1和L2正则化</span></span><br><span class="line">    <span class="string">&#x27;penalty&#x27;</span>: [<span class="string">&#x27;l2&#x27;</span>, <span class="string">&#x27;l1&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>], <span class="comment"># C是正则化强度的倒数，值越小正则化越强</span></span><br><span class="line">    <span class="string">&#x27;solver&#x27;</span>: [<span class="string">&#x27;liblinear&#x27;</span>]</span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="comment"># 第二组：使用&#x27;lbfgs&#x27;求解器，测试不使用正则化的情况</span></span><br><span class="line">    <span class="string">&#x27;penalty&#x27;</span>: [<span class="string">&#x27;none&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>],</span><br><span class="line">    <span class="string">&#x27;solver&#x27;</span>: [<span class="string">&#x27;lbfgs&#x27;</span>]</span><br><span class="line">&#125;, &#123;</span><br><span class="line">    <span class="comment"># 第三组：使用&#x27;saga&#x27;求解器，测试弹性网络(ElasticNet)正则化</span></span><br><span class="line">    <span class="string">&#x27;penalty&#x27;</span>: [<span class="string">&#x27;elasticnet&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;C&#x27;</span>: [<span class="number">0.0001</span>, <span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>],</span><br><span class="line">    <span class="string">&#x27;l1_ratio&#x27;</span>: [<span class="number">0</span>, <span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>, <span class="number">1</span>], <span class="comment"># l1_ratio控制L1和L2正则化的混合比例</span></span><br><span class="line">    <span class="string">&#x27;solver&#x27;</span>: [<span class="string">&#x27;saga&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;max_iter&#x27;</span>: [<span class="number">200</span>] <span class="comment"># 增加迭代次数以保证收敛</span></span><br><span class="line">&#125;]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建GridSearchCV实例</span></span><br><span class="line">grid = GridSearchCV(</span><br><span class="line">    estimator=LogisticRegression(), <span class="comment"># 1. 要进行超参数优化的基础模型</span></span><br><span class="line">    param_grid=params,              <span class="comment"># 2. 要搜索的超参数网格</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>                       <span class="comment"># 3. 使用所有可用的CPU核心进行并行计算，加快搜索速度</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在训练集上执行网格搜索。它会自动进行交叉验证来评估每组参数的性能。</span></span><br><span class="line">grid.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看在交叉验证过程中，表现最好的那组参数所对应的平均分（通常是准确率）</span></span><br><span class="line">grid.best_score_</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用网格搜索找到的最佳模型（best_estimator_）在独立的测试集上进行最终评估</span></span><br><span class="line">grid.best_estimator_.score(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看最终被选中的那一组最佳超参数组合</span></span><br><span class="line">grid.best_params_</span><br></pre></td></tr></table></figure>

<h2 id="5-多分类问题"><a href="#5-多分类问题" class="headerlink" title="5 多分类问题"></a>5 多分类问题</h2><p>我们之前讨论的<strong>逻辑回归</strong>，其本质是一个**二元分类器 (Binary Classifier)**。它的设计初衷是回答“是”或“否”的问题（例如，是垃圾邮件/不是垃圾邮件）。</p>
<p>那么，如何用一个天生只能做“二选一”的工具，来解决“多选一”的问题呢？答案就是采用一种“<strong>分而治之</strong>”的策略，将一个复杂的多分类问题，拆解成多个简单的二元分类问题来解决。</p>
<p>这里介绍两种最主流的拆解策略： <strong>One-vs-One</strong> 和 <strong>One-vs-Rest</strong> 。</p>
<h3 id="5-1-One-vs-One-OvO-一对一"><a href="#5-1-One-vs-One-OvO-一对一" class="headerlink" title="5.1 One-vs-One (OvO) 一对一"></a>5.1 One-vs-One (OvO) 一对一</h3><p><strong>核心思想</strong>：为每一对类别组合都训练一个专门的二元分类器。</p>
<p><strong>训练过程</strong>：假设有 A, B, C, D 四个类别，我们需要为所有可能的配对都训练一个分类器：</p>
<ul>
<li><strong>分类器 1</strong>: A vs. B</li>
<li><strong>分类器 2</strong>: A vs. C</li>
<li><strong>分类器 3</strong>: A vs. D</li>
<li><strong>分类器 4</strong>: B vs. C</li>
<li><strong>分类器 5</strong>: B vs. D</li>
<li><strong>分类器 6</strong>: C vs. D</li>
</ul>
<p>如下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718163041.png" alt="image.png|634"></p>
<blockquote>
<p> 对于 <code>n</code> 个类别，总共需要训练 $C_n^2 = \frac{n(n - 1)}{2}$ 个分类器。</p>
</blockquote>
<p><strong>预测过程</strong>： 当一个新数据点到来时，我们同样会把它输入到所有的 <code>n(n-1)/2</code> 个分类器中。每个分类器都会对它所负责的两个类别中的一个进行“投票”。最终，得票最多的那个类别就是我们的预测结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库和模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsOneClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 数据准备 ---</span></span><br><span class="line"><span class="comment"># 加载scikit-learn内置的鸢尾花(Iris)数据集，这是一个经典的多分类问题（3个类别）</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 将特征数据赋值给x</span></span><br><span class="line">x = iris.data</span><br><span class="line"><span class="comment"># 将目标类别(0, 1, 2)赋值给y</span></span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集分割为训练集和测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 模型训练与评估 (One-vs-One策略) ---</span></span><br><span class="line"><span class="comment"># 定义一个基础的二元分类器，这里使用逻辑回归</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建OneVsOneClassifier实例，它会使用我们提供的基础分类器(clf)来处理多分类问题</span></span><br><span class="line">ovo = OneVsOneClassifier(clf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练数据对OvO分类器进行拟合。</span></span><br><span class="line"><span class="comment"># 对于3个类别的鸢尾花数据，它会自动为每对类别(0 vs 1, 0 vs 2, 1 vs 2)训练一个基础分类器，共3个。</span></span><br><span class="line">ovo.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估OvO模型的性能，返回准确率。</span></span><br><span class="line"><span class="comment"># 预测时，每个样本会由3个分类器投票，得票最多的类别为最终预测结果。</span></span><br><span class="line">ovo.score(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测，输出最终的类别标签</span></span><br><span class="line">ovo.predict(x_test)</span><br></pre></td></tr></table></figure>

<h3 id="5-2-One-vs-Rest-OvR-一对多"><a href="#5-2-One-vs-Rest-OvR-一对多" class="headerlink" title="5.2 One-vs-Rest (OvR) 一对多"></a>5.2 One-vs-Rest (OvR) 一对多</h3><p>这是最常用、最直观的一种策略，有时也叫 **One-vs-All (OvA)**。</p>
<p><strong>核心思想</strong>：对于一个有 <code>n</code> 个类别的问题，我们训练 <code>n</code> 个独立的二元分类器。<br><strong>训练过程</strong>：假设有 A, B, C, D 四个类别：</p>
<ul>
<li><strong>分类器 1</strong>: 负责判断 “是不是类别 A？” (即：<strong>A</strong> vs. <strong>非 A(B,C,D)</strong>)</li>
<li><strong>分类器 2</strong>: 负责判断 “是不是类别 B？” (即：<strong>B</strong> vs. <strong>非 B(A,C,D)</strong>)</li>
<li><strong>分类器 3</strong>: 负责判断 “是不是类别 C？” (即：<strong>C</strong> vs. <strong>非 C(A,B,D)</strong>)</li>
<li><strong>分类器 4</strong>: 负责判断 “是不是类别 D？” (即：<strong>D</strong> vs. <strong>非 D(A,B,C)</strong>)</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718163352.png" alt="image.png|696"></p>
<p><strong>预测过程</strong>： 当一个新数据点到来时，我们会把它<strong>同时输入到这 <code>n</code> 个分类器中</strong>。每个分类器都会输出一个“属于其正类的概率”。我们选择那个<strong>输出概率最高、最“自信”的分类器</strong>所代表的类别，作为最终的预测结果。这个过程就是图中的“**投票 (Voting)**”。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库和模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 数据准备 ---</span></span><br><span class="line"><span class="comment"># 加载scikit-learn内置的鸢尾花(Iris)数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 将特征数据赋值给x</span></span><br><span class="line">x = iris.data</span><br><span class="line"><span class="comment"># 将目标类别(0, 1, 2)赋值给y</span></span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集分割为训练集和测试集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 模型训练与评估 (One-vs-Rest策略) ---</span></span><br><span class="line"><span class="comment"># 定义一个基础的二元分类器，这里使用逻辑回归</span></span><br><span class="line">clf = LogisticRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建OneVsRestClassifier实例，它会使用我们提供的基础分类器(clf)来处理多分类问题</span></span><br><span class="line">ovr = OneVsRestClassifier(clf)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练数据对OvR分类器进行拟合。</span></span><br><span class="line"><span class="comment"># 对于3个类别的鸢尾花数据，它会为每个类别训练一个&quot;该类 vs. 所有其他类&quot;的分类器，共3个。</span></span><br><span class="line">ovr.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在测试集上评估OvR模型的性能，返回准确率。</span></span><br><span class="line"><span class="comment"># 预测时，每个样本会由3个分类器给出概率，概率最高的类别为最终预测结果。</span></span><br><span class="line">ovr.score(x_test, y_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对测试集进行预测，输出最终的类别标签</span></span><br><span class="line">ovr.predict(x_test)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718163650.png" alt="image.png|543"></p>
<h3 id="5-3-感知机不能处理多分类问题"><a href="#5-3-感知机不能处理多分类问题" class="headerlink" title="5.3 感知机不能处理多分类问题"></a>5.3 感知机不能处理多分类问题</h3><h4 id="5-3-1-“一对多”-One-vs-Rest-的失败模式：分类盲区"><a href="#5-3-1-“一对多”-One-vs-Rest-的失败模式：分类盲区" class="headerlink" title="5.3.1 “一对多” (One-vs-Rest) 的失败模式：分类盲区"></a>5.3.1 “一对多” (One-vs-Rest) 的失败模式：分类盲区</h4><p>感知机能借助一对多来完成多分类吗？答案是不能。比如下面这样包含三种类别的数据集：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722095402.png" alt="image.png"></p>
<p>根据一对多的方法，可能得到如下三种二分类，但在预测的时候每一种都认为 $\color{green}{\blacktriangle}$ 不是正类：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722101526.png" alt="image.png"></p>
<p>所以感知机不能借助一对多来完成多分类。</p>
<p><strong>结论</strong>：“一对多”策略会因为感知机的硬判决特性，在空间中划分出一些所有分类器都“拒绝”的区域，导致分类失败。</p>
<h4 id="5-3-2-“一对一”-One-vs-One-的失败模式：投票平局"><a href="#5-3-2-“一对一”-One-vs-One-的失败模式：投票平局" class="headerlink" title="5.3.2 “一对一” (One-vs-One) 的失败模式：投票平局"></a>5.3.2 “一对一” (One-vs-One) 的失败模式：投票平局</h4><p>“一对一”策略看起来更健壮，但对于感知机来说，同样存在无法解决的问题。</p>
<ul>
<li>想象一个三分类问题，类别为 <strong>A, B, C</strong>。我们需要训练三个分类器：(A vs B), (B vs C), (C vs A)。</li>
<li>现在来了一个新的数据点，投票过程如下：<ol>
<li>在 <strong>(A vs B)</strong> 的比赛中，<strong>A</strong> 胜出。</li>
<li>在 <strong>(B vs C)</strong> 的比赛中，<strong>B</strong> 胜出。</li>
<li>在 <strong>(C vs A)</strong> 的比赛中，<strong>C</strong> 胜出。</li>
</ol>
</li>
<li><strong>计票结果</strong>：A 获得 1 票，B 获得 1 票，C 获得 1 票。三方<strong>平局</strong>！</li>
<li>模型再次陷入困境：既然票数一样多，应该把这个点判给谁呢？感知机无法提供更多信息来打破这个僵局。</li>
</ul>
<p><strong>结论</strong>：“一对一”策略虽然避免了“分类盲区”，但由于其投票机制和感知机的硬判决特性，可能会出现无法决出胜负的“平局”情况，同样导致分类失败。</p>
<h1 id="多项式逻辑回归"><a href="#多项式逻辑回归" class="headerlink" title="多项式逻辑回归"></a>多项式逻辑回归</h1><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718103353.png" alt="image.png|620"></p>
<ul>
<li><strong>目的</strong>：多项式逻辑回归是为了让逻辑回归能够学习<strong>非线性的决策边界</strong>。</li>
<li><strong>原理</strong>：它通过<strong>创造原始特征的多项式项</strong>（如 $x^2, x_1x_2$ 等）作为新特征，然后将这些所有特征喂给一个<strong>标准的逻辑回归模型</strong>进行训练。</li>
<li><strong>本质</strong>：它是一种<strong>特征工程</strong>技术，而非一个全新的算法。它把一个非线性分类问题，转化成了一个更高维度的线性分类问题。</li>
<li><strong>关键</strong>：必须在模型的复杂性（由多项式阶数决定）和过拟合风险之间做出权衡。</li>
</ul>
<h1 id="分类模型评估指标"><a href="#分类模型评估指标" class="headerlink" title="分类模型评估指标"></a>分类模型评估指标</h1><p><strong>混淆矩阵</strong>（基础计数） → <strong>计算出</strong> <code>TP, FP, TN, FN</code> → <strong>用于构建</strong> → <strong>ROC 曲线</strong> 和 <strong>PR 曲线</strong>（可视化权衡） → <strong>计算出</strong> → <strong>AUC</strong>（单一数值总结性能）。</p>
<h2 id="1-混淆矩阵-Confusion-Matrix"><a href="#1-混淆矩阵-Confusion-Matrix" class="headerlink" title="1 混淆矩阵 (Confusion Matrix)"></a>1 混淆矩阵 (Confusion Matrix)</h2><p>混淆矩阵是一个表格，它用最直观的方式<strong>总结了分类模型的预测结果</strong>，展示了预测值与真实值之间的对应关系。它是后续所有指标的计算基础。</p>
<p>一个 2x2 的矩阵，包含四种预测情况：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722102436.png" alt="image.png"></p>
<ul>
<li><strong>TP (True Positive) - 真正例</strong>：真实是病人，模型也正确地预测为病人。<strong>（找对了）</strong></li>
<li><strong>TN (True Negative) - 真负例</strong>：真实是健康，模型也正确地预测为健康。<strong>（排除了）</strong></li>
<li><strong>FP (False Positive) - 假正例</strong>：真实是健康，但模型错误地预测为病人。<strong>（误诊了，Type I Error）</strong></li>
<li><strong>FN (False Negative) - 假负例</strong>：真实是病人，但模型错误地预测为健康。<strong>（漏诊了，Type II Error）</strong></li>
</ul>
<p><strong>核心价值</strong>：混淆矩阵清晰地展示了模型<strong>犯了哪种错误</strong>。在医疗诊断中，“漏诊”（FN）的代价通常远高于“误诊”（FP），混淆矩阵能帮助我们量化这两种不同的风险。</p>
<h3 id="1-1-F1-分数-F1-Score"><a href="#1-1-F1-分数-F1-Score" class="headerlink" title="1.1 F1 分数 (F1-Score)"></a>1.1 F1 分数 (F1-Score)</h3><p>通过混淆矩阵可以计算一系列重要评估指标，这些指标从不同角度衡量模型性能：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721170351.png" alt="image.png|490"></p>
<ul>
<li>**精确率 (Precision)**：<code>TP / (TP + FP)</code><ul>
<li>含义：在所有被模型<strong>预测为正例</strong>的样本中，有多少是真正的正例？</li>
<li>例子中：<code>9 / (9 + 5) = 64.3%</code>。这表示模型诊断出的癌症病例中，只有 64.3% 是准确的。这个指标关注的是“<strong>别误报</strong>”。</li>
</ul>
</li>
<li>**召回率 (Recall) / 灵敏度 (Sensitivity)**：<code>TP / (TP + FN)</code><ul>
<li>含义：在所有<strong>真正是正例</strong>的样本中，模型成功找出了多少？</li>
<li>例子中：<code>9 / (9 + 1) = 90%</code>。这表示模型成功找到了 90% 的癌症患者。这个指标关注的是“<strong>别漏报</strong>”。</li>
</ul>
</li>
</ul>
<p>由于精确率和召回率存在矛盾，我们就需要一个单一的指标来综合评估模型的整体性能。</p>
<ul>
<li><strong>F1 分数 (F1-Score)<strong>：精确率和召回率的</strong>调和平均数</strong>，是综合评价模型性能的常用指标。<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722102835.png" alt="|256"></li>
</ul>
<p><strong>为什么用调和平均数？</strong>: 因为它对两个指标中较差的那个更敏感。一个模型的 F1 分数要想高，必须保证精确率和召回率两者<strong>都比较高</strong>。如果其中任何一个很低，F1 分数就会被严重拉低。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721170801.png" alt="image.png|459"></p>
<h3 id="1-2-Accuracy-准确率"><a href="#1-2-Accuracy-准确率" class="headerlink" title="1.2 Accuracy (准确率)"></a>1.2 Accuracy (准确率)</h3><p>除此之外，还有一个<strong>Accuracy (准确率)</strong> 关注的是<strong>所有样本</strong>的整体判断正确率，不区分正例和负例。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722103624.png" alt="image.png"></p>
<p>易于理解和解释，“我的模型在所有样本里答对了 95%”，一听就懂，但在<strong>数据不平衡</strong>时会产生误导。</p>
<p>假设我们有一个包含 1000 名患者的数据集，其中：</p>
<ul>
<li>990 人是健康的（负例）</li>
<li>10 人患有癌症（正例）</li>
</ul>
<p>现在我们有一个非常“懒惰”的模型，它<strong>无论输入什么，都预测为“健康”</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722104009.png" alt="image.png"></p>
<p><strong>它的准确率是多少？</strong></p>
<ul>
<li><code>Accuracy = (0 + 990) / (0 + 990 + 0 + 10) = 990 / 1000 = 99%</code></li>
</ul>
<p>这个完全没有识别出任何一个癌症患者的、毫无用处的模型，其准确率竟然高达**99%**！这就是准确率的欺骗性。</p>
<p>而 F1-Score：</p>
<ul>
<li><code>Precision = TP / (TP + FP) = 0 / (0 + 0)</code>（分母为 0，无意义，或定义为 0）</li>
<li><code>Recall = TP / (TP + FN) = 0 / (0 + 10) = 0</code></li>
<li><strong><code>F1-Score</code></strong> 因为召回率为 0，所以最终的 F1 分数也为<strong>0</strong>。</li>
</ul>
<p><strong>结论</strong>：F1 分数（为 0）准确地揭示了这个模型的无用性，而准确率（为 99%）则给出了极具误导性的评价。</p>
<p><span style="color: #badc58; font-weight: 550;">总结：</span></p>
<ul>
<li><strong>准确率</strong>衡量的是所有类别（正例和负例）整体的预测正确性，</li>
<li>而 <strong>F1 分数</strong>更关注模型在正例（我们感兴趣的类别）上的表现，并且是精确率（Precision）和召回率（Recall）之间的一种平衡。</li>
</ul>
<p>这个区别导致了在某些场景下，特别是<strong>数据不平衡</strong>时，准确率会产生误导，而 F1 分数则能更真实地反映模型的性能。</p>
<h3 id="1-3-代码实现"><a href="#1-3-代码实现" class="headerlink" title="1.3 代码实现"></a>1.3 代码实现</h3><p>相关 API</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix  </span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line">confusion_matrix(y_test,y_predict)</span><br><span class="line">precision_score(y_test,y_predict)</span><br><span class="line">recall_score(y_test,y_predict)</span><br><span class="line">f1_score(y_test,y_predict)</span><br></pre></td></tr></table></figure>

<p><span style="color: #badc58; font-weight: 550;">具体案例：绘制精确率和召回率随阈值变化的曲线</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入NumPy库，用于进行高效的数值计算</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 导入matplotlib的pyplot模块，用于数据可视化</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 从scikit-learn中导入datasets模块，用于加载示例数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 加载和准备数据 ---</span></span><br><span class="line"><span class="comment"># 加载经典的鸢尾花（Iris）数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"><span class="comment"># 提取特征数据 X（花萼长度、花萼宽度、花瓣长度、花瓣宽度）</span></span><br><span class="line">X = iris.data</span><br><span class="line"><span class="comment"># 提取标签数据 y（花的类别），并创建一个副本以防意外修改原始数据</span></span><br><span class="line">y = iris.target.copy()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将原始的多分类问题（类别0, 1, 2）转化为二分类问题（类别0 vs 类别1）</span></span><br><span class="line"><span class="comment"># 所有标签不等于0的样本（即类别1和2）都被统一标记为类别1</span></span><br><span class="line">y[y!=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 训练模型 ---</span></span><br><span class="line"><span class="comment"># 从scikit-learn中导入用于分割数据集的train_test_split函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 从scikit-learn中导入逻辑回归模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集分割为训练集和测试集，random_state保证每次分割结果都一样，便于复现</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个逻辑回归模型的实例</span></span><br><span class="line">logistic_regression = LogisticRegression()</span><br><span class="line"><span class="comment"># 使用训练集数据来训练（拟合）逻辑回归模型</span></span><br><span class="line">logistic_regression.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 对测试集进行预测（注意：这里的预测使用的是模型默认的0作为决策边界阈值）</span></span><br><span class="line">y_predict = logistic_regression.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取模型在测试集上每个样本的决策分数（decision score）</span></span><br><span class="line"><span class="comment"># 这个分数是模型内部计算出的原始分值，代表了样本属于正类的“信心强度”</span></span><br><span class="line"><span class="comment"># 它是后续计算PR曲线或ROC曲线的基础</span></span><br><span class="line">decision_scores = logistic_regression.decision_function(X_test)</span><br><span class="line">decision_scores</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722110141.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 3. 计算PR曲线所需的点 ---</span></span><br><span class="line"><span class="comment"># 从scikit-learn中导入精确率（precision）和召回率（recall）的计算函数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化两个空列表，用于存储在不同阈值下的精确率和召回率</span></span><br><span class="line">precision_scores = []</span><br><span class="line">recall_scores = []</span><br><span class="line"><span class="comment"># 将决策分数从小到大排序，并将每个分数都作为一次潜在的分类阈值</span></span><br><span class="line"><span class="comment"># 这是生成PR曲线上所有数据点的标准方法</span></span><br><span class="line">thresholds = np.sort(decision_scores)</span><br><span class="line"><span class="comment"># 遍历每一个可能的阈值</span></span><br><span class="line"><span class="keyword">for</span> threshold <span class="keyword">in</span> thresholds:</span><br><span class="line">    <span class="comment"># 根据当前阈值，将决策分数转化为0或1的二元预测结果</span></span><br><span class="line">    <span class="comment"># 分数大于等于当前阈值的样本被预测为正类（1），否则为负类（0）</span></span><br><span class="line">    y_predict = np.array(decision_scores&gt;=threshold, dtype=<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">    <span class="comment"># 计算在当前阈值下，预测结果的精确率</span></span><br><span class="line">    precision = precision_score(y_test, y_predict)</span><br><span class="line">    <span class="comment"># 计算在当前阈值下，预测结果的召回率</span></span><br><span class="line">    recall = recall_score(y_test,y_predict)</span><br><span class="line">    <span class="comment"># 将计算出的精确率和召回率分别存入列表中，用于后续绘制PR曲线</span></span><br><span class="line">    precision_scores.append(precision)</span><br><span class="line">    recall_scores.append(recall)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. 绘制精确率和召回率随阈值变化的曲线 ---</span></span><br><span class="line"><span class="comment"># 使用matplotlib绘制精确率（Precision）随阈值（threshold）变化的曲线</span></span><br><span class="line"><span class="comment"># x轴为阈值，y轴为精确率分数，设置为红色（&#x27;r&#x27;），并添加图例标签&quot;precision&quot;</span></span><br><span class="line">plt.plot(thresholds, precision_scores, color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&quot;precision&quot;</span>)</span><br><span class="line"><span class="comment"># 同样地，在同一张图上绘制召回率（Recall）随阈值变化的曲线</span></span><br><span class="line"><span class="comment"># x轴为阈值，y轴为召回率分数，设置为蓝色（&#x27;b&#x27;），并添加图例标签&quot;recall&quot;</span></span><br><span class="line">plt.plot(thresholds, recall_scores, color=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&quot;recall&quot;</span>)</span><br><span class="line"><span class="comment"># 显示图例，用于区分两条曲线分别代表什么</span></span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># 将最终绘制的图形显示出来</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722110402.png" alt="image.png"></p>
<p>或者直接调包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve  </span><br><span class="line">precision_scores, recall_scores,thresholds =  precision_recall_curve(y_test, decision_scores)</span><br><span class="line">plt.plot(thresholds, precision_scores[:-<span class="number">1</span>], color=<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&quot;precision&quot;</span>)  </span><br><span class="line">plt.plot(thresholds, recall_scores[:-<span class="number">1</span>], color=<span class="string">&#x27;b&#x27;</span>,label=<span class="string">&quot;recall&quot;</span>)  </span><br><span class="line">plt.legend()  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>结果是一样的：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722110632.png" alt="image.png"></p>
<h2 id="2-ROC-曲线-Receiver-Operating-Characteristic-Curve"><a href="#2-ROC-曲线-Receiver-Operating-Characteristic-Curve" class="headerlink" title="2 ROC 曲线 (Receiver Operating Characteristic Curve)"></a>2 ROC 曲线 (Receiver Operating Characteristic Curve)</h2><p>ROC 曲线是一个图形，它展示了当<strong>分类阈值</strong>变化时，模型“<strong>找出病人的能力</strong>”与“<strong>误诊健康人的风险</strong>”之间的权衡关系。</p>
<ul>
<li><strong>纵坐标 (Y-axis): 真正例率 (True Positive Rate, TPR)</strong><ul>
<li><strong>公式</strong>: <code>TPR = TP / (TP + FN)</code></li>
<li><strong>含义</strong>: 在所有<strong>真正患病</strong>的病人中，模型成功找出了多少比例。这个指标也叫**召回率 (Recall) 或灵敏度 (Sensitivity)**。我们希望它尽可能高（接近 1）。</li>
</ul>
</li>
<li><strong>横坐标 (X-axis): 假正例率 (False Positive Rate, FPR)</strong><ul>
<li><strong>公式</strong>: <code>FPR = FP / (FP + TN)</code></li>
<li><strong>含义</strong>: 在所有<strong>真正健康</strong>的人中，模型错误地诊断为病人的比例。我们希望它尽可能低（接近 0）。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve  </span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_test,decision_scores)</span><br><span class="line">plt.plot(fpr,tpr)  </span><br><span class="line">plt.xlabel(<span class="string">&quot;FPR&quot;</span>)  </span><br><span class="line">plt.ylabel(<span class="string">&quot;TPR&quot;</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722104938.png" alt="image.png|570"></p>
<ul>
<li><strong>理想模型</strong>：ROC 曲线会尽可能地向<strong>左上角</strong>靠近。<code>(0, 1)</code> 点代表了完美的分类器（FPR=0, TPR=1），即“零误诊，百分百检出”。</li>
<li><strong>随机猜测模型</strong>: 对应的是从 <code>(0, 0)</code> 到 <code>(1, 1)</code> 的对角线。</li>
<li><strong>曲线越凸向左上角，模型的性能越好</strong>。</li>
</ul>
<h2 id="3-PR-曲线-Precision-Recall-Curve"><a href="#3-PR-曲线-Precision-Recall-Curve" class="headerlink" title="3 PR 曲线 (Precision-Recall Curve)"></a>3 PR 曲线 (Precision-Recall Curve)</h2><p>PR 曲线是另一个图形，它展示了当<strong>分类阈值</strong>变化时，模型的“<strong>诊断准确性</strong>”与“<strong>找出病人的能力</strong>”之间的权衡关系。</p>
<ul>
<li><strong>纵坐标 (Y-axis): 精确率 (Precision)</strong><ul>
<li><strong>公式</strong>: <code>Precision = TP / (TP + FP)</code></li>
<li><strong>含义</strong>: 在所有被模型<strong>诊断为患病</strong>的人中，有多少是真正的病人。它关注的是预测结果的准确性。</li>
</ul>
</li>
<li><strong>横坐标 (X-axis): 召回率 (Recall)</strong><ul>
<li><strong>公式</strong>: <code>Recall = TP / (TP + FN)</code></li>
<li><strong>含义</strong>: 和 ROC 曲线的 TPR 一样，代表找出真正病人的能力。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line">precision_scores, recall_scores,thresholds =  precision_recall_curve(y_test,decision_scores)</span><br><span class="line">plt.plot(recall_scores, precision_scores)  </span><br><span class="line">plt.xlabel(<span class="string">&quot;Recall&quot;</span>)  </span><br><span class="line">plt.ylabel(<span class="string">&quot;Precision&quot;</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722111141.png" alt="image.png"></p>
<ul>
<li><strong>理想模型</strong>：PR 曲线会尽可能地向<strong>右上角</strong>靠近。<code>(1, 1)</code> 点代表了完美的分类器（Recall=1, Precision=1），即“在全部找出病人的基础上，还做到了零误诊”。</li>
<li><strong>曲线越凸向右上角，模型的性能越好</strong>。</li>
<li><strong>特别用途</strong>：当数据集中<strong>正负样本数量极不平衡</strong>时（例如，健康人远多于病人），PR 曲线比 ROC 曲线能更真实地反映模型的性能。</li>
</ul>
<h2 id="4-AUC-Area-Under-the-Curve"><a href="#4-AUC-Area-Under-the-Curve" class="headerlink" title="4 AUC (Area Under the Curve)"></a>4 AUC (Area Under the Curve)</h2><p>AUC 是“<strong>曲线下面积</strong>”的缩写。它是一个<strong>单一的数值</strong>，用来概括整个曲线所代表的模型性能。这个数值介于 0 和 1 之间。</p>
<ul>
<li><strong>AUC-ROC</strong>: 指的是 <strong>ROC 曲线</strong>下方的面积。<ul>
<li><strong>含义</strong>: AUC-ROC 的值可以被直观地理解为：<strong>随机抽取一个真正的病人（正样本）和一个真正的健康人（负样本），你的模型将这个病人预测为“患病”的概率排在将那个健康人预测为“患病”的概率之上的可能性有多大。</strong></li>
<li><strong>解读</strong>:<ul>
<li>AUC = 1.0: 完美分类器。</li>
<li>AUC = 0.5: 随机猜测。</li>
<li>AUC &gt; 0.5: 模型有预测价值，值越高越好。</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score  </span><br><span class="line">auc_roc = roc_auc_score(y_test,decision_scores)  </span><br><span class="line">auc_roc <span class="comment"># 1.0</span></span><br></pre></td></tr></table></figure>

<ul>
<li><strong>AUC-PR</strong>: 指的是 <strong>PR 曲线</strong>下方的面积。<ul>
<li><strong>含义</strong>: 它综合了模型在不同召回率水平下的精确率表现。</li>
<li><strong>解读</strong>: AUC-PR 的值越高，说明模型在保证高查全率的同时，也能保持高准确性，性能越好。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> average_precision_score </span><br><span class="line">auc_pr = average_precision_score(y_test, decision_scores) </span><br><span class="line">auc_pr <span class="comment"># 1.0</span></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://VernalScenery.github.io">Scenery</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://vernalscenery.github.io/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/">https://vernalscenery.github.io/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://VernalScenery.github.io" target="_blank">春和景明的记事本</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/./img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/06_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%86%B3%E7%AD%96%E6%A0%91/" title="06_机器学习_决策树"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">06_机器学习_决策树</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/05_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%86%B5(Entropy)/" title="05_机器学习_熵(Entropy)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">05_机器学习_熵(Entropy)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/" title="00_机器学习_KNN算法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-04</div><div class="title">00_机器学习_KNN算法</div></div></a></div><div><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">01_机器学习_感知机</div></div></a></div><div><a href="/2025/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E8%A7%88%E3%80%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="00_机器学习_概览、环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-03</div><div class="title">00_机器学习_概览、环境搭建</div></div></a></div><div><a href="/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="02_机器学习_线性回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="title">02_机器学习_线性回归</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="04_机器学习_基础概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">04_机器学习_基础概念</div></div></a></div><div><a href="/2025/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/07_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(MLP)/" title="07_机器学习_多层感知机(MLP)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-23</div><div class="title">07_机器学习_多层感知机(MLP)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Scenery</div><div class="author-info__description">今天不想跑，所以才去跑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chjm0121" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1595718686@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-text">线性逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%BC%95%E5%85%A5"><span class="toc-text">1 引入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Sigmoid-%E5%87%BD%E6%95%B0"><span class="toc-text">2 Sigmoid 函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">3 损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%BB%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A0%B8%E5%BF%83%E5%81%87%E8%AE%BE%E5%BC%80%E5%A7%8B%EF%BC%88%E6%A6%82%E7%8E%87%E8%A7%86%E8%A7%92%EF%BC%89"><span class="toc-text">3.1 从模型的核心假设开始（概率视角）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%94%A8%E4%B8%80%E4%B8%AA%E7%BB%9F%E4%B8%80%E7%9A%84%E5%85%AC%E5%BC%8F%E6%9D%A5%E8%A1%A8%E7%A4%BA%E6%A6%82%E7%8E%87"><span class="toc-text">3.2 用一个统一的公式来表示概率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%BC%95%E5%85%A5%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E2%80%94%E2%80%94%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-MLE"><span class="toc-text">3.3 引入核心思想——最大似然估计 (MLE)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E7%AE%80%E5%8C%96%E9%97%AE%E9%A2%98%E2%80%94%E2%80%94%E5%8F%96%E5%AF%B9%E6%95%B0"><span class="toc-text">3.4 简化问题——取对数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E8%BD%AC%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E2%80%94%E2%80%94%E5%8F%96%E8%B4%9F%E5%8F%B7%E5%B9%B6%E5%B9%B3%E5%9D%87"><span class="toc-text">3.5 转为损失函数——取负号并平均</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E9%98%B2%E6%AD%A2%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9A%E6%AD%A3%E5%88%99%E5%8C%96-%E6%83%A9%E7%BD%9A%E9%A1%B9"><span class="toc-text">3.6 防止过拟合：正则化 (惩罚项)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%B1%82%E8%A7%A3%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-text">3.7 损失函数的求解：梯度下降</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%BA%8C%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">4 二分类问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">5 多分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-One-vs-One-OvO-%E4%B8%80%E5%AF%B9%E4%B8%80"><span class="toc-text">5.1 One-vs-One (OvO) 一对一</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-One-vs-Rest-OvR-%E4%B8%80%E5%AF%B9%E5%A4%9A"><span class="toc-text">5.2 One-vs-Rest (OvR) 一对多</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E6%84%9F%E7%9F%A5%E6%9C%BA%E4%B8%8D%E8%83%BD%E5%A4%84%E7%90%86%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">5.3 感知机不能处理多分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-1-%E2%80%9C%E4%B8%80%E5%AF%B9%E5%A4%9A%E2%80%9D-One-vs-Rest-%E7%9A%84%E5%A4%B1%E8%B4%A5%E6%A8%A1%E5%BC%8F%EF%BC%9A%E5%88%86%E7%B1%BB%E7%9B%B2%E5%8C%BA"><span class="toc-text">5.3.1 “一对多” (One-vs-Rest) 的失败模式：分类盲区</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-3-2-%E2%80%9C%E4%B8%80%E5%AF%B9%E4%B8%80%E2%80%9D-One-vs-One-%E7%9A%84%E5%A4%B1%E8%B4%A5%E6%A8%A1%E5%BC%8F%EF%BC%9A%E6%8A%95%E7%A5%A8%E5%B9%B3%E5%B1%80"><span class="toc-text">5.3.2 “一对一” (One-vs-One) 的失败模式：投票平局</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E9%A1%B9%E5%BC%8F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-text">多项式逻辑回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">分类模型评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5-Confusion-Matrix"><span class="toc-text">1 混淆矩阵 (Confusion Matrix)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-F1-%E5%88%86%E6%95%B0-F1-Score"><span class="toc-text">1.1 F1 分数 (F1-Score)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Accuracy-%E5%87%86%E7%A1%AE%E7%8E%87"><span class="toc-text">1.2 Accuracy (准确率)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">1.3 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-ROC-%E6%9B%B2%E7%BA%BF-Receiver-Operating-Characteristic-Curve"><span class="toc-text">2 ROC 曲线 (Receiver Operating Characteristic Curve)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-PR-%E6%9B%B2%E7%BA%BF-Precision-Recall-Curve"><span class="toc-text">3 PR 曲线 (Precision-Recall Curve)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-AUC-Area-Under-the-Curve"><span class="toc-text">4 AUC (Area Under the Curve)</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/13_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" title="13_机器学习_概率图模型">13_机器学习_概率图模型</a><time datetime="2025-07-29T00:29:02.000Z" title="发表于 2025-07-29 08:29:02">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/11_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%81%9A%E7%B1%BB/" title="11_机器学习_聚类">11_机器学习_聚类</a><time datetime="2025-07-28T00:19:53.000Z" title="发表于 2025-07-28 08:19:53">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维">12_机器学习_降维</a><time datetime="2025-07-27T20:48:42.000Z" title="发表于 2025-07-28 04:48:42">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="10_机器学习_集成学习">10_机器学习_集成学习</a><time datetime="2025-07-26T01:58:53.000Z" title="发表于 2025-07-26 09:58:53">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="09_机器学习_朴素贝叶斯">09_机器学习_朴素贝叶斯</a><time datetime="2025-07-25T03:08:47.000Z" title="发表于 2025-07-25 11:08:47">2025-07-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./img/1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Scenery</div><div class="footer_custom_text"><div>波澜不惊</div><div class="footer-div"><img class="footer-icon" src="./img/备案图标.png"><a class="footer-a" target="_blank" rel="noopener" href="http://beian.miit.gov.cn/">皖ICP备2021016944号-1</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>