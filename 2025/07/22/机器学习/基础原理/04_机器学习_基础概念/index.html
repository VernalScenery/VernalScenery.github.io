<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>04_机器学习_基础概念 | 春和景明的记事本</title><meta name="author" content="Scenery"><meta name="copyright" content="Scenery"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="损失函数1 什么是损失函数？损失函数，也称为代价函数（Cost Function）或目标函数（Objective Function） 它的根本任务是量化模型的预测结果与真实情况之间的差距。这个“差距”或“误差”就是损失（Loss）。整个模型的训练过程，就是一个通过不断调整参数来最小化损失函数值 我们"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://vernalscenery.github.io/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '04_机器学习_基础概念',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-05 14:17:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="春和景明的记事本"><span class="site-name">春和景明的记事本</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">04_机器学习_基础概念</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-22T03:20:30.000Z" title="发表于 2025-07-22 11:20:30">2025-07-22</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T06:17:15.000Z" title="更新于 2025-08-05 14:17:15">2025-08-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">11.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>38分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="04_机器学习_基础概念"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><h2 id="1-什么是损失函数？"><a href="#1-什么是损失函数？" class="headerlink" title="1 什么是损失函数？"></a>1 什么是损失函数？</h2><p>损失函数，也称为代价函数（Cost Function）或目标函数（Objective Function）</p>
<p>它的根本任务是<strong>量化模型的预测结果与真实情况之间的差距</strong>。这个“差距”或“误差”就是<strong>损失（Loss）</strong>。整个模型的训练过程，就是一个通过不断调整参数来<strong>最小化损失函数值</strong></p>
<p>我们可以用一个简单的比喻来理解：</p>
<ul>
<li><strong>模型</strong>：就像一个正在学习射箭的学生。</li>
<li><strong>真实值</strong>：就是靶心。</li>
<li><strong>预测值</strong>：就是学生射出的箭的位置。</li>
<li><strong>损失函数</strong>：就是衡量箭离靶心有多远的尺子。箭离靶心越远，测出的“损失”就越大。</li>
<li><strong>训练过程</strong>：就是学生根据每次射箭的结果（损失值），不断调整自己的姿势和力度，争取下一箭能离靶心更近一些（最小化损失）。</li>
</ul>
<h2 id="2-损失函数有哪些？"><a href="#2-损失函数有哪些？" class="headerlink" title="2 损失函数有哪些？"></a>2 损失函数有哪些？</h2><h3 id="2-1-距离度量-主要用于回归问题"><a href="#2-1-距离度量-主要用于回归问题" class="headerlink" title="2.1 距离度量 (主要用于回归问题)"></a>2.1 距离度量 (主要用于回归问题)</h3><p>这类损失函数衡量的是预测值和真实值在数值上的“距离”。</p>
<h4 id="2-1-1-均方误差-Mean-Squared-Error-MSE"><a href="#2-1-1-均方误差-Mean-Squared-Error-MSE" class="headerlink" title="2.1.1 均方误差 (Mean Squared Error, MSE)"></a>2.1.1 均方误差 (Mean Squared Error, MSE)</h4><p>这是回归问题中最常用、最经典的损失函数之一。它计算的是预测值与真实值之差的平方的平均值。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719102719.png" alt="image.png"></p>
<p>其中，$n$ 是样本数量，$y_i$​ 是第 $i$ 个样本的真实值，$\hat{y}_i​$ 是模型对第 $i$ 个样本的预测值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error  </span><br><span class="line">mean_squared_error(y_real, y_predict)</span><br></pre></td></tr></table></figure>

<h4 id="2-1-2-均方根误差（Root-Mean-Square-Error-RMSE）"><a href="#2-1-2-均方根误差（Root-Mean-Square-Error-RMSE）" class="headerlink" title="2.1.2 均方根误差（Root Mean Square Error, RMSE）"></a>2.1.2 均方根误差（Root Mean Square Error, RMSE）</h4><p>它相比于 MSE，有一个非常重要的优点：</p>
<ul>
<li><strong>单位一致，易于解释</strong></li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719103241.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_squared_error(y_real, y_predict, squared=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h4 id="2-1-3-平均绝对误差-Mean-Absolute-Error-MAE"><a href="#2-1-3-平均绝对误差-Mean-Absolute-Error-MAE" class="headerlink" title="2.1.3 平均绝对误差 (Mean Absolute Error, MAE)"></a>2.1.3 平均绝对误差 (Mean Absolute Error, MAE)</h4><p>它计算的是预测值与真实值之差的绝对值的平均值。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719102915.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error  </span><br><span class="line">mean_absolute_error(y_real, y_predict)</span><br></pre></td></tr></table></figure>

<h3 id="2-2-补充知识：熵"><a href="#2-2-补充知识：熵" class="headerlink" title="2.2 补充知识：熵"></a>2.2 补充知识：熵</h3><blockquote>
<p> 熵是用来衡量一个系统的“<strong>混乱程度</strong>”或“<strong>不确定性</strong>”</p>
</blockquote>
<h4 id="2-2-1-信息熵-Information-Entropy-H-p"><a href="#2-2-1-信息熵-Information-Entropy-H-p" class="headerlink" title="2.2.1 信息熵 (Information Entropy, H(p))"></a>2.2.1 信息熵 (Information Entropy, H(p))</h4><p>它衡量的是<strong>单一</strong>概率分布 <code>p</code> 的<strong>不确定性</strong>或<strong>混乱程度</strong>。一个分布越混乱、越不可预测，其信息熵就越高。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719104041.png" alt="image.png"></p>
<ul>
<li>$p(x_i​)$: 这是事件 $x_i$​ 发生的概率。</li>
<li>$[−log2​p(x_i​)]$: 这部分可以被理解为事件 $x_i$​ 发生时，所带来的“信息量”或“惊讶程度”。</li>
</ul>
<p>为什么用对数，还是负的对数？</p>
<ul>
<li><strong>概率越小，信息量越大</strong>：一件非常不可能发生的事情（概率 p 很小）一旦发生了，会让我们非常惊讶，我们从中获得的信息量就很大。<code>log</code> 函数正好满足这个特性，当 $p(x_i​)$ 趋近于 0 时，$logp(x_i​)$ 趋近于负无穷。<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719104255.png" alt="image.png|179"></li>
<li><strong>信息量必须是正数</strong>：概率 $p(x_i​)$ 是一个 0 到 1 之间的数，所以它的对数 $log2​p(x_i​)$ 是小于等于 0 的。为了让“信息量”成为一个正数，我们在前面加上一个<strong>负号</strong>，变成了 $−log2​p(x_i​)$。</li>
<li><strong>两个独立事件的信息量应该相加</strong>：假设你有两个独立的事件 A 和 B，它们同时发生的概率是 $p(A)⋅p(B)$。我们直观上认为，接收到这两个独立事件信息的总信息量，应该是它们各自信息量之和，即 $I(A and B)=I(A)+I(B)$。只有对数函数 <code>log</code> 满足这个性质：$log(ab)=log(a)+log(b)$。这就是选择对数函数最关键的数学原因。</li>
</ul>
<p>现在我们理解了 $[−log2​p(x_i​)]$ 是<strong>单个事件 $x_i$​ 发生所带来的信息量</strong></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719104522.png" alt="image.png"></p>
<p>用每个结果的‘<strong>发生概率</strong>’去加权该结果的‘<strong>信息量</strong>’，最终得到整个系统信息量的平均期望。</p>
<blockquote>
<p> 这里的<strong>信息量</strong>也可以理解为 <strong>不确定性</strong>，概率越小的事件，不确定越大，然后对其加权求平均</p>
</blockquote>
<p>信息熵的本质定义就是：一个随机变量所有可能结果带来的信息量的数学<strong>期望</strong>。</p>
<h4 id="2-2-2-交叉熵-Cross-Entropy-L-p-q"><a href="#2-2-2-交叉熵-Cross-Entropy-L-p-q" class="headerlink" title="2.2.2 交叉熵 (Cross Entropy, L(p, q))"></a>2.2.2 交叉熵 (Cross Entropy, L(p, q))</h4><p>交叉熵 <code>L(p, q)</code> 是衡量你的预测 <code>q</code> 与现实 <code>p</code> 之间差距的一种方式。你的预测越不准，你需要付出的“额外成本”（编码长度）就越多，交叉熵的值就越大。</p>
<p>信息论有一个基本原则：给<strong>概率越高的事件分配越短的编码</strong>，总成本（平均编码长度）就越低。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719105146.png"></p>
<ul>
<li><code>p(xᵢ)</code>：<strong>现实</strong>。即第 i 个事件实际发生的概率。它在这里充当<strong>权重</strong>。</li>
<li><code>-log q(xᵢ)</code>：<strong>成本</strong>。即根据你的预测 <code>q</code>，为第 i 个事件编码所需要的长度。你预测的概率 <code>q</code> 越小，这个编码长度就越长，成本越高。</li>
</ul>
<p><strong>交叉熵</strong>衡量的是，当我们用一个<strong>预测的概率分布 <code>q</code></strong> 来编码来自<strong>真实的概率分布 <code>p</code></strong> 的事件时，所需要的<strong>平均成本</strong>（或编码长度）。</p>
<ul>
<li>成本越高（即交叉熵值越大），代表模型的预测分布 <code>q</code> 与真实的分布 <code>p</code> 之间的差距越大，因此模型的预测就越不准确。</li>
</ul>
<h4 id="2-2-3-相对熵-Relative-Entropy-KL-散度-D-KL-p-q"><a href="#2-2-3-相对熵-Relative-Entropy-KL-散度-D-KL-p-q" class="headerlink" title="2.2.3 相对熵 (Relative Entropy / KL 散度, D_KL(p||q))"></a>2.2.3 相对熵 (Relative Entropy / KL 散度, D_KL(p||q))</h4><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719110638.png" alt="image.png"></p>
<p><strong>相对熵</strong>可以由<strong>交叉熵</strong>和<strong>信息熵</strong>导出：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719111013.png" alt="image.png"></p>
<ul>
<li>信息熵 <code>H(p)</code> — 理论上的最低成本</li>
<li>交叉熵 <code>H(p,q)</code> — 你实际付出的总成本</li>
</ul>
<p><code>D_KL(p||q)</code> 衡量的就是，当我们用<strong>预测分布 <code>q</code></strong> 去近似<strong>真实分布 <code>p</code></strong> 时，我们多付出的<strong>额外成本</strong>，亦或损失了多少信息</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719111708.png" alt="image.png"></p>
<h3 id="2-3-熵的度量-主要用于分类问题"><a href="#2-3-熵的度量-主要用于分类问题" class="headerlink" title="2.3 熵的度量 (主要用于分类问题)"></a>2.3 熵的度量 (主要用于分类问题)</h3><p><span style="color: #badc58; font-weight: 550;">交叉熵损失 (Cross-Entropy Loss)</span></p>
<ul>
<li>是分类问题中<strong>最常用</strong>的损失函数，尤其是在处理多分类问题和逻辑回归中。</li>
<li>它的核心思想是衡量模型预测的概率分布与真实的概率分布之间的差异。</li>
</ul>
<p>它包括：</p>
<ul>
<li><strong>二元交叉熵损失 (Binary Cross-Entropy)</strong>: 用于二分类问题。</li>
<li><strong>分类交叉熵损失 (Categorical Cross-Entropy)</strong>: 用于多分类问题。</li>
</ul>
<h4 id="2-3-1-分类交叉熵损失-Categorical-Cross-Entropy-Loss"><a href="#2-3-1-分类交叉熵损失-Categorical-Cross-Entropy-Loss" class="headerlink" title="2.3.1 分类交叉熵损失 (Categorical Cross-Entropy Loss)"></a>2.3.1 分类交叉熵损失 (Categorical Cross-Entropy Loss)</h4><p>用于多分类问题（例如，识别数字 0-9）。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719112028.png" alt="image.png"></p>
<p>其中，$n$ 是样本数，$C$ 是类别数。$y_{ij}​$ 是一个符号函数（0 或 1），如果样本 $i$ 的真实类别是 $j$ 则为 1，否则为 0。$\hat{y}_{ij}$ 是模型预测样本 $i$ 属于类别 $j$ 的概率。</p>
<p><span style="color: #badc58; font-weight: 550;">第一层：“内层”的期望（单个样本的交叉熵）</span></p>
<p>我们先看括号里的部分：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719112239.png" alt="image.png"></p>
<p>这部分计算的是<strong>第 <code>i</code> 个样本</strong>的损失。</p>
<ul>
<li><code>yᵢⱼ</code> 是真实分布的概率（权重）。</li>
<li><code>-log(ŷᵢⱼ)</code> 是信息量（值）。</li>
</ul>
<p>这里就是加权求期望</p>
<p><span style="color: #badc58; font-weight: 550;">第二层：“外层”的期望（所有样本的平均损失）</span></p>
<p>现在我们看整个公式：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719112506.png" alt="image.png"></p>
<p>就是求平均值</p>
<h4 id="2-3-2-二元交叉熵损失-Binary-Cross-Entropy-Loss"><a href="#2-3-2-二元交叉熵损失-Binary-Cross-Entropy-Loss" class="headerlink" title="2.3.2 二元交叉熵损失 (Binary Cross-Entropy Loss)"></a>2.3.2 二元交叉熵损失 (Binary Cross-Entropy Loss)</h4><p>用于二分类问题（例如，是/否，0/1）。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719111643.png" alt="image.png|416"></p>
<p><span style="color: #badc58; font-weight: 550;">从“分类交叉熵损失”出发</span></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719113013.png" alt="image.png"></p>
<p>在二分类问题中，我们只有两个类别，可以称之为<strong>类别 1</strong>（正类）和<strong>类别 0</strong>（负类）。所以，类别总数 <code>C=2</code>。</p>
<p>对于单个样本 <code>i</code>，我们可以把内层的求和 <code>Σ</code> 展开：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719113045.png" alt="image.png"></p>
<ul>
<li><code>yᵢ₀</code>: 第 <code>i</code> 个样本的真实标签是否为<strong>类别 0</strong>。</li>
<li><code>yᵢ₁</code>: 第 <code>i</code> 个样本的真实标签是否为<strong>类别 1</strong>。</li>
<li><code>ŷᵢ₀</code>: 模型预测第 <code>i</code> 个样本为<strong>类别 0</strong>的概率。</li>
<li><code>ŷᵢ₁</code>: 模型预测第 <code>i</code> 个样本为<strong>类别 1</strong>的概率。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">简化符号表示</span></p>
<ul>
<li>我们定义 <code>yᵢ</code> 为第 <code>i</code> 个样本的真实标签。如果真实是正类（类别 1），则 <code>yᵢ = 1</code>；如果真实是负类（类别 0），则 <code>yᵢ = 0</code>。</li>
<li>我们定义 <code>ŷᵢ</code> 为模型预测第 <code>i</code> 个样本为 <strong>正类（类别 1）</strong> 的概率。</li>
</ul>
<p>现在，我们用 <code>yᵢ</code> 和 <code>ŷᵢ</code> 来表示上面展开式中的所有项：</p>
<p>针对第 $i$ 个样本，我们可以知道：</p>
<ul>
<li><p>当 <code>yᵢ=1</code> 时，<code>1-yᵢ=0</code>；当 <code>yᵢ=0</code> 时，<code>1-yᵢ=1</code>，故有：</p>
<ul>
<li><code>yᵢ₁</code> (是类别 1 的标签) 就等于 <code>yᵢ</code>。</li>
<li><code>yᵢ₀</code> (是类别 0 的标签) 就等于 <code>1 - yᵢ</code>。</li>
</ul>
</li>
<li><p>因为两个类别的预测概率之和必须为 1，故有</p>
<ul>
<li><code>ŷᵢ₁</code> (预测为类别 1 的概率) 就等于 <code>ŷᵢ</code>。</li>
<li><code>ŷᵢ₀</code> (预测为类别 0 的概率) 就等于 <code>1 - ŷᵢ₁</code>，也就是 <code>1 - ŷᵢ</code>。</li>
</ul>
</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">代入可得二元交叉熵损失的单样本形式：</span></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719114156.png" alt="image.png"></p>
<p>最后再对 <code>n</code> 个样本求平均即可</p>
<h3 id="2-4-其他损失函数"><a href="#2-4-其他损失函数" class="headerlink" title="2.4 其他损失函数"></a>2.4 其他损失函数</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719114359.png" alt="image.png|557"></p>
<h2 id="3-学习准则"><a href="#3-学习准则" class="headerlink" title="3 学习准则"></a>3 学习准则</h2><h3 id="3-1-经验风险最小化-ERM"><a href="#3-1-经验风险最小化-ERM" class="headerlink" title="3.1 经验风险最小化 (ERM)"></a>3.1 经验风险最小化 (ERM)</h3><p>ERM 准则非常直接：<strong>一个模型在训练数据上犯的错误越少，它就越好</strong>。因此，我们的学习目标就是，调整模型的参数 <code>w</code>，使得模型在所有我们“看过的”（即训练集里的）数据上的平均损失达到最小。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719114437.png" alt="image.png|496"></p>
<ul>
<li>$L(…)$ 是<strong>损失函数</strong>，它计算单个样本的预测错误程度。</li>
<li>$\frac{1}{m} \sum_{i=1}^{m} L(…)$ 这部分，即<strong>所有训练样本的平均损失</strong>，被称为“<strong>经验风险 (Empirical Risk)<strong>”。它代表了模型在</strong>已知的训练数据上</strong>表现得有多差。</li>
</ul>
<p>这个准则有一个巨大的风险：如果模型的能力太强（过于复杂），它可能会“<strong>死记硬背</strong>”训练数据，导致在训练集上表现完美（经验风险为 0），但在<strong>没见过的</strong>测试数据上表现极差。这就是所谓的 <strong>过拟合</strong>（Overfitting）</p>
<h3 id="3-2-结构风险最小化-SRM"><a href="#3-2-结构风险最小化-SRM" class="headerlink" title="3.2 结构风险最小化 (SRM)"></a>3.2 结构风险最小化 (SRM)</h3><p><strong>SRM 是对 ERM 的一种改进和泛化</strong>。它通过引入正则化项，在追求“拟合得好”和“模型简单”之间寻找最佳平衡点。</p>
<p>SRM 准则认为，一个好的模型<strong>不应该仅仅在训练数据上表现好，它还应该足够简单</strong>。因此，我们的学习目标变成了一个权衡：</p>
<ol>
<li>一方面要<strong>降低经验风险</strong>，让模型尽可能地拟合训练数据。</li>
<li>另一方面又要<strong>降低模型的复杂度</strong>，防止它变得过于复杂而死记硬背。</li>
</ol>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250719114607.png" alt="image.png"></p>
<ul>
<li>$\frac{1}{m} \sum L(…)$：这部分依然是<strong>经验风险</strong>，代表模型对训练数据的拟合程度。</li>
<li>$\lambda \Phi(w)$：这是新增的<strong>正则项 (Regularization Term)<strong>，也叫</strong>惩罚项</strong>。<ul>
<li>$\Phi(w)$ 是一个用来<strong>衡量模型复杂度</strong>的函数。通常，参数 <code>w</code> 的值越大、越复杂，$\Phi(w)$ 的值就越大。常见的有 L1 正则化和 L2 正则化。</li>
<li>$\lambda$ (lambda) 是一个超参数，用来<strong>控制我们对模型复杂度的惩罚力度</strong>。$\lambda$ 越大，我们对模型的“简单性”要求就越高。</li>
</ul>
</li>
</ul>
<h3 id="3-3-两者的关系与区别"><a href="#3-3-两者的关系与区别" class="headerlink" title="3.3 两者的关系与区别"></a>3.3 两者的关系与区别</h3><table>
<thead>
<tr>
<th>特性</th>
<th>经验风险最小化 (ERM)</th>
<th>结构风险最小化 (SRM)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>目标</strong></td>
<td>只关心在<strong>训练数据上</strong>的损失最小。</td>
<td>同时关心在<strong>训练数据上的损失</strong>和<strong>模型自身的复杂度</strong>。</td>
</tr>
<tr>
<td><strong>公式</strong></td>
<td><code>损失 = 训练误差</code></td>
<td><code>损失 = 训练误差 + 模型复杂度惩罚</code></td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>目标简单直接，易于理解。</td>
<td>能够有效<strong>防止过拟合</strong>，提高模型的<strong>泛化能力</strong>（在新数据上的表现）。</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>在模型复杂时，容易导致<strong>过拟合</strong>。</td>
<td>需要额外调整一个超参数 <code>λ</code>。</td>
</tr>
</tbody></table>
<h1 id="寻找最优解：梯度下降-Gradient-Descent"><a href="#寻找最优解：梯度下降-Gradient-Descent" class="headerlink" title="寻找最优解：梯度下降 (Gradient Descent)"></a>寻找最优解：梯度下降 (Gradient Descent)</h1><h2 id="1-超越方程"><a href="#1-超越方程" class="headerlink" title="1 超越方程"></a>1 超越方程</h2><p><strong>超越方程（Transcendental Equation）</strong> 是指含有<strong>超越函数</strong>的方程。与我们熟悉的代数方程相比，它更为复杂，通常也更难求解。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721143711.png" alt="image.png"></p>
<p>因此，只要一个方程中包含了上述这些超越函数，并且这些函数无法被代数运算消除，那么这个方程就是<strong>超越方程</strong>。</p>
<h2 id="2-核心思想：最快下山路径"><a href="#2-核心思想：最快下山路径" class="headerlink" title="2 核心思想：最快下山路径"></a>2 核心思想：最快下山路径</h2><p>想象一下你站在一座连绵起伏的山脉上，眼睛被蒙住，你的目标是走到山谷的最低点。你该怎么办？</p>
<p>一个非常符合直觉的策略是：</p>
<ol>
<li>用脚在当前位置的四周试探，找到最陡峭的下坡方向。</li>
<li>朝着这个最陡峭的方向，迈出一小步。</li>
<li>重复以上过程，不断地试探、下行，直到你感觉自己已经到达了山谷的最低点（脚下已经是平地了）。</li>
</ol>
<p>梯度下降就是这个“蒙眼下山”过程的数学化身。</p>
<ul>
<li><strong>山脉</strong>：在机器学习中，这代表<strong>损失函数（Loss Function）</strong> 或<strong>成本函数（Cost Function）</strong>。这个函数用来衡量模型的预测结果与真实情况之间的差距。损失值越大，代表模型“越差”。</li>
<li><strong>你所在的位置</strong>：代表模型当前的<strong>参数</strong>（也称为权重和偏置）。</li>
<li><strong>山谷的最低点</strong>：代表损失函数的<strong>最小值</strong>，也就是模型参数的<strong>最优解</strong>。我们的目标就是找到这组参数。</li>
<li><strong>最陡峭的下坡方向</strong>：在数学中，<strong>梯度（Gradient）</strong> 是一个向量，指向函数值增长最快的方向。因此，梯度的<strong>反方向</strong>就是函数值下降最快的方向。</li>
<li><strong>迈出的一小步</strong>：这被称为<strong>学习率（Learning Rate）</strong>，它控制着我们每一步更新参数的幅度。</li>
</ul>
<p><strong>因此，梯度下降的本质是：一种通过迭代，沿着损失函数梯度下降最快的方向，逐步调整模型参数，最终找到损失函数最小值的优化算法。</strong></p>
<h2 id="3-梯度下降的数学步骤（以线性回归为例）"><a href="#3-梯度下降的数学步骤（以线性回归为例）" class="headerlink" title="3 梯度下降的数学步骤（以线性回归为例）"></a>3 梯度下降的数学步骤（以线性回归为例）</h2><p>假设我们有一个非常简单的数据集，我们想找到最佳的直线 $y=wx+b$ 来拟合它。</p>
<blockquote>
<p> 其中 w（权重）和 b（偏置）是我们想学习的参数。</p>
</blockquote>
<p>选择一个函数来衡量所有预测值与真实值之间的总差距。这里我们使用均方误差（Mean Squared Error, MSE）作为<strong>损失函数</strong></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721144221.png" alt="image.png"></p>
<p>我们的目标是找到能让 $J(w,b)$ 最小的 $w$ 和 $b$</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721144419.png" alt="image.png"></p>
<p>其中学习率是一个<strong>极其敏感且重要</strong>的超参数，<strong>太小则收敛慢，太大则不稳定甚至不收敛</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721150933.png" alt="image.png"></p>
<blockquote>
<p> 上图使用 $\eta$ 记为学习率</p>
</blockquote>
<h2 id="4-梯度下降的类型"><a href="#4-梯度下降的类型" class="headerlink" title="4 梯度下降的类型"></a>4 梯度下降的类型</h2><p>根据每次迭代计算梯度时使用的数据量不同，梯度下降分为三种主要类型：</p>
<ol>
<li><p><strong>批量梯度下降 (Batch Gradient Descent, BGD)</strong></p>
<ul>
<li><strong>工作方式</strong>：在每次更新参数时，使用<strong>全部</strong>训练数据来计算梯度。</li>
<li><strong>优点</strong>：梯度计算得最准确，下降方向稳定，能够保证收敛到局部或全局最小值。</li>
<li><strong>缺点</strong>：当数据集非常大时（例如百万级样本），每次迭代的计算成本极高，速度非常慢。<br>  <img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721150654.png" alt="image.png"></li>
</ul>
</li>
<li><p><strong>随机梯度下降 (Stochastic Gradient Descent, SGD)</strong></p>
<ul>
<li><strong>工作方式</strong>：每次更新参数时，<strong>只随机选择一个</strong>训练样本来计算梯度。</li>
<li><strong>优点</strong>：更新速度极快，计算开销小。其随机性有时反而能帮助算法“跳出”糟糕的局部最小值。</li>
<li><strong>缺点</strong>：由于每次只看一个样本，梯度方向的噪音很大，导致收敛过程非常不稳定，<strong>损失函数会剧烈震荡，并且最终只会在最优点附近徘徊，难以精确收敛</strong>。<br>  <img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721150710.png">]()</li>
</ul>
</li>
<li><p><strong>小批量梯度下降 (Mini-batch Gradient Descent)</strong></p>
<ul>
<li><strong>工作方式</strong>：这是 BGD 和 SGD 的完美折中。每次更新时，使用一小批（比如 32、64 或 128 个）训练样本来计算梯度。</li>
<li><strong>优点</strong>：<ul>
<li>结合了 BGD 的稳定性和 SGD 的高效性。</li>
<li>可以充分利用现代计算硬件（如 GPU）的并行计算能力，计算效率非常高。</li>
</ul>
</li>
<li><strong>现状</strong>：这是目前训练深度学习等复杂模型时<strong>最常用</strong>的方法。我们通常说的 SGD，在实际应用中往往指的就是小批量梯度下降。</li>
</ul>
</li>
</ol>
<h2 id="5-挑战与现代优化器"><a href="#5-挑战与现代优化器" class="headerlink" title="5 挑战与现代优化器"></a>5 挑战与现代优化器</h2><p>标准的梯度下降也面临一些挑战：</p>
<ul>
<li><strong>局部最小值（Local Minima）</strong>：如果损失函数的形状很复杂（非凸函数），算法可能会卡在一个“小山谷”里，误以为是最低点，而错过了真正的全局最低点。</li>
<li><strong>鞍点（Saddle Points）</strong>：在高维空间中，比局部最小值更常见的是鞍点。在一个方向上它是最小值，在另一个方向上却是最大值。梯度在鞍点处为零，可能导致训练停滞。</li>
<li><strong>梯度消失/爆炸</strong>：在深度神经网络中，梯度在逐层反向传播时可能会变得极小（消失）或极大（爆炸），导致训练困难。</li>
</ul>
<p>为了解决这些问题，研究者们提出了许多更先进的优化算法，它们都是基于梯度下降的改进：</p>
<ul>
<li>**Momentum (动量)**：引入了“动量”的概念，就像一个从山上滚下来的球。它不仅考虑当前梯度，还积累了之前的速度，有助于冲过平坦区域和小的局部最小值，加速收 vergne。</li>
<li>**RMSprop (Root Mean Square Propagation)**：为每个参数自适应地调整学习率。对于梯度较大的参数，它会减小学习率；对于梯度较小的参数，它会增大学习率。这有助于在不同方向上平衡学习速度。</li>
<li><strong>Adam (Adaptive Moment Estimation)<strong>：</strong>目前最流行、最常用的优化器之一</strong>。它巧妙地结合了 Momentum 和 RMSprop 的优点，既有动量来加速，又能为每个参数自适应地调整学习率。在大多数情况下，Adam 都能取得非常好的效果。</li>
</ul>
<h1 id="解决过拟合：正则化-Regularization"><a href="#解决过拟合：正则化-Regularization" class="headerlink" title="解决过拟合：正则化 (Regularization)"></a>解决过拟合：正则化 (Regularization)</h1><h2 id="1-过拟合与欠拟合"><a href="#1-过拟合与欠拟合" class="headerlink" title="1 过拟合与欠拟合"></a>1 过拟合与欠拟合</h2><p>欠拟合、过拟合主要和模型复杂度相关：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722142130.png" alt="image.png|582"></p>
<p>增加模型复杂度可以解决欠拟合问题，当然过度增加会导致过拟合问题：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722142935.png" alt="image.png"></p>
<p><strong>过拟合的根源</strong>：通常是因为<strong>模型过于复杂</strong>。</p>
<p>类比多项式逼近思想，就是用一条<strong>非常弯曲、高次数</strong>的多项式曲线去拟合几个本可以用直线大致描述的点。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721155605.png" alt="|757"></p>
<p>过拟合问题是机器学习的关键问题，可以通过增大数据集和减小模型复杂度来解决。</p>
<p>但一般而言，<strong>数据集是很难增大的</strong>，更多是考虑减小模型复杂度。</p>
<p>我们需要一种方法，来<strong>限制模型的复杂度</strong>，强迫它去学习更简单、更通用的规律。<strong>正则化</strong>就是实现这个目标的“纪律约束”。</p>
<h2 id="2-正则化项-罚项"><a href="#2-正则化项-罚项" class="headerlink" title="2 正则化项/罚项"></a>2 正则化项/罚项</h2><p>正则化是一种在模型训练过程中，通过在损失函数中加入“惩罚项”，来系统性地降低模型复杂度、避免过拟合的技术。</p>
<p>它的核心思想是：在模型努力<strong>最小化训练误差</strong>（拟合数据）的同时，也必须努力<strong>保持自身足够简单</strong>。</p>
<p>下面再通过图像来进一步认识下罚项的作用。对于抛物线假设空间</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722144248.png" alt="image.png"></p>
<p>没有条件的时候意味着：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722144259.png" alt="image.png"></p>
<p>也就是说 $\boldsymbol{w}$ 可以在 $w_0w_1$ 这个二维平面内任意取点：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722144325.png" alt="image.png"></p>
<p>而加上条件 $||\boldsymbol{w}||^2=w_0^2+w_1^2\le r^2$ 后，$\boldsymbol{w}$ 就只能在半径为 r 的圆内取点：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722144402.png" alt="image.png"></p>
<p>因此罚项对 $\boldsymbol{w}$ 的取值范围进行了“惩罚”，使得其不能任意取值。而假设空间 $\mathcal{H}_c$ 是由 $\boldsymbol{w}$ 决定的，所以罚项实际上就是缩小了假设空间 $\mathcal{H}_c$ 的大小，降低了假设空间 $\mathcal{H}_c$ 的复杂度。</p>
<blockquote>
<p> 在数学中，这就是求解有条件的极值，可以通过拉格朗日乘数法以及拉格朗日乘数法的 KKT 条件得到拉格朗日函数，对该拉格朗日函数求极值相当于求解上述的条件极值，从而得到 $\hat{\boldsymbol{w}}$</p>
</blockquote>
<h2 id="3-范数-Norm-：量化模型复杂度"><a href="#3-范数-Norm-：量化模型复杂度" class="headerlink" title="3 范数 (Norm)：量化模型复杂度"></a>3 范数 (Norm)：量化模型复杂度</h2><p>现在的问题是，我们如何用数学语言来“量化一个模型的复杂度”呢？答案就是范数。</p>
<p><strong>范数是一种衡量向量“大小”或“长度”的数学工具。</strong> 在正则化中，我们用模型<strong>权重向量 <code>w</code> 的范数</strong>来作为模型复杂度的代理指标。</p>
<p><strong>直观理解</strong>：一个模型的权重 <code>w</code> 值普遍很大，意味着模型可能非常复杂、抖动剧烈。相反，如果权重值都比较小，模型通常更简单、更平滑。</p>
<h3 id="3-1-为什么“权重向量-w-的范数”可以用来代表“模型的复杂度”？"><a href="#3-1-为什么“权重向量-w-的范数”可以用来代表“模型的复杂度”？" class="headerlink" title="3.1 为什么“权重向量 w 的范数”可以用来代表“模型的复杂度”？"></a>3.1 为什么“权重向量 <code>w</code> 的范数”可以用来代表“模型的复杂度”？</h3><p><span style="color: #badc58; font-weight: 550;">答案的核心在于：</span>一个模型的权重大小，直接决定了其函数图像的“陡峭”或“抖动”程度，而一个“抖动”剧烈的函数，就是我们所说的复杂函数。</p>
<p>对于多元线性回归模型 $y=w_0​+w_1​x_1​+⋯+w_n​x_n​$。</p>
<ul>
<li>这里的每一个权重 <code>wᵢ</code> 都扮演着类似的角色：它决定了模型的输出 <code>y</code> 对特征 <code>xᵢ</code> 的敏感程度。</li>
<li>如果 <code>wᵢ</code> 很大，那么特征 <code>xᵢ</code> 的<strong>微小变化</strong>都会对最终结果产生巨大影响。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">结论：</span>权重向量 <code>w</code> 的范数，是<strong>模型整体敏感度的量化指标</strong></p>
<p><span style="color: #badc58; font-weight: 550;">现在我们回答最关键的问题：</span>为什么<strong>敏感</strong>的模型就是复杂的、更容易过拟合的模型？</p>
<p>想象一下一个<strong>过拟合</strong>的模型。它的任务是穿过训练集中的每一个数据点，包括那些因噪音而偏离正常轨道的点。</p>
<ul>
<li><strong>要做到这一点，模型函数必须是什么样的？</strong> <ul>
<li>它必须是一个抖动非常剧烈、极其“扭曲”的函数。它需要在某个地方急剧上升，以穿过一个偏高的点，然后又在另一个地方急剧下降，以穿过一个偏低的点。</li>
</ul>
</li>
<li><strong>什么样的权重 <code>w</code> 才能产生这样抖动的函数？</strong> <ul>
<li>只有<strong>非常大</strong>的权重才能产生如此陡峭的变化。一个所有权重都很小的模型，其函数图像必然是平滑、缓和的，它无法做到这种“上蹿下跳”的精细拟合。</li>
</ul>
</li>
</ul>
<h3 id="3-2-L1-范数和-L2-范数"><a href="#3-2-L1-范数和-L2-范数" class="headerlink" title="3.2 L1 范数和 L2 范数"></a>3.2 L1 范数和 L2 范数</h3><p>最常用的两种范数是 L1 范数和 L2 范数：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722150520.png" alt="image.png"></p>
<p>更一般的情况</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721160542.png" alt="image.png|695"></p>
<h2 id="4-L1-正则化"><a href="#4-L1-正则化" class="headerlink" title="4 L1 正则化"></a>4 L1 正则化</h2><p>损失函数:</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721161720.png" alt="image.png|437"></p>
<ul>
<li><strong>目标</strong>：这个公式表达了 L1 正则化的优化目标。我们需要找到一组最优的权重 <code>w*</code>，它能让两项之和最小。</li>
<li><strong>第一项 <code>MSE(...)</code><strong>：这是模型的</strong>损失函数</strong>（此处为均方误差）。它代表了模型拟合数据的能力。这一项越小，说明模型对训练数据的拟合越好。</li>
<li>**第二项 <code>λ Σ|wᵢ|</code>**：这是 <strong>L1 正则化项</strong>。<ul>
<li><code>Σ|wᵢ|</code> 是模型所有权重 <code>w</code> 的绝对值之和，也就是 <strong>L1 范数</strong>。</li>
<li><code>λ</code> 是一个超参数，控制着正则化的<strong>强度</strong>。<code>λ</code> 越大，对权重的“惩罚”就越重。</li>
</ul>
</li>
<li><strong>核心权衡</strong>：模型必须在“<strong>好好拟合数据</strong>”（最小化 MSE）和“<strong>保持自身简单</strong>”（最小化 L1 范数）之间找到一个平衡。</li>
</ul>
<p>损失函数并没有写 $Σ|wᵢ| ≤ r$，而是 $λ * Σ|wᵢ|$ 是因为：</p>
<ul>
<li><strong><code>λ</code> (lambda) 就是罚款的“税率”</strong>。</li>
<li>模型可以自由地选择任何 <code>w</code> 值，但是 <code>w</code> 的 L1 范数（<code>Σ|wᵢ|</code>）越大，它需要付出的“罚款” (<code>λ * Σ|wᵢ|</code>) 就越多。</li>
<li><code>λ</code> 越大，税率越高，模型为了让总成本（损失 + 罚款）最小，就越不敢让自己的权重变得太大。</li>
<li><code>λ</code> 越小，税率越低，模型就更愿意选择较大的权重来努力降低 <code>MSE</code>。</li>
</ul>
<blockquote>
<p> 我们是要求损失函数的最小值，所以这里是<strong>加号</strong></p>
</blockquote>
<p>以 $\boldsymbol{w}=(w_0,w_1)$ 为例，$L_1$ 范数：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722153229.png" alt="image.png"></p>
<p>的意义是将 $\boldsymbol{w}$ 限制在如下的正方形内（$2r$ 就是该正方形的对角线长度）：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722153239.png" alt="image.png"></p>
<p>未加罚项之前，<strong>损失函数</strong>为凸函数，画出来类似下面左图，其最小值为左图中最下面的红点；或者在下面右图将它的等高线在 $w_0w_1$ 坐标系中画出，最小值为中间的红点：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722151623.png" alt="image.png"></p>
<p>加了罚项 $L_1$ 范数之后，$\boldsymbol{w}$ 被限制在正方形内</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722151845.png" alt="image.png|629"></p>
<p>首先，我们再明确一下优化目标：</p>
<ul>
<li>我们有一个“目标点”：<strong>损失函数（MSE）的最小值点</strong>，也就是图中椭圆等高线的中心。我们的模型“本能地”想要到达这个点。</li>
<li>我们有一个“限制区”：<strong>L1 约束区域</strong>，也就是那个菱形/正方形。我们的模型被告知<strong>不能离开</strong>这个区域。</li>
</ul>
<p>最终的解，就是<strong>在不离开限制区的前提下，尽可能地靠近目标点</strong>。</p>
<ul>
<li>求解过程等价于寻找损失函数的椭圆等高线与该区域的<strong>第一个接触点</strong>。</li>
<li>在几何上，这表现为<strong>损失函数的椭圆等高线，从中心点开始不断“膨胀”，直到第一次接触到正方形限制区</strong>的那个点。</li>
</ul>
<p>因为正方形（L1 约束）的四个角是“凸”出来的，当膨胀的椭圆边界靠近时，它会<strong>首先碰到离它最近的那个顶点</strong>，而不是平坦的边。</p>
<p>所以最终解出的 $\hat{\boldsymbol{w}}$ 大概率为正方形的 4 个顶点（或附近），即下图中红色的点：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722152218.png" alt="image.png"></p>
<p>这 4 个红色的点分别为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722152233.png" alt="image.png"></p>
<p>因为顶点位于坐标轴上，所以一旦解落在顶点，就意味着<strong>某个权重参数的值为 0</strong>，从而产生了<strong>稀疏解</strong>，实现了特征选择。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722152631.png" alt="image.png|594"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 必要的准备代码 (Necessary Setup Code) ---</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并准备数据</span></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">x = boston.data[:,<span class="number">12</span>]</span><br><span class="line">y = boston.target</span><br><span class="line">x = x[y&lt;<span class="number">50</span>]</span><br><span class="line">y = y[y&lt;<span class="number">50</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(x.reshape(-<span class="number">1</span>,<span class="number">1</span>), y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数并训练Lasso模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">LassoRegression</span>(<span class="params">degree, alpha</span>):</span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        (<span class="string">&quot;poly&quot;</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">        (<span class="string">&quot;std_scaler&quot;</span>, StandardScaler()),</span><br><span class="line">        (<span class="string">&quot;lasso_reg&quot;</span>, Lasso(alpha=alpha))</span><br><span class="line">    ])</span><br><span class="line">lasso_reg = LassoRegression(<span class="number">20</span>, <span class="number">0.001</span>)</span><br><span class="line">lasso_reg.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># -------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数，用于绘制原始数据点和模型拟合的曲线</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_plot</span>(<span class="params">model</span>):</span><br><span class="line">    <span class="comment"># 绘制原始数据的散点图</span></span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    <span class="comment"># 创建一个从0到40的平滑的x轴数据点，用于绘制模型的拟合曲线</span></span><br><span class="line">    X_plot = np.linspace(<span class="number">0</span>, <span class="number">40</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 使用训练好的模型对这些平滑的x点进行预测</span></span><br><span class="line">    y_plot = model.predict(X_plot)</span><br><span class="line">    <span class="comment"># 绘制模型的拟合曲线，红色</span></span><br><span class="line">    plt.plot(X_plot[:,<span class="number">0</span>], y_plot, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    <span class="comment"># 设置图表的坐标轴范围 [xmin, xmax, ymin, ymax]</span></span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">40</span>, <span class="number">0</span>, <span class="number">50</span>])</span><br><span class="line">    <span class="comment"># 显示图形</span></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用绘图函数，可视化我们训练的Lasso回归模型的拟合效果</span></span><br><span class="line">show_plot(lasso_reg)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722165024.png" alt="image.png|461"></p>
<h2 id="5-L2-正则化"><a href="#5-L2-正则化" class="headerlink" title="5 L2 正则化"></a>5 L2 正则化</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722153854.png" alt="image.png"></p>
<p>以 $\boldsymbol{w}=(w_0,w_1)$ 为例，$L_2$ 范数：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722153916.png"></p>
<p>的意义是将 $\boldsymbol{w}$ 限制在半径为 $r$ 的圆内：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722154054.png" alt="image.png"></p>
<p>类比 L1 正则化</p>
<ul>
<li><strong>圆形（L2 约束）</strong>：它的边界是<strong>平滑、均匀</strong>的。从圆心到边界上任何一点的“突出程度”都完全一样。当椭圆膨胀时，它可以平滑地与圆形的任何一点相切，这个切点的位置是随机的，<strong>没有哪个点比其他点更“特殊”</strong>。因此，切点几乎不可能正好落在坐标轴上。</li>
<li>当然也可能圆形包含了凸优化的最小值点，所以极值仍然有可能取在最小值点（下图的右侧）：</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722154538.png" alt="image.png"></p>
<p>所以说最终解出的 $\hat{\boldsymbol{w}}$ 大概率会在圆边上，这样导致的后果是，$w_0$ 和 $w_1$ 不会相差太大</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722154731.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 必要的准备代码 (Necessary Setup Code) ---</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge <span class="comment"># 导入Ridge</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并准备波士顿房价数据</span></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">x = boston.data[:,<span class="number">12</span>]</span><br><span class="line">y = boston.target</span><br><span class="line">x = x[y&lt;<span class="number">50</span>]</span><br><span class="line">y = y[y&lt;<span class="number">50</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(x.reshape(-<span class="number">1</span>,<span class="number">1</span>), y, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个绘图函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_plot</span>(<span class="params">model</span>):</span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    X_plot = np.linspace(<span class="number">0</span>, <span class="number">40</span>, <span class="number">100</span>).reshape(<span class="number">100</span>, <span class="number">1</span>)</span><br><span class="line">    y_plot = model.predict(X_plot)</span><br><span class="line">    plt.plot(X_plot[:,<span class="number">0</span>], y_plot, color=<span class="string">&#x27;red&#x27;</span>)</span><br><span class="line">    plt.axis([<span class="number">0</span>, <span class="number">40</span>, <span class="number">0</span>, <span class="number">50</span>])</span><br><span class="line">    plt.show()</span><br><span class="line"><span class="comment"># -------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个函数来创建Ridge回归的模型流水线</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">RidgeRegression</span>(<span class="params">degree, alpha</span>):</span><br><span class="line">    <span class="keyword">return</span> Pipeline([</span><br><span class="line">        <span class="comment"># 1. 创建多项式特征，degree控制复杂度</span></span><br><span class="line">        (<span class="string">&quot;poly&quot;</span>, PolynomialFeatures(degree=degree)),</span><br><span class="line">        <span class="comment"># 2. 对特征进行标准化处理</span></span><br><span class="line">        (<span class="string">&quot;std_scaler&quot;</span>, StandardScaler()),</span><br><span class="line">        <span class="comment"># 3. 使用Ridge回归模型，alpha是L2正则化强度</span></span><br><span class="line">        (<span class="string">&quot;ridge_reg&quot;</span>, Ridge(alpha=alpha))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个20次多项式的Ridge回归模型实例</span></span><br><span class="line"><span class="comment"># degree=20: 创造一个非常复杂的、容易过拟合的场景</span></span><br><span class="line"><span class="comment"># alpha=100: 设置一个很强的L2正则化强度来约束模型</span></span><br><span class="line">ridge_reg = RidgeRegression(<span class="number">20</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练数据训练Ridge模型</span></span><br><span class="line">ridge_reg.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练好的模型对测试集进行预测</span></span><br><span class="line">y_predict = ridge_reg.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算并输出模型在测试集上的均方误差</span></span><br><span class="line">mean_squared_error(y_test, y_predict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用绘图函数，可视化Ridge回归模型的拟合效果</span></span><br><span class="line">show_plot(ridge_reg)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722165050.png" alt="image.png"></p>
<h2 id="6-弹性网络-Elastic-Net"><a href="#6-弹性网络-Elastic-Net" class="headerlink" title="6 弹性网络 (Elastic Net)"></a>6 弹性网络 (Elastic Net)</h2><p>弹性网络认识到 L1 和 L2 各有优劣，于是将它们<strong>集于一身</strong>，创造出一个更通用的正则化方法。它的惩罚项是 L1 和 L2 惩罚项的<strong>加权组合</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250724105428.png" alt="image.png"></p>
<p><strong>核心特点：兼具 L1 和 L2 的优点</strong></p>
<ul>
<li>通过 L1 部分，弹性网络可以像 Lasso 一样进行<strong>特征选择</strong>，生成稀疏模型。</li>
<li>通过 L2 部分，弹性网络可以像岭回归一样处理<strong>高度相关的特征</strong>，倾向于将它们作为一个整体保留或剔除，而不是像 L1 那样随机选择一个。</li>
</ul>
<p><strong>优点</strong>：</p>
<ul>
<li>结合了 L1 的特征选择能力和 L2 的稳定性。</li>
<li>在特征数量远大于样本数量，或者特征之间存在高度相关性的情况下，表现通常优于单独的 L1 或 L2。</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>需要调整两个超参数（整体惩罚力度 λ 和混合比例 α），调参过程更复杂。</li>
</ul>
<h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7 总结"></a>7 总结</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721164300.png" alt="image.png|654"></p>
<table>
<thead>
<tr>
<th>特性</th>
<th>L1 正则 (Lasso)</th>
<th>L2 正则 (Ridge)</th>
<th>弹性网络 (Elastic Net)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>惩罚项</strong></td>
<td>参数绝对值之和（L1​范数）</td>
<td>参数平方和（L2​范数）</td>
<td>L1 和 L2 惩罚项的加权组合</td>
</tr>
<tr>
<td><strong>参数收缩</strong></td>
<td>可将参数压缩至<strong>精确为零</strong></td>
<td>将参数向零收缩，但<strong>不为零</strong></td>
<td>结合两者，可产生稀疏解</td>
</tr>
<tr>
<td><strong>特征选择</strong></td>
<td><strong>是</strong>，自动选择特征</td>
<td><strong>否</strong>，保留所有特征</td>
<td><strong>是</strong>，可以进行特征选择</td>
</tr>
<tr>
<td><strong>处理相关特征</strong></td>
<td>倾向于随机选择一个</td>
<td>将相关特征赋予相似的权重</td>
<td>倾向于将相关特征作为一组保留或剔除</td>
</tr>
<tr>
<td><strong>关系</strong></td>
<td>基础正则化方法</td>
<td>基础正则化方法</td>
<td><strong>L1 和 L2 的结合与推广</strong></td>
</tr>
</tbody></table>
<h1 id="模型泛化-Model-Generalization"><a href="#模型泛化-Model-Generalization" class="headerlink" title="模型泛化 (Model Generalization)"></a>模型泛化 (Model Generalization)</h1><blockquote>
<p> <strong>模型泛化</strong>指的是一个训练好的模型在<strong>未曾见过的新数据</strong>上的表现能力。一个泛化能力强的模型，意味着它真正学到了数据背后普适的规律，而不仅仅是“死记硬背”了训练数据。</p>
</blockquote>
<p>在机器学习中，我们追求的不是一个在训练集上得满分的模型，而是一个泛化能力强的模型。</p>
<h2 id="1-泛化的度量：模型误差-Model-Error"><a href="#1-泛化的度量：模型误差-Model-Error" class="headerlink" title="1 泛化的度量：模型误差 (Model Error)"></a>1 泛化的度量：模型误差 (Model Error)</h2><p>既然泛化是目标，我们如何量化一个模型的泛化能力好不好呢？答案就是通过衡量误差。</p>
<h3 id="1-1-模型误差的构成"><a href="#1-1-模型误差的构成" class="headerlink" title="1.1 模型误差的构成"></a>1.1 模型误差的构成</h3><p>一个模型的<strong>总误差</strong>可以被分解为三个主要部分：<strong>偏差</strong>（Bias）、<strong>方差</strong>（Variance）和<strong>噪声</strong>（Noise）。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721154555.png" alt="image.png|568"></p>
<ul>
<li><strong>偏差 (Bias)</strong>: 代表<strong>算法的拟合能力</strong>。高偏差意味着模型过于简单，无法捕捉数据的真实规律（<strong>欠拟合</strong>）。</li>
<li><strong>方差 (Variance)</strong>: 代表<strong>数据扰动的影响力</strong>。高方差意味着模型对训练数据过于敏感，一点变动就让模型结果产生巨大差异（<strong>过拟合</strong>）。</li>
<li><strong>噪声 (Noise)</strong>: 代表<strong>任务本身的内在难度</strong>（数据中不可避免的随机性），是任何模型都无法消除的误差下限。</li>
</ul>
<p>理想情况是 <strong>低偏差、低方差</strong>，这样模型误差才小</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721154359.png" alt="image.png|861"></p>
<ul>
<li><strong>高方差 (High Variance)<strong>：箭落点</strong>分散</strong>。</li>
<li><strong>低方差 (Low Variance)<strong>：箭落点</strong>集中</strong>。</li>
<li><strong>高偏差 (High Bias)<strong>：箭落点整体</strong>偏离靶心</strong>。</li>
<li><strong>低偏差 (Low Bias)<strong>：箭落点整体</strong>围绕靶心</strong>。</li>
</ul>
<h3 id="1-2-偏差和方差的关系：此消彼长的权衡"><a href="#1-2-偏差和方差的关系：此消彼长的权衡" class="headerlink" title="1.2 偏差和方差的关系：此消彼长的权衡"></a>1.2 偏差和方差的关系：此消彼长的权衡</h3><ul>
<li><strong>偏差曲线 (Bias²)</strong>: 随着训练程度加深（或模型变复杂），偏差会<strong>持续下降</strong>。</li>
<li><strong>方差曲线 (Variance)</strong>: 随着训练程度加深，方差会<strong>持续上升</strong>。</li>
<li><strong>总体误差曲线 (Total Error)</strong>: 它是偏差和方差之和，呈现出一条 <strong>U 形曲线</strong>。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721155023.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">核心结论:</span></p>
<ul>
<li><strong>偏差和方差是矛盾的</strong>：降低偏差（让模型更复杂以更好地拟合数据）往往会导致方差的提高；而降低方差（让模型更简单以抵抗数据扰动）又会带来偏差的提高。</li>
<li><strong>最佳模型</strong>: 我们的目标不是追求零偏差或零方差，而是在这个权衡中找到一个<strong>平衡点</strong>，使得<strong>总体误差最小</strong>（U 形曲线的谷底）。</li>
</ul>
<h3 id="1-3-偏差、方差和模型复杂度的关系"><a href="#1-3-偏差、方差和模型复杂度的关系" class="headerlink" title="1.3 偏差、方差和模型复杂度的关系"></a>1.3 偏差、方差和模型复杂度的关系</h3><p>这部分将“训练程度”这个概念更具体化为“模型复杂度”。横轴从左到右，代表模型从简单到复杂。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721155005.png" alt="image.png|708"></p>
<p><strong>不同算法有不同的“天性”</strong>:</p>
<ul>
<li><strong>非参数算法</strong>（如 kNN）通常是<strong>低偏差、高方差</strong>的模型，它们非常灵活，能拟合复杂数据，但也容易过拟合。</li>
<li><strong>参数算法</strong>（如线性回归）通常是<strong>高偏差、低方差</strong>的模型，它们结构简单，表现稳定，但可能无法捕捉复杂规律。</li>
</ul>
<p><strong>偏差和方差通常是可调节的</strong>: 我们可以通过调整模型的<strong>超参数</strong>（比如 kNN 中的 k 值，或多项式回归的次数），来控制模型的复杂度，从而在偏差 - 方差曲线上移动，寻找最优的平衡点。</p>
<h2 id="2-诊断泛化问题的工具：学习曲线-Learning-Curve"><a href="#2-诊断泛化问题的工具：学习曲线-Learning-Curve" class="headerlink" title="2 诊断泛化问题的工具：学习曲线 (Learning Curve)"></a>2 诊断泛化问题的工具：学习曲线 (Learning Curve)</h2><h3 id="2-1-核心思想"><a href="#2-1-核心思想" class="headerlink" title="2.1 核心思想"></a>2.1 核心思想</h3><p>我们如何动态地观察这两种误差的变化，并诊断出模型是否存在泛化问题（如过拟合或欠拟合）呢？这就需要用到学习曲线。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721153003.png" alt="image.png"></p>
<ul>
<li><strong>坐标轴</strong>:<ul>
<li><strong>纵轴 (Y-axis)</strong>: <code>误差 (Error)</code>，代表模型的损失值。</li>
<li><strong>横轴 (X-axis)</strong>: <code>训练集大小 m</code>，代表用于训练模型的数据量。</li>
</ul>
</li>
<li><strong>两条曲线</strong>:<ul>
<li><strong><code>J_train(θ)</code> (训练误差)</strong>: 模型在它“学习过”的数据上的表现。</li>
<li><strong><code>J_test(θ)</code> (测试误差)</strong>: 模型在它“没见过”的新数据上的表现，代表了模型的泛化能力。</li>
</ul>
</li>
</ul>
<table>
<thead>
<tr>
<th>情况</th>
<th>曲线特征</th>
<th>诊断</th>
<th>主要对策 (怎么办？)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>高偏差 (欠拟合)</strong></td>
<td>1. 训练和验证损失都<strong>收敛于高位</strong>。  <br>2. 两条曲线最终<strong>差距很小</strong>。</td>
<td>模型过于简单，未能充分学习数据规律，在训练集和验证集上表现均不佳。</td>
<td>• <strong>增加模型复杂度</strong> (如增加网络层数/神经元)。  <br>• <strong>添加/构造新特征</strong>。  <br>• <strong>减少正则化</strong>强度。  <br>—  <br>• <strong>禁忌</strong>: 此时增加训练数据通常无效。</td>
</tr>
<tr>
<td><strong>高方差 (过拟合)</strong></td>
<td>1. 训练损失<strong>很低</strong>，但验证损失<strong>很高</strong>。  <br>2. 两条曲线之间存在<strong>巨大差距</strong>。</td>
<td>模型几乎“背诵”了训练数据，对新数据的泛化能力很差。</td>
<td>• <strong>增加训练数据</strong> (最有效的方法之一)。  <br>• <strong>数据增强</strong> (Data Augmentation)。  <br>• <strong>增加正则化</strong> (如 Dropout, L1/L2)。  <br>• <strong>使用更简单的模型</strong>。  <br>• **早停 (Early Stopping)**。</td>
</tr>
<tr>
<td><strong>理想状态</strong></td>
<td>1. 训练和验证损失都<strong>收敛于低位</strong>。  <br>2. 两条曲线最终<strong>差距很小</strong>。</td>
<td>模型学习充分，并且在新数据上同样表现出色，泛化能力强。</td>
<td>• 模型状态良好，训练成功。  <br>• 保存模型以备使用。</td>
</tr>
</tbody></table>
<h3 id="2-2-代码实现"><a href="#2-2-代码实现" class="headerlink" title="2.2 代码实现"></a>2.2 代码实现</h3><p>下面使用的是 <strong><code>mean_squared_error</code> (均方误差, MSE)</strong> 来评估模型。</p>
<ul>
<li><strong>含义</strong>: MSE 衡量的是预测值与真实值之间<strong>差值的平方的平均值</strong>。</li>
<li><strong>解读</strong>: 它是一个<strong>误差</strong>指标，所以<strong>值越小越好</strong>。它的单位是 y 的单位的平方，不那么直观。</li>
</ul>
<figure class="highlight python"><figcaption><span>fold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 必要的准备代码 (Necessary Setup Code) ---</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures, StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并准备波士顿房价数据</span></span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line">x = boston.data[:,<span class="number">12</span>]</span><br><span class="line">y = boston.target</span><br><span class="line">x = x[y&lt;<span class="number">50</span>]</span><br><span class="line">y = y[y&lt;<span class="number">50</span>]</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">666</span>)</span><br><span class="line"><span class="comment"># -------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置最终生成图形的大小</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;figure.figsize&quot;</span>] = (<span class="number">12</span>, <span class="number">8</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个要进行比较的多项式次数列表</span></span><br><span class="line">degrees = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">20</span>]</span><br><span class="line"><span class="comment"># 遍历不同的多项式次数，为每一种复杂度的模型绘制学习曲线</span></span><br><span class="line"><span class="keyword">for</span> i, degree <span class="keyword">in</span> <span class="built_in">enumerate</span>(degrees):</span><br><span class="line">    <span class="comment"># --- 1. 为当前次数创建多项式特征 ---</span></span><br><span class="line">    <span class="comment"># 创建多项式特征转换器</span></span><br><span class="line">    polynomial_features = PolynomialFeatures(degree = degree)</span><br><span class="line">    <span class="comment"># 将训练集和测试集的x值转换为多项式特征</span></span><br><span class="line">    <span class="comment"># 注意：更严谨的做法是在训练集上fit_transform, 在测试集上仅transform</span></span><br><span class="line">    X_poly_train = polynomial_features.fit_transform(x_train.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    X_poly_test = polynomial_features.fit_transform(x_test.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># --- 2. 手动生成学习曲线数据 ---</span></span><br><span class="line">    <span class="comment"># 初始化用于存储训练误差和测试误差的列表</span></span><br><span class="line">    train_error, test_error = [], []</span><br><span class="line">    <span class="comment"># 遍历训练集，模拟训练样本数量从1逐渐增加到m的全过程</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_train)):</span><br><span class="line">        <span class="comment"># 创建一个新的线性回归模型实例</span></span><br><span class="line">        linear_regression = LinearRegression()</span><br><span class="line">        <span class="comment"># 仅使用前 k+1 个训练样本来训练模型</span></span><br><span class="line">        linear_regression.fit(X_poly_train[:k + <span class="number">1</span>], y_train[:k + <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># a. 计算训练误差</span></span><br><span class="line">        <span class="comment"># 在模型刚刚学习过的k+1个样本上进行预测</span></span><br><span class="line">        y_train_pred = linear_regression.predict(X_poly_train[:k + <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># 计算这k+1个样本的均方误差，并存入列表</span></span><br><span class="line">        train_error.append(mean_squared_error(y_train[:k + <span class="number">1</span>], y_train_pred))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># b. 计算测试误差</span></span><br><span class="line">        <span class="comment"># 使用在k+1个样本上训练好的模型，对**完整**的测试集进行预测</span></span><br><span class="line">        y_test_pred = linear_regression.predict(X_poly_test)</span><br><span class="line">        <span class="comment"># 计算在整个测试集上的均方误差，并存入列表</span></span><br><span class="line">        test_error.append(mean_squared_error(y_test, y_test_pred))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># --- 3. 绘制当前次数的学习曲线 ---</span></span><br><span class="line">    <span class="comment"># 创建一个2x2的子图网格，并在第 i+1 个位置上绘图</span></span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, i + <span class="number">1</span>)</span><br><span class="line">    <span class="comment"># 设置子图的标题，显示当前的多项式次数</span></span><br><span class="line">    plt.title(<span class="string">&quot;Degree: &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(degree))</span><br><span class="line">    <span class="comment"># 固定y轴的范围以便于比较不同子图</span></span><br><span class="line">    plt.ylim(-<span class="number">5</span>, <span class="number">50</span>)</span><br><span class="line">    <span class="comment"># 绘制训练误差曲线（红色）</span></span><br><span class="line">    plt.plot([k + <span class="number">1</span> <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_train))], train_error, color = <span class="string">&quot;red&quot;</span>, label = <span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    <span class="comment"># 绘制测试误差曲线（蓝色）</span></span><br><span class="line">    plt.plot([k + <span class="number">1</span> <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(x_train))], test_error, color = <span class="string">&quot;blue&quot;</span>, label = <span class="string">&#x27;test&#x27;</span>)</span><br><span class="line">    <span class="comment"># 显示图例</span></span><br><span class="line">    plt.legend()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将包含四个子图的整个图形显示出来</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722170421.png" alt="image.png"></p>
<p>下面使用的是模型自带的 <strong><code>.score()</code></strong> 方法来评估模型。对于 <code>LinearRegression</code> 模型，<code>.score()</code> 方法返回的是 <strong>R² 分数 (R-squared)</strong></p>
<ul>
<li><strong>含义</strong>: R²分数，也叫<strong>决定系数 (Coefficient of Determination)<strong>，衡量的是模型对数据方差的</strong>解释程度</strong>。</li>
<li><strong>解读</strong>: 它是一个<strong>拟合优度</strong>指标，<strong>值越接近 1 越好</strong>。值为 1 表示模型完美解释了数据的变化。值为 0 表示模型和直接取均值的效果一样差。值也可能为负，表示模型还不如取均值。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 必要的准备代码 (Necessary Setup Code) ---# (此处省略和之前相同的库导入和数据准备代码)  </span></span><br><span class="line"><span class="comment"># -------------------------------------------------  </span></span><br><span class="line">  </span><br><span class="line"><span class="comment"># 设置最终生成图形的大小  </span></span><br><span class="line">plt.rcParams[<span class="string">&quot;figure.figsize&quot;</span>] = (<span class="number">12</span>, <span class="number">8</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 定义要进行比较的多项式次数列表  </span></span><br><span class="line">degrees = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">20</span>]  </span><br><span class="line"><span class="comment"># 遍历不同的多项式次数，为每一种复杂度的模型绘制学习曲线  </span></span><br><span class="line"><span class="keyword">for</span> i, degree <span class="keyword">in</span> <span class="built_in">enumerate</span>(degrees):  </span><br><span class="line">    <span class="comment"># --- 1. 为当前次数创建多项式特征 ---    polynomial_features = PolynomialFeatures(degree = degree)  </span></span><br><span class="line">    X_poly_train = polynomial_features.fit_transform(x_train.reshape(-<span class="number">1</span>, <span class="number">1</span>))  </span><br><span class="line">    X_poly_test = polynomial_features.fit_transform(x_test.reshape(-<span class="number">1</span>, <span class="number">1</span>))  </span><br><span class="line">    <span class="comment"># --- 2. 手动生成学习曲线数据 (使用R²分数) ---  </span></span><br><span class="line">    <span class="comment"># 初始化用于存储训练分数和测试分数的列表  </span></span><br><span class="line">    train_error, test_error = [], []  </span><br><span class="line">    <span class="comment"># 遍历训练集，模拟训练样本数量从1逐渐增加到m-1的全过程  </span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(x_train)):  </span><br><span class="line">        <span class="comment"># 创建一个新的线性回归模型实例  </span></span><br><span class="line">        linear_regression = LinearRegression()  </span><br><span class="line">        <span class="comment"># 仅使用前 k+1 个训练样本来训练模型  </span></span><br><span class="line">        linear_regression.fit(X_poly_train[:k + <span class="number">1</span>], y_train[:k + <span class="number">1</span>])  </span><br><span class="line">  </span><br><span class="line">        <span class="comment"># a. 计算训练分数 (R-squared)        # 使用.score()方法计算模型在训练子集上的R²分数  </span></span><br><span class="line">        train_error.append(linear_regression.score(X_poly_train[:k + <span class="number">1</span>], y_train[:k + <span class="number">1</span>]))  </span><br><span class="line">        <span class="comment"># b. 计算测试分数 (R-squared)        # 计算模型在完整测试集上的R²分数  </span></span><br><span class="line">        test_error.append(linear_regression.score(X_poly_test, y_test))  </span><br><span class="line">    <span class="comment"># --- 3. 绘制当前次数的学习曲线 ---    # 创建一个2x2的子图网格，并在第 i+1 个位置上绘图  </span></span><br><span class="line">    plt.subplot(<span class="number">2</span>, <span class="number">2</span>, i + <span class="number">1</span>)  </span><br><span class="line">    <span class="comment"># 设置子图的标题  </span></span><br><span class="line">    plt.title(<span class="string">&quot;Degree: &#123;0&#125;&quot;</span>.<span class="built_in">format</span>(degree))  </span><br><span class="line">    <span class="comment"># 设置y轴范围为[-1, 1]，这是R²分数的常见范围  </span></span><br><span class="line">    plt.ylim(-<span class="number">1</span>, <span class="number">1</span>)  </span><br><span class="line">    <span class="comment"># 绘制训练分数曲线（红色）  </span></span><br><span class="line">    plt.plot([k + <span class="number">1</span> <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(x_train))], train_error, color = <span class="string">&quot;red&quot;</span>, label = <span class="string">&#x27;train&#x27;</span>)  </span><br><span class="line">    <span class="comment"># 绘制测试分数曲线（蓝色）  </span></span><br><span class="line">    plt.plot([k + <span class="number">1</span> <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(x_train))], test_error, color = <span class="string">&quot;blue&quot;</span>, label = <span class="string">&#x27;test&#x27;</span>)  </span><br><span class="line">    <span class="comment"># 显示图例  </span></span><br><span class="line">    plt.legend()  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 将包含四个子图的整个图形显示出来  </span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722170809.png" alt="image.png"></p>
<h1 id="模型评价"><a href="#模型评价" class="headerlink" title="模型评价"></a>模型评价</h1><h2 id="1-回归模型评估指标"><a href="#1-回归模型评估指标" class="headerlink" title="1 回归模型评估指标"></a>1 回归模型评估指标</h2><p>[[02_机器学习_线性回归#回归模型评估指标]]</p>
<h2 id="2-分类模型评估指标"><a href="#2-分类模型评估指标" class="headerlink" title="2 分类模型评估指标"></a>2 分类模型评估指标</h2><p>[[03_机器学习_逻辑回归#分类模型评估指标]]</p>
<h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h2 id="1-超参数搜索"><a href="#1-超参数搜索" class="headerlink" title="1 超参数搜索"></a>1 超参数搜索</h2><p>这里以 KNN 超参数搜索为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需库</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 数据准备 ---</span></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集分割为训练集和测试集 (70%训练, 30%测试)</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = <span class="number">0.7</span>, random_state = <span class="number">233</span>, stratify = y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 手动网格搜索寻找最佳超参数 ---</span></span><br><span class="line"><span class="comment"># 初始化用于记录最佳结果的变量</span></span><br><span class="line">best_score = -<span class="number">1</span>     <span class="comment"># 记录最高分</span></span><br><span class="line">best_n = -<span class="number">1</span>         <span class="comment"># 记录最佳的n_neighbors值</span></span><br><span class="line">best_weight = <span class="string">&#x27;&#x27;</span>    <span class="comment"># 记录最佳的weights值</span></span><br><span class="line">best_p = -<span class="number">1</span>         <span class="comment"># 记录最佳的p值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历n_neighbors参数（邻居数量）</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">20</span>):</span><br><span class="line">    <span class="comment"># 遍历weights参数（权重类型：&#x27;uniform&#x27;为等权重，&#x27;distance&#x27;为距离倒数权重）</span></span><br><span class="line">    <span class="keyword">for</span> weight <span class="keyword">in</span> [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>]:</span><br><span class="line">        <span class="comment"># 遍历p参数（距离度量：p=1为曼哈顿距离，p=2为欧式距离等）</span></span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">            <span class="comment"># 使用当前循环的超参数组合创建KNN分类器实例</span></span><br><span class="line">            neigh = KNeighborsClassifier(</span><br><span class="line">                n_neighbors = n,</span><br><span class="line">                weights = weight,</span><br><span class="line">                p = p</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 训练模型</span></span><br><span class="line">            neigh.fit(x_train, y_train)</span><br><span class="line">            <span class="comment"># 在测试集上评估模型得分（准确率）</span></span><br><span class="line">            score = neigh.score(x_test, y_test)</span><br><span class="line">            <span class="comment"># 如果当前组合的得分优于之前记录的最佳得分，则更新记录</span></span><br><span class="line">            <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">                best_score = score</span><br><span class="line">                best_n = n</span><br><span class="line">                best_weight = weight</span><br><span class="line">                best_p = p</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 打印最佳结果 ---</span></span><br><span class="line"><span class="comment"># 打印搜索到的最佳超参数组合和对应的最高分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;n_neighbors:&quot;</span>, best_n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights:&quot;</span>, best_weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p:&quot;</span>, best_p)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, best_score)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722163523.png" alt="image.png"></p>
<p>也可以直接调包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 定义超参数搜索空间 ---</span></span><br><span class="line"><span class="comment"># 创建一个字典，定义需要搜索的超参数及其候选值</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="comment"># 邻居数量 n_neighbors 从1遍历到19</span></span><br><span class="line">    <span class="string">&#x27;n_neighbors&#x27;</span>: [n <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">20</span>)],</span><br><span class="line">    <span class="comment"># 权重类型 weights 测试 &#x27;uniform&#x27;（等权重）和 &#x27;distance&#x27;（距离权重）</span></span><br><span class="line">    <span class="string">&#x27;weights&#x27;</span>: [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>],</span><br><span class="line">    <span class="comment"># 距离度量 p 从1遍历到6（p=1为曼哈顿距离，p=2为欧式距离等）</span></span><br><span class="line">    <span class="string">&#x27;p&#x27;</span>: [p <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>)]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 创建并配置网格搜索实例 ---</span></span><br><span class="line"><span class="comment"># 创建GridSearchCV实例，它会自动进行交叉验证来寻找最佳参数</span></span><br><span class="line">grid = GridSearchCV(</span><br><span class="line">    <span class="comment"># 指定基础模型为KNN分类器</span></span><br><span class="line">    estimator = KNeighborsClassifier(),</span><br><span class="line">    <span class="comment"># 传入上面定义的超参数搜索空间</span></span><br><span class="line">    param_grid = params,</span><br><span class="line">    <span class="comment"># 使用所有可用的CPU核心进行并行计算，以加快搜索速度</span></span><br><span class="line">    n_jobs = -<span class="number">1</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 执行搜索 ---</span></span><br><span class="line"><span class="comment"># 在训练集上执行网格搜索，这会自动完成所有参数组合的交叉验证</span></span><br><span class="line">grid.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. 查看最佳结果 ---</span></span><br><span class="line"><span class="comment"># 打印在交叉验证中表现最好的那一组超参数组合</span></span><br><span class="line">grid.best_params_  <span class="comment"># 示例输出: &#123;&#x27;n_neighbors&#x27;: 9, &#x27;p&#x27;: 2, &#x27;weights&#x27;: &#x27;uniform&#x27;&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印最佳超参数组合在交叉验证中的平均得分（通常是准确率）</span></span><br><span class="line">grid.best_score_ <span class="comment"># 示例输出: 0.961904761904762</span></span><br></pre></td></tr></table></figure>

<h2 id="2-交叉验证"><a href="#2-交叉验证" class="headerlink" title="2 交叉验证"></a>2 交叉验证</h2><p>理论知识见：[[01_机器学习_感知机#5.2 K 折交叉验证]]</p>
<p>代码实现如下：</p>
<p>这里以 KNN 为例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个KNN分类器实例（使用默认参数）</span></span><br><span class="line">neigh = KNeighborsClassifier()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对KNN模型在训练集上进行5折交叉验证（5-fold cross-validation）</span></span><br><span class="line"><span class="comment"># cv=5: 将训练集分成5份，轮流使用4份训练，1份验证，重复5次</span></span><br><span class="line"><span class="comment"># 函数返回一个包含5次验证得分的列表</span></span><br><span class="line">cv_scores = cross_val_score(neigh, x_train, y_train, cv = <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<p><span style="color: #badc58; font-weight: 550;">结合超参数搜索</span>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 必要的准备代码 (Necessary Setup Code) ---</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, cross_val_score</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载并分割鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line">x = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state = <span class="number">666</span>)</span><br><span class="line"><span class="comment"># -------------------------------------------------</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 1. 手动网格搜索与交叉验证 ---</span></span><br><span class="line"><span class="comment"># 初始化用于记录最佳结果的变量</span></span><br><span class="line">best_score = -<span class="number">1</span>          <span class="comment"># 记录最高的交叉验证平均分</span></span><br><span class="line">best_n = -<span class="number">1</span>              <span class="comment"># 记录最佳的n_neighbors值</span></span><br><span class="line">best_weight = <span class="string">&#x27;&#x27;</span>         <span class="comment"># 记录最佳的weights值</span></span><br><span class="line">best_p = -<span class="number">1</span>              <span class="comment"># 记录最佳的p值</span></span><br><span class="line">best_cv_scores = <span class="literal">None</span>    <span class="comment"># 记录最佳配置下的5次交叉验证得分</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历所有超参数的组合</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> weight <span class="keyword">in</span> [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>]:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">            <span class="comment"># 使用当前组合创建KNN分类器实例</span></span><br><span class="line">            neigh = KNeighborsClassifier(</span><br><span class="line">                n_neighbors = n,</span><br><span class="line">                weights = weight,</span><br><span class="line">                p = p</span><br><span class="line">            )</span><br><span class="line">            <span class="comment"># 对当前模型进行5折交叉验证，得到包含5个分数的列表</span></span><br><span class="line">            cv_scores = cross_val_score(neigh, x_train, y_train, cv = <span class="number">5</span>)</span><br><span class="line">            <span class="comment"># 计算5次交叉验证的平均分，作为当前超参数组合的最终性能评估</span></span><br><span class="line">            score = np.mean(cv_scores)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 如果当前组合的平均分优于之前记录的最佳分数，则更新记录</span></span><br><span class="line">            <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">                best_score = score</span><br><span class="line">                best_n = n</span><br><span class="line">                best_weight = weight</span><br><span class="line">                best_p = p</span><br><span class="line">                best_cv_scores = cv_scores</span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 打印最佳结果 ---</span></span><br><span class="line"><span class="comment"># 打印搜索到的最佳超参数组合</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;n_neighbors:&quot;</span>, best_n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights:&quot;</span>, best_weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p:&quot;</span>, best_p)</span><br><span class="line"><span class="comment"># 打印最佳组合在交叉验证中的平均得分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, best_score)</span><br><span class="line"><span class="comment"># 打印最佳组合的5次交叉验证具体得分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;best_cv_scores:&quot;</span>, best_cv_scores)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250722164138.png" alt="image.png"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://VernalScenery.github.io">Scenery</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://vernalscenery.github.io/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/">https://vernalscenery.github.io/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://VernalScenery.github.io" target="_blank">春和景明的记事本</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/./img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/07_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(MLP)/" title="07_机器学习_多层感知机(MLP)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">07_机器学习_多层感知机(MLP)</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/06_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%86%B3%E7%AD%96%E6%A0%91/" title="06_机器学习_决策树"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">06_机器学习_决策树</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/" title="00_机器学习_KNN算法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-04</div><div class="title">00_机器学习_KNN算法</div></div></a></div><div><a href="/2025/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E8%A7%88%E3%80%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="00_机器学习_概览、环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-03</div><div class="title">00_机器学习_概览、环境搭建</div></div></a></div><div><a href="/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="02_机器学习_线性回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="title">02_机器学习_线性回归</div></div></a></div><div><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">01_机器学习_感知机</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="03_机器学习_逻辑回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">03_机器学习_逻辑回归</div></div></a></div><div><a href="/2025/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/07_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(MLP)/" title="07_机器学习_多层感知机(MLP)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-23</div><div class="title">07_机器学习_多层感知机(MLP)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Scenery</div><div class="author-info__description">今天不想跑，所以才去跑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chjm0121" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1595718686@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9F"><span class="toc-text">1 什么是损失函数？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%9C%89%E5%93%AA%E4%BA%9B%EF%BC%9F"><span class="toc-text">2 损失函数有哪些？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F-%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98"><span class="toc-text">2.1 距离度量 (主要用于回归问题)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-1-%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE-Mean-Squared-Error-MSE"><span class="toc-text">2.1.1 均方误差 (Mean Squared Error, MSE)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-2-%E5%9D%87%E6%96%B9%E6%A0%B9%E8%AF%AF%E5%B7%AE%EF%BC%88Root-Mean-Square-Error-RMSE%EF%BC%89"><span class="toc-text">2.1.2 均方根误差（Root Mean Square Error, RMSE）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-3-%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE-Mean-Absolute-Error-MAE"><span class="toc-text">2.1.3 平均绝对误差 (Mean Absolute Error, MAE)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%A1%A5%E5%85%85%E7%9F%A5%E8%AF%86%EF%BC%9A%E7%86%B5"><span class="toc-text">2.2 补充知识：熵</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-1-%E4%BF%A1%E6%81%AF%E7%86%B5-Information-Entropy-H-p"><span class="toc-text">2.2.1 信息熵 (Information Entropy, H(p))</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-2-%E4%BA%A4%E5%8F%89%E7%86%B5-Cross-Entropy-L-p-q"><span class="toc-text">2.2.2 交叉熵 (Cross Entropy, L(p, q))</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-3-%E7%9B%B8%E5%AF%B9%E7%86%B5-Relative-Entropy-KL-%E6%95%A3%E5%BA%A6-D-KL-p-q"><span class="toc-text">2.2.3 相对熵 (Relative Entropy &#x2F; KL 散度, D_KL(p||q))</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E7%86%B5%E7%9A%84%E5%BA%A6%E9%87%8F-%E4%B8%BB%E8%A6%81%E7%94%A8%E4%BA%8E%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-text">2.3 熵的度量 (主要用于分类问题)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1-Categorical-Cross-Entropy-Loss"><span class="toc-text">2.3.1 分类交叉熵损失 (Categorical Cross-Entropy Loss)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-%E4%BA%8C%E5%85%83%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1-Binary-Cross-Entropy-Loss"><span class="toc-text">2.3.2 二元交叉熵损失 (Binary Cross-Entropy Loss)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%85%B6%E4%BB%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">2.4 其他损失函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AD%A6%E4%B9%A0%E5%87%86%E5%88%99"><span class="toc-text">3 学习准则</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E7%BB%8F%E9%AA%8C%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96-ERM"><span class="toc-text">3.1 经验风险最小化 (ERM)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E7%BB%93%E6%9E%84%E9%A3%8E%E9%99%A9%E6%9C%80%E5%B0%8F%E5%8C%96-SRM"><span class="toc-text">3.2 结构风险最小化 (SRM)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E4%B8%A4%E8%80%85%E7%9A%84%E5%85%B3%E7%B3%BB%E4%B8%8E%E5%8C%BA%E5%88%AB"><span class="toc-text">3.3 两者的关系与区别</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BC%98%E8%A7%A3%EF%BC%9A%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D-Gradient-Descent"><span class="toc-text">寻找最优解：梯度下降 (Gradient Descent)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%B6%85%E8%B6%8A%E6%96%B9%E7%A8%8B"><span class="toc-text">1 超越方程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A%E6%9C%80%E5%BF%AB%E4%B8%8B%E5%B1%B1%E8%B7%AF%E5%BE%84"><span class="toc-text">2 核心思想：最快下山路径</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E6%95%B0%E5%AD%A6%E6%AD%A5%E9%AA%A4%EF%BC%88%E4%BB%A5%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%BA%E4%BE%8B%EF%BC%89"><span class="toc-text">3 梯度下降的数学步骤（以线性回归为例）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%B1%BB%E5%9E%8B"><span class="toc-text">4 梯度下降的类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%8C%91%E6%88%98%E4%B8%8E%E7%8E%B0%E4%BB%A3%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">5 挑战与现代优化器</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88%EF%BC%9A%E6%AD%A3%E5%88%99%E5%8C%96-Regularization"><span class="toc-text">解决过拟合：正则化 (Regularization)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-text">1 过拟合与欠拟合</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%AD%A3%E5%88%99%E5%8C%96%E9%A1%B9-%E7%BD%9A%E9%A1%B9"><span class="toc-text">2 正则化项&#x2F;罚项</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%8C%83%E6%95%B0-Norm-%EF%BC%9A%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6"><span class="toc-text">3 范数 (Norm)：量化模型复杂度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%B8%BA%E4%BB%80%E4%B9%88%E2%80%9C%E6%9D%83%E9%87%8D%E5%90%91%E9%87%8F-w-%E7%9A%84%E8%8C%83%E6%95%B0%E2%80%9D%E5%8F%AF%E4%BB%A5%E7%94%A8%E6%9D%A5%E4%BB%A3%E8%A1%A8%E2%80%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%A4%8D%E6%9D%82%E5%BA%A6%E2%80%9D%EF%BC%9F"><span class="toc-text">3.1 为什么“权重向量 w 的范数”可以用来代表“模型的复杂度”？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-L1-%E8%8C%83%E6%95%B0%E5%92%8C-L2-%E8%8C%83%E6%95%B0"><span class="toc-text">3.2 L1 范数和 L2 范数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-L1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">4 L1 正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-L2-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">5 L2 正则化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E5%BC%B9%E6%80%A7%E7%BD%91%E7%BB%9C-Elastic-Net"><span class="toc-text">6 弹性网络 (Elastic Net)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%80%BB%E7%BB%93"><span class="toc-text">7 总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%B3%9B%E5%8C%96-Model-Generalization"><span class="toc-text">模型泛化 (Model Generalization)</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%B3%9B%E5%8C%96%E7%9A%84%E5%BA%A6%E9%87%8F%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE-Model-Error"><span class="toc-text">1 泛化的度量：模型误差 (Model Error)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%A8%A1%E5%9E%8B%E8%AF%AF%E5%B7%AE%E7%9A%84%E6%9E%84%E6%88%90"><span class="toc-text">1.1 模型误差的构成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE%E7%9A%84%E5%85%B3%E7%B3%BB%EF%BC%9A%E6%AD%A4%E6%B6%88%E5%BD%BC%E9%95%BF%E7%9A%84%E6%9D%83%E8%A1%A1"><span class="toc-text">1.2 偏差和方差的关系：此消彼长的权衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%81%8F%E5%B7%AE%E3%80%81%E6%96%B9%E5%B7%AE%E5%92%8C%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">1.3 偏差、方差和模型复杂度的关系</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E8%AF%8A%E6%96%AD%E6%B3%9B%E5%8C%96%E9%97%AE%E9%A2%98%E7%9A%84%E5%B7%A5%E5%85%B7%EF%BC%9A%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF-Learning-Curve"><span class="toc-text">2 诊断泛化问题的工具：学习曲线 (Learning Curve)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">2.1 核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2.2 代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7"><span class="toc-text">模型评价</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">1 回归模型评估指标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">2 分类模型评估指标</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%A1%A5%E5%85%85"><span class="toc-text">补充</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%B6%85%E5%8F%82%E6%95%B0%E6%90%9C%E7%B4%A2"><span class="toc-text">1 超参数搜索</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-text">2 交叉验证</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/05/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/02_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_Numpy/" title="02_数据分析_Numpy">02_数据分析_Numpy</a><time datetime="2025-08-04T21:11:38.000Z" title="发表于 2025-08-05 05:11:38">2025-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/05/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/01_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_python%E8%AF%AD%E6%B3%95/" title="01_数据分析_python语法">01_数据分析_python语法</a><time datetime="2025-08-04T18:44:45.000Z" title="发表于 2025-08-05 02:44:45">2025-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/13_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" title="13_机器学习_概率图模型">13_机器学习_概率图模型</a><time datetime="2025-07-29T00:29:02.000Z" title="发表于 2025-07-29 08:29:02">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/11_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%81%9A%E7%B1%BB/" title="11_机器学习_聚类">11_机器学习_聚类</a><time datetime="2025-07-28T00:19:53.000Z" title="发表于 2025-07-28 08:19:53">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维">12_机器学习_降维</a><time datetime="2025-07-27T20:48:42.000Z" title="发表于 2025-07-28 04:48:42">2025-07-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./img/1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Scenery</div><div class="footer_custom_text"><div>波澜不惊</div><div class="footer-div"><img class="footer-icon" src="./img/备案图标.png"><a class="footer-a" target="_blank" rel="noopener" href="http://beian.miit.gov.cn/">皖ICP备2021016944号-1</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>