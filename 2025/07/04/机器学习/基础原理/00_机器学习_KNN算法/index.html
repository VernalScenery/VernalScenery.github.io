<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>00_机器学习_KNN算法 | 春和景明的记事本</title><meta name="author" content="Scenery"><meta name="copyright" content="Scenery"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1 核心思想1.1 算法如何工作？（直观比喻）想象一下，你想判断一个新搬来的邻居属于哪个社交圈（比如“爱运动的”、“爱读书的”或“爱美食的”）。一个很自然的方法是：  看看离他家最近的 k 户人家都是什么样的人。 如果这 k 户人家里，大部分（比如 k&amp;#x3D;5，有 4 户）都是“爱运动的”。 那你很有可"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://vernalscenery.github.io/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '00_机器学习_KNN算法',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-05 14:17:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="春和景明的记事本"><span class="site-name">春和景明的记事本</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">00_机器学习_KNN算法</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-04T01:06:40.000Z" title="发表于 2025-07-04 09:06:40">2025-07-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T06:17:15.000Z" title="更新于 2025-08-05 14:17:15">2025-08-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">5.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="00_机器学习_KNN算法"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="1-核心思想"><a href="#1-核心思想" class="headerlink" title="1 核心思想"></a>1 核心思想</h2><h3 id="1-1-算法如何工作？（直观比喻）"><a href="#1-1-算法如何工作？（直观比喻）" class="headerlink" title="1.1 算法如何工作？（直观比喻）"></a>1.1 算法如何工作？（直观比喻）</h3><p>想象一下，你想判断一个新搬来的邻居属于哪个社交圈（比如“爱运动的”、“爱读书的”或“爱美食的”）。一个很自然的方法是：</p>
<ol>
<li>看看离他家<strong>最近的 k 户人家</strong>都是什么样的人。</li>
<li>如果这 k 户人家里，大部分（比如 k=5，有 4 户）都是“爱运动的”。</li>
<li>那你很有可能会得出结论：这位新邻居也属于“爱运动的”社交圈。</li>
</ol>
<p>这就是 KNN 的工作方式。</p>
<h3 id="1-2-KNN-的算法步骤"><a href="#1-2-KNN-的算法步骤" class="headerlink" title="1.2 KNN 的算法步骤"></a>1.2 KNN 的算法步骤</h3><p>对于一个需要预测的新数据点，KNN 的执行步骤如下：</p>
<ol>
<li><strong>确定参数 <code>k</code> 和距离度量方式</strong>：首先，你需要选择一个整数 <code>k</code>（代表要看多少个邻居）和一个计算数据点之间“距离”或“相似度”的方法。</li>
<li><strong>计算距离</strong>：计算新的数据点与训练数据集中<strong>每一个</strong>点之间的距离。</li>
<li><strong>找到 k 个最近邻</strong>：将计算出的所有距离进行排序，并找出距离最小的前 <code>k</code> 个数据点，这些就是新数据点的“k 个最近邻居”。</li>
<li><strong>做出预测</strong>：<ul>
<li><strong>用于分类任务</strong>：进行“<strong>投票</strong>”。在这 k 个邻居中，哪个类别的样本数量最多，新的数据点就被预测为哪个类别。（例如，k=5，其中有 3 个邻居是 A 类，2 个是 B 类，那么新数据点就被预测为 A 类）。<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715191537.png" alt="|844"></li>
<li><strong>用于回归任务</strong>：进行“<strong>平均</strong>”。将这 k 个邻居的数值（例如房价、分数）取平均值或中位数，作为新数据点的预测值。<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715191636.png" alt="image.png|857"></li>
</ul>
</li>
</ol>
<h3 id="1-3-KNN-算法的优缺点"><a href="#1-3-KNN-算法的优缺点" class="headerlink" title="1.3 KNN 算法的优缺点"></a>1.3 KNN 算法的优缺点</h3><table>
<thead>
<tr>
<th>优点 (Pros)</th>
<th>缺点 (Cons)</th>
</tr>
</thead>
<tbody><tr>
<td>✓ 简单直观，易于实现</td>
<td>✗ 预测慢，计算成本高</td>
</tr>
<tr>
<td>✓ 无需训练，准备时间快</td>
<td>✗ 内存占用大</td>
</tr>
<tr>
<td>✓ 模型灵活，能处理非线性问题</td>
<td>✗ 对高维数据效果差（维度灾难）</td>
</tr>
<tr>
<td>✓ 易于添加新数据</td>
<td>✗ 必须进行特征缩放</td>
</tr>
<tr>
<td></td>
<td>✗ 对不平衡数据敏感</td>
</tr>
</tbody></table>
<p><span style="color: #badc58; font-weight: 550;">适用场景：</span></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715211847.png" alt="image.png|589"></p>
<h2 id="2-关键要素的详细说明"><a href="#2-关键要素的详细说明" class="headerlink" title="2 关键要素的详细说明"></a>2 关键要素的详细说明</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715192642.png" alt="image.png|968"></p>
<h3 id="2-1-k-值的选择"><a href="#2-1-k-值的选择" class="headerlink" title="2.1 k 值的选择"></a>2.1 <code>k</code> 值的选择</h3><p><code>k</code> 是一个超参数，它的选择对模型结果有至关重要的影响。</p>
<ul>
<li><p><strong>较小的 <code>k</code> 值</strong> (例如 <code>k=1</code>)：</p>
<ul>
<li><strong>优点</strong>：模型能更好地捕捉数据的局部细节。</li>
<li><strong>缺点</strong>：模型对噪声非常敏感。如果一个邻居恰好是异常值，预测结果就会出错。这容易导致<strong>过拟合（Overfitting）</strong>。</li>
</ul>
</li>
<li><p><strong>较大的 <code>k</code> 值</strong>：</p>
<ul>
<li><strong>优点</strong>：模型更稳定，能减少噪声数据的影响。</li>
<li><strong>缺点</strong>：如果 <code>k</code> 值过大，模型会变得过于平滑，忽略掉数据中重要的局部模式，导致不同类别之间的界限变得模糊。这容易导致<strong>欠拟合（Underfitting）</strong>。</li>
</ul>
</li>
</ul>
<p><strong>如何选择 k？</strong> 通常没有固定的答案，一般通过<strong>交叉验证（Cross-Validation）</strong> 的方法来尝试不同的 <code>k</code> 值，选择在验证集上表现最好的那一个。在二分类问题中，<code>k</code> 值通常会选择<strong>奇数</strong>，以避免投票时出现平票的情况。</p>
<blockquote>
<p> 交叉验证是一种通过将数据集重复划分为不同的训练集和测试集，并对多次评估结果取平均，从而更稳定、更可靠地评估模型泛化性能的技术。</p>
</blockquote>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715192222.png" alt="image.png|807"></p>
<h3 id="2-2-距离度量-Distance-Metric"><a href="#2-2-距离度量-Distance-Metric" class="headerlink" title="2.2 距离度量 (Distance Metric)"></a>2.2 距离度量 (Distance Metric)</h3><p>如何定义“近”取决于距离度量方式。最常用的有：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715192416.png" alt="image.png|665"></p>
<ul>
<li>**欧几里得距离 (Euclidean Distance)**：最常用的一种，即我们在几何空间中熟悉的“两点之间的直线距离”。</li>
<li>**曼哈顿距离 (Manhattan Distance)**：也称为“城市街区距离”，它计算的是在各个坐标轴上距离差的总和，就像在城市网格中从一个点走到另一个点要经过的街区数。</li>
</ul>
<h3 id="2-3-归一化-Normalization"><a href="#2-3-归一化-Normalization" class="headerlink" title="2.3 归一化 (Normalization)"></a>2.3 归一化 (Normalization)</h3><p>因为 k-NN 是基于距离的算法，所以<strong>特征缩放（Feature Scaling）</strong> 非常重要。如果不同特征的数值范围相差巨大（例如，一个特征是年龄 <code>[0-100]</code>，另一个是收入 <code>[0-1,000,000]</code>），那么收入这个特征会在距离计算中占据主导地位。通常需要在使用 k-NN 前对数据进行<strong>归一化</strong>（Normalization）或标准化（Standardization）。</p>
<h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><p><span style="color: #badc58; font-weight: 550;">数据预处理</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本特征</span></span><br><span class="line">data_X = [</span><br><span class="line">    [<span class="number">1.3</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">3.5</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">4.2</span>, <span class="number">2</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">3.3</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">9</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">7.5</span>],</span><br><span class="line">    [<span class="number">7.2</span>, <span class="number">4</span> ],</span><br><span class="line">    [<span class="number">8.1</span>, <span class="number">8</span>],</span><br><span class="line">    [<span class="number">9</span>, <span class="number">2.5</span>]</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 样本标记</span></span><br><span class="line">data_y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练集</span></span><br><span class="line">X_train = np.array(data_X)</span><br><span class="line">y_train = np.array(data_y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 新的样本点  </span></span><br><span class="line">data_new = np.array([<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">X_train[y_train==<span class="number">0</span>,<span class="number">0</span>] <span class="comment"># 从`X_train`中筛选出`y_train`标签值等于 0 的那些样本，然后提取这些样本的第 0 列特征数据</span></span><br><span class="line">X_train[y_train==<span class="number">0</span>,<span class="number">1</span>] <span class="comment"># 从`X_train`中筛选出`y_train`标签值等于 0 的那些样本，然后提取这些样本的第 1 列特征数据</span></span><br><span class="line"></span><br><span class="line">plt.scatter(X_train[y_train==<span class="number">0</span>,<span class="number">0</span>], X_train[y_train==<span class="number">0</span>,<span class="number">1</span>], color=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.scatter(X_train[y_train==<span class="number">1</span>,<span class="number">0</span>],X_train[y_train==<span class="number">1</span>,<span class="number">1</span>],color=<span class="string">&#x27;black&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>)</span><br><span class="line">plt.scatter(data_new[<span class="number">0</span>], data_new[<span class="number">1</span>],color=<span class="string">&#x27;b&#x27;</span>, marker=<span class="string">&#x27;^&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715193411.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">KNN 预测的过程</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 计算距离</span></span><br><span class="line"><span class="comment"># 使用列表推导式，高效地计算新数据点 data_new 与 X_train 中每一个数据点之间的欧几里得距离。</span></span><br><span class="line">distances = [np.sqrt(np.<span class="built_in">sum</span>((data - data_new)**<span class="number">2</span>)) <span class="keyword">for</span> data <span class="keyword">in</span> X_train]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 获取排序后的索引</span></span><br><span class="line"><span class="comment"># np.argsort() 返回排序后各距离值在原始列表中的索引，告诉我们哪些原始数据点是最近的。</span></span><br><span class="line">sort_index = np.argsort(distances)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 设定 k 值</span></span><br><span class="line"><span class="comment"># k 是超参数，代表我们要参考多少个最近的邻居来做决策。</span></span><br><span class="line">k = <span class="number">5</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 找到 k 个最近邻居的标签</span></span><br><span class="line"><span class="comment"># sort_index[:k] 获取距离最近的 k 个点的索引。</span></span><br><span class="line"><span class="comment"># 然后，我们用这些索引从 y_train 中找到对应邻居的标签。</span></span><br><span class="line"><span class="comment"># 假设 first_k 的结果是 [&#x27;A&#x27;, &#x27;B&#x27;, &#x27;A&#x27;, &#x27;A&#x27;, &#x27;B&#x27;]</span></span><br><span class="line">first_k = [y_train[i] <span class="keyword">for</span> i <span class="keyword">in</span> sort_index[:k]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 进行多数投票 (Majority Vote)</span></span><br><span class="line"><span class="comment"># Counter(first_k) 会统计列表中每个元素出现的次数，结果是一个字典形式的对象。</span></span><br><span class="line"><span class="comment"># 例如：Counter(&#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 2&#125;)</span></span><br><span class="line"><span class="comment"># .most_common(1) 会返回一个列表，其中包含出现次数最多的一个元素及其计数的元组。</span></span><br><span class="line"><span class="comment"># 例如：[(&#x27;A&#x27;, 3)]</span></span><br><span class="line"><span class="comment"># [0] 获取这个列表中的第一个元组：(&#x27;A&#x27;, 3)</span></span><br><span class="line"><span class="comment"># [0] 获取这个元组中的第一个元素，也就是出现次数最多的标签：&#x27;A&#x27;</span></span><br><span class="line">predict_y = Counter(first_k).most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终，predict_y 变量就包含了 k-NN 算法对新数据点的预测类别。</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;The predicted class is: <span class="subst">&#123;predict_y&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><span style="color: #badc58; font-weight: 550;">scikit-learn 中的 KNN 算法</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="comment"># 初始化K近邻分类器，设置近邻数为5</span></span><br><span class="line">kNN_classifier = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 使用训练数据集(X_train)和对应的标签(y_train)训练分类器，拟合模型</span></span><br><span class="line">kNN_classifier.fit(X_train,y_train)</span><br><span class="line"><span class="comment"># 将新数据点data_new从一维数组转换为二维数组（形状为[1, n_features]），以符合sklearn模型的输入要求</span></span><br><span class="line">data_new.reshape(<span class="number">1</span>,-<span class="number">1</span>) </span><br><span class="line"><span class="comment"># 对转换后的新数据点进行预测，返回预测的标签结果（存储在predict_y中）</span></span><br><span class="line">predict_y = kNN_classifier.predict(data_new.reshape(<span class="number">1</span>,-<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="4-划分数据集"><a href="#4-划分数据集" class="headerlink" title="4 划分数据集"></a>4 划分数据集</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715195009.png" alt="image.png|598"></p>
<p><span style="color: #badc58; font-weight: 550;">生成数据集</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用make_blobs函数生成聚类数据集</span></span><br><span class="line"><span class="comment"># 返回值x为样本特征数据，y为样本对应的簇标签</span></span><br><span class="line">x, y = make_blobs(</span><br><span class="line">    n_samples=<span class="number">300</span>,  <span class="comment"># 生成的总样本数量为300个</span></span><br><span class="line">    n_features=<span class="number">2</span>,   <span class="comment"># 每个样本包含2个特征（维度为2），便于后续二维可视化</span></span><br><span class="line">    centers=<span class="number">3</span>,      <span class="comment"># 设定生成3个聚类中心，即数据会形成3个明显的簇</span></span><br><span class="line">    cluster_std=<span class="number">1</span>,  <span class="comment"># 每个簇的标准差为1，控制簇内数据的分散程度（值越小越集中）</span></span><br><span class="line">    center_box=(-<span class="number">10</span>, <span class="number">10</span>),  <span class="comment"># 聚类中心的坐标范围限制在[-10, 10]之间（随机生成时生效）</span></span><br><span class="line">    random_state=<span class="number">233</span>,      <span class="comment"># 随机数种子固定为233，保证每次运行生成完全相同的数据集，便于结果复现</span></span><br><span class="line">    return_centers=<span class="literal">False</span>    <span class="comment"># 不返回聚类中心的具体坐标（若为True，会额外返回centers参数）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用matplotlib的scatter函数绘制散点图</span></span><br><span class="line"><span class="comment"># x[:,0] 表示取数据集x中所有样本的第0列特征（横轴数据）</span></span><br><span class="line"><span class="comment"># x[:,1] 表示取数据集x中所有样本的第1列特征（纵轴数据）</span></span><br><span class="line"><span class="comment"># c = y 表示根据标签y为不同簇的点分配颜色（同一簇的点颜色相同，不同簇颜色不同）</span></span><br><span class="line"><span class="comment"># s = 15 表示每个散点的大小为15（数值越大，点越大）</span></span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>], x[:,<span class="number">1</span>], c = y, s = <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715195201.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">划分数据集</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_train_test_split</span>(<span class="params">x, y, train_size = <span class="number">0.7</span>, random_state = <span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">if</span> random_state:</span><br><span class="line">        np.random.seed(random_state)</span><br><span class="line">    shuffle = np.random.permutation(<span class="built_in">len</span>(x))</span><br><span class="line">    train_index = shuffle[:<span class="built_in">int</span>(<span class="built_in">len</span>(x) * train_size)]</span><br><span class="line">    test_index = shuffle[<span class="built_in">int</span>(<span class="built_in">len</span>(x) * train_size):]</span><br><span class="line">    <span class="keyword">return</span> x[train_index], x[test_index], y[train_index], y[test_index]</span><br><span class="line">    </span><br><span class="line">x_train, x_test, y_train, y_test = my_train_test_split(x, y, train_size = <span class="number">0.7</span>, random_state = <span class="number">233</span>)</span><br></pre></td></tr></table></figure>

<p><span style="color: #badc58; font-weight: 550;">sklearn 划分数据集</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 使用train_test_split函数将数据集划分为训练集和测试集</span></span><br><span class="line"><span class="comment"># 参数说明：</span></span><br><span class="line">	<span class="comment"># x：特征数据（输入变量），包含所有样本的特征信息</span></span><br><span class="line">	<span class="comment"># y：标签数据（目标变量），包含所有样本对应的类别或结果</span></span><br><span class="line">	<span class="comment"># train_size = 0.7：指定训练集占总数据集的比例为70%，测试集则占30%</span></span><br><span class="line">	<span class="comment"># random_state = 233：设置随机数种子，确保每次运行代码时划分结果一致（便于实验复现）</span></span><br><span class="line">	<span class="comment"># stratify = y：按照标签y的分布比例进行分层抽样，保证训练集和测试集中各类别的比例与原数据集一致</span></span><br><span class="line"><span class="comment"># 函数返回值：</span></span><br><span class="line">	<span class="comment"># x_train：训练集的特征数据</span></span><br><span class="line">	<span class="comment"># x_test：测试集的特征数据</span></span><br><span class="line">	<span class="comment"># y_train：训练集的标签数据</span></span><br><span class="line">	<span class="comment"># y_test：测试集的标签数据</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = <span class="number">0.7</span>, random_state = <span class="number">233</span>, stratify = y)</span><br></pre></td></tr></table></figure>

<h2 id="5-模型评价"><a href="#5-模型评价" class="headerlink" title="5 模型评价"></a>5 模型评价</h2><p><span style="color: #badc58; font-weight: 550;">加载数据集</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">iris</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715201232.png" alt="image.png"></p>
<p>这是经典的<strong>鸢尾花（Iris）数据集</strong>，是机器学习和统计学中常用的分类数据集，以下是核心信息解析：</p>
<p>数据结构</p>
<ul>
<li><strong>data</strong>：150 个样本的特征数据，每个样本包含 4 个数值特征（以厘米为单位）：<ul>
<li>花萼长度（sepal length）</li>
<li>花萼宽度（sepal width）</li>
<li>花瓣长度（petal length）</li>
<li>花瓣宽度（petal width）</li>
</ul>
</li>
<li><strong>target</strong>：150 个样本的标签（0/1/2），对应 3 种鸢尾花类别。</li>
<li><strong>target_names</strong>：标签对应的类别名称：<ul>
<li>0 → ‘setosa’（山鸢尾）</li>
<li>1 → ‘versicolor’（变色鸢尾）</li>
<li>2 → ‘virginica’（维吉尼亚鸢尾）</li>
</ul>
</li>
<li><strong>feature_names</strong>：4 个特征的具体名称（与上述特征对应）。</li>
</ul>
<p>数据集特点</p>
<ul>
<li><strong>样本分布</strong>：3 类鸢尾花各含 50 个样本，类别平衡。</li>
<li><strong>特征属性</strong>：均为数值型，可直接用于模型训练。</li>
<li><strong>核心用途</strong>：常用于分类算法的测试（如逻辑回归、决策树、SVM 等），因特征与类别关联性强（尤其是花瓣长度和宽度，相关性达 0.95 左右），适合作为入门案例。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">拆分数据集</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line">y</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715201618.png" alt="image.png"></p>
<p>在划分训练集和测试集之前，必须<strong>检查数据是否有序</strong>。如果数据是按类别等方式排序的，需要先通过生成一个<strong>随机索引序列</strong>，并用这个序列来同步地打乱特征数据（<code>X</code>）和标签（<code>y</code>），以保证数据划分的随机性和有效性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">shuffle_index = np.random.permutation(<span class="built_in">len</span>(y))</span><br><span class="line"></span><br><span class="line">train_ratio = <span class="number">0.8</span></span><br><span class="line">train_size = <span class="built_in">int</span>(<span class="built_in">len</span>(X)*train_ratio)</span><br><span class="line">train_index = shuffle_index[:train_size]</span><br><span class="line">test_index = shuffle_index[train_size:]</span><br><span class="line"></span><br><span class="line">X_train = X[train_index]</span><br><span class="line">y_train = y[train_index]</span><br><span class="line"></span><br><span class="line">X_test = X[test_index]</span><br><span class="line">y_test = y[test_index]</span><br></pre></td></tr></table></figure>

<p>或者直接调包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=<span class="number">0.8</span>,random_state=<span class="number">666</span>)</span><br></pre></td></tr></table></figure>

<p><span style="color: #badc58; font-weight: 550;">预测</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_classifier = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">knn_classifier.fit(X_train, y_train)</span><br><span class="line">y_predict = knn_classifier.predict(X_test)</span><br><span class="line">y_predict</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715202329.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">评价</span></p>
<p>预测的 y 值是否等于真实值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">accutacy = np.<span class="built_in">sum</span>(y_predict == y_test) / <span class="built_in">len</span>(y_test)</span><br></pre></td></tr></table></figure>

<p>或者直接调包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test,y_predict)</span><br></pre></td></tr></table></figure>

<h2 id="6-超参数"><a href="#6-超参数" class="headerlink" title="6 超参数"></a>6 超参数</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715202829.png" alt="image.png|691"></p>
<p><strong>参数</strong>是模型内部的、通过<strong>训练数据自动学习到的变量</strong>（如权重）。</p>
<p>而<strong>超参数</strong>是机器学习模型外部的、需要<strong>人为在训练前设定的配置参数</strong>（如学习率、k-NN 中的 k 值），它决定了模型如何学习。</p>
<ul>
<li><strong>经验值</strong>：依靠操作者的经验来设定。比如，对于某个算法，我们根据以往的经验知道学习率 <code>α</code> 设置为 0.01 通常效果不错，或者 k-NN 算法中的 <code>k</code> 值选 5 比较合适。</li>
<li><strong>参数搜索 (Parameter Search)<strong>：这是一种更系统、更科学的方法。我们不只凭感觉，而是让程序去</strong>尝试多种不同的“旋钮”组合</strong>，然后通过<strong>交叉验证（Cross-Validation）</strong> 等方法评估每种组合的效果，最终找出能让模型表现最好的那一组设置。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">KNN 中的超参数</span></p>
<table>
<thead>
<tr>
<th>超参数 (Hyperparameter)</th>
<th>Scikit-learn 参数名</th>
<th>描述</th>
<th>关键影响</th>
</tr>
</thead>
<tbody><tr>
<td><strong>邻居数量</strong></td>
<td><code>n_neighbors</code></td>
<td>决策时参考的最近邻居个数 <code>k</code></td>
<td><strong>模型复杂度</strong>，控制过拟合与欠拟合的平衡</td>
</tr>
<tr>
<td><strong>距离度量</strong></td>
<td><code>metric</code></td>
<td>计算样本间“远近”的数学公式</td>
<td><strong>邻域的定义</strong>，影响谁是“最近邻”</td>
</tr>
<tr>
<td><strong>邻居权重</strong></td>
<td><code>weights</code></td>
<td>如何为不同的邻居分配投票权重</td>
<td><strong>决策的公平性</strong>，是否给予近邻更大的影响力</td>
</tr>
<tr>
<td><strong>实现算法</strong></td>
<td><code>algorithm</code></td>
<td>查找最近邻居的内部数据结构</td>
<td><strong>计算效率和速度</strong>，不影响理论预测结果</td>
</tr>
</tbody></table>
<p>在实践中，最重要的三个超参数是 <code>n_neighbors</code>、<code>metric</code> 和 <code>weights</code>。通常我们会使用<strong>网格搜索（Grid Search）</strong> 结合<strong>交叉验证（Cross-Validation）</strong> 来系统地寻找这些超参数的最佳组合。</p>
<p><span style="color: #badc58; font-weight: 550;">代码实现</span></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1、加载鸢尾花数据集并划分训练集和测试集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris = load_iris()  <span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">x = iris.data  <span class="comment"># 特征数据（花萼长度、宽度等）</span></span><br><span class="line">y = iris.target  <span class="comment"># 标签（花的类别）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按7:3划分训练集和测试集，stratify=y保证分层抽样（各类别比例与原数据一致），random_state固定随机种子</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">233</span>, stratify=y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、网格搜索最优超参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化最优参数和分数的记录变量</span></span><br><span class="line">best_score = -<span class="number">1</span>  <span class="comment"># 最优模型得分</span></span><br><span class="line">best_n = -<span class="number">1</span>  <span class="comment"># 最优近邻数</span></span><br><span class="line">best_weight = <span class="string">&#x27;&#x27;</span>  <span class="comment"># 最优权重方式</span></span><br><span class="line">best_p = -<span class="number">1</span>  <span class="comment"># 最优距离度量参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历可能的超参数组合：近邻数1-19，权重方式（uniform/distance），距离参数1-6</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">20</span>):</span><br><span class="line">    <span class="keyword">for</span> weight <span class="keyword">in</span> [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>]:</span><br><span class="line">        <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">            <span class="comment"># 初始化K近邻分类器，传入当前超参数</span></span><br><span class="line">            neigh = KNeighborsClassifier(</span><br><span class="line">                n_neighbors=n,  <span class="comment"># 近邻数量</span></span><br><span class="line">                weights=weight,  <span class="comment"># 权重计算方式（均匀/距离加权）</span></span><br><span class="line">                p=p  <span class="comment"># 距离度量（p=1为曼哈顿距离，p=2为欧氏距离等）</span></span><br><span class="line">            )</span><br><span class="line">            neigh.fit(x_train, y_train)  <span class="comment"># 用训练集训练模型</span></span><br><span class="line">            score = neigh.score(x_test, y_test)  <span class="comment"># 计算测试集得分（准确率）</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新最优参数：若当前模型得分更高，则记录当前超参数和分数</span></span><br><span class="line">            <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">                best_score = score</span><br><span class="line">                best_n = n</span><br><span class="line">                best_weight = weight</span><br><span class="line">                best_p = p</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出最优超参数及对应的模型得分</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;n_neighbors:&quot;</span>, best_n)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;weights:&quot;</span>, best_weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p:&quot;</span>, best_p)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;score:&quot;</span>, best_score)</span><br></pre></td></tr></table></figure>

<p>或者直接调包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV  <span class="comment"># 导入网格搜索工具</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义待搜索的超参数组合：近邻数、权重方式、距离度量参数</span></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;n_neighbors&#x27;</span>: [n <span class="keyword">for</span> n <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">20</span>)],  <span class="comment"># 近邻数候选值：1-19</span></span><br><span class="line">    <span class="string">&#x27;weights&#x27;</span>: [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>],  <span class="comment"># 权重方式候选值：均匀/距离加权</span></span><br><span class="line">    <span class="string">&#x27;p&#x27;</span>: [p <span class="keyword">for</span> p <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>)]  <span class="comment"># 距离度量参数候选值：1-6（p=1为曼哈顿距离，p=2为欧氏距离等）</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化网格搜索对象</span></span><br><span class="line">grid = GridSearchCV(</span><br><span class="line">    estimator=KNeighborsClassifier(),  <span class="comment"># 待优化的模型：K近邻分类器</span></span><br><span class="line">    param_grid=params,  <span class="comment"># 超参数搜索范围</span></span><br><span class="line">    n_jobs=-<span class="number">1</span>  <span class="comment"># 并行计算（使用所有可用CPU核心加速搜索）</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">grid.fit(x_train, y_train)  <span class="comment"># 用训练集训练并搜索最优超参数</span></span><br><span class="line"></span><br><span class="line">grid.best_params_  <span class="comment"># 输出最优超参数组合 eg：&#123;&#x27;n_neighbors&#x27;: 9, &#x27;p&#x27;: 2, &#x27;weights&#x27;: &#x27;uniform&#x27;&#125;</span></span><br><span class="line">grid.best_score_  <span class="comment"># 输出最优模型在交叉验证中的得分 </span></span><br><span class="line">grid.best_estimator_  <span class="comment"># 输出最优超参数对应的模型实例</span></span><br><span class="line">grid.best_estimator_.predict(x_test)  <span class="comment"># 用最优模型预测测试集</span></span><br><span class="line">grid.best_estimator_.score(x_test, y_test)  <span class="comment"># 计算最优模型在测试集上的准确率</span></span><br></pre></td></tr></table></figure>

<h2 id="7-特征归一化"><a href="#7-特征归一化" class="headerlink" title="7 特征归一化"></a>7 特征归一化</h2><p>特征的<strong>量纲</strong>（或数值范围）不一致 导致 <strong>大范围特征会“主导”距离计算</strong>。为了解决这种由量纲不同导致的不公平性，我们需要进行<strong>特征归一化</strong>。</p>
<h3 id="7-1-两者方法"><a href="#7-1-两者方法" class="headerlink" title="7.1 两者方法"></a>7.1 两者方法</h3><p>这里介绍<strong>两种</strong>最常用的特征归一化（Feature Scaling）方法。</p>
<p><span style="color: #badc58; font-weight: 550;">1、最大最小值归一化 (Min-Max Scaling)：</span></p>
<ul>
<li><strong>工作原理</strong>：这个方法进行线性的函数变换。它计算一个值在整个数据集的最大值和最小值区间中所处的位置比例。</li>
<li><strong>缺点</strong>：对异常值（Outliers）非常敏感。如果数据中存在一个极大的或极小的值，它会导致 <code>x_max</code> 或 <code>x_min</code> 变得很极端，从而将绝大多数其他数据点压缩到一个非常小的范围内。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715204751.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">2、零均值归一化 (Zero-Score Normalization)</span></p>
<ul>
<li><strong>工作原理</strong>：它将数据转换为一个标准的正态分布。转换后的值（称为 Z-score）代表了原始值偏离平均值的距离，以标准差为单位。</li>
<li><strong>结果</strong>：转换后的新特征数据将具有<strong>平均值为 0，标准差为 1</strong> 的分布特性。它的数值范围没有固定边界（不像 Min-Max Scaling 的 <code>[0, 1]</code>）。</li>
<li><strong>优点</strong>：相比于最大最小值归一化，它对数据中的<strong>异常值不那么敏感</strong>，适用性更广。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715204853.png" alt="image.png|196"></p>
<ul>
<li>其中，μ 是该特征所有数据的<strong>平均值（Mean）</strong>。</li>
<li>σ 是该特征所有数据的<strong>标准差（Standard Deviation）</strong>。<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715205135.png" alt="image.png"></li>
</ul>
<h3 id="7-2-代码实现"><a href="#7-2-代码实现" class="headerlink" title="7.2 代码实现"></a>7.2 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载鸢尾花数据集</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data  <span class="comment"># 特征数据</span></span><br><span class="line">y = iris.target  <span class="comment"># 标签数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------</span></span><br><span class="line"><span class="comment"># 方法1：归一化 (Min-Max Scaling)</span></span><br><span class="line"><span class="comment"># 将特征缩放到 [0, 1] 区间</span></span><br><span class="line"><span class="comment"># --------------------------</span></span><br><span class="line">X_normalized = X.copy()  <span class="comment"># 复制原始数据避免修改</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(X_normalized.shape[<span class="number">1</span>]):  <span class="comment"># 遍历每一列特征</span></span><br><span class="line">    col_data = X_normalized[:, col]</span><br><span class="line">    <span class="comment"># 归一化公式：(x - min) / (max - min)</span></span><br><span class="line">    X_normalized[:, col] = (col_data - np.<span class="built_in">min</span>(col_data)) / (np.<span class="built_in">max</span>(col_data) - np.<span class="built_in">min</span>(col_data))</span><br><span class="line"></span><br><span class="line"><span class="comment"># --------------------------</span></span><br><span class="line"><span class="comment"># 方法2：标准化 (Standardization)</span></span><br><span class="line"><span class="comment"># 将特征缩放为均值=0，标准差=1</span></span><br><span class="line"><span class="comment"># --------------------------</span></span><br><span class="line">X_standardized = X.copy()  <span class="comment"># 复制原始数据避免修改</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(X_standardized.shape[<span class="number">1</span>]):  <span class="comment"># 遍历每一列特征</span></span><br><span class="line">    col_data = X_standardized[:, col]</span><br><span class="line">    <span class="comment"># 标准化公式：(x - mean) / std</span></span><br><span class="line">    X_standardized[:, col] = (col_data - np.mean(col_data)) / np.std(col_data)</span><br></pre></td></tr></table></figure>

<p>或者调包实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载鸢尾花数据集，X为特征数据，y为标签</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler  <span class="comment"># 导入标准化工具</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化标准化处理器</span></span><br><span class="line">standard_scaler = StandardScaler()</span><br><span class="line">standard_scaler.fit(X)  <span class="comment"># 计算X的均值和标准差（用于后续标准化）</span></span><br><span class="line"></span><br><span class="line">standard_scaler.mean_  <span class="comment"># 输出各特征的均值</span></span><br><span class="line">standard_scaler.scale_  <span class="comment"># 输出各特征的标准差（用于标准化计算）</span></span><br><span class="line"></span><br><span class="line">X = standard_scaler.transform(X)  <span class="comment"># 用计算好的均值和标准差对X进行标准化（均值=0，标准差=1）</span></span><br></pre></td></tr></table></figure>

<h3 id="7-3-使用归一化"><a href="#7-3-使用归一化" class="headerlink" title="7.3 使用归一化"></a>7.3 使用归一化</h3><p>对测试集进行归一化时，<strong>绝不能</strong>使用测试集自身的统计数据，而<strong>必须沿用</strong>从训练集中学习到的统计参数，以保证数据变换的一致性并防止数据泄露。</p>
<ul>
<li>机器学习的一条黄金法则是：<strong>测试集在模型评估之前，对于整个训练流程来说必须是完全未知的</strong>。</li>
<li>如果你在预处理阶段计算了测试集的均值、标准差、最大/最小值等任何统计数据，并用它们来转换测试集，就相当于让你的模型在训练前“偷看”了测试集的信息。</li>
<li>这种“数据泄露”会导致模型在测试集上的评估结果过于乐观，无法真实反映模型在未来未知数据上的表现。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入数据集划分工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 将鸢尾花数据集按8:2划分为训练集（X_train, y_train）和测试集（X_test, y_test）</span></span><br><span class="line"><span class="comment"># random_state=666固定随机种子，保证划分结果可复现</span></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(iris.data,iris.target,train_size=<span class="number">0.8</span>,random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler  <span class="comment"># 导入标准化工具</span></span><br><span class="line">standard_scaler = StandardScaler()  <span class="comment"># 初始化标准化处理器</span></span><br><span class="line">standard_scaler.fit(X_train)  <span class="comment"># 用训练集计算均值和标准差（仅基于训练集，避免数据泄露）</span></span><br><span class="line"></span><br><span class="line">standard_scaler.mean_  <span class="comment"># 查看训练集各特征的均值</span></span><br><span class="line">standard_scaler.scale_  <span class="comment"># 查看训练集各特征的标准差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用训练集的均值和标准差对训练集、测试集进行标准化（保持缩放规则一致）</span></span><br><span class="line">X_train_standard = standard_scaler.transform(X_train)</span><br><span class="line">X_test_standard = standard_scaler.transform(X_test)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier  <span class="comment"># 导入K近邻分类器</span></span><br><span class="line">knn_classifier = KNeighborsClassifier(n_neighbors=<span class="number">5</span>)  <span class="comment"># 初始化KNN模型（近邻数=5）</span></span><br><span class="line">knn_classifier.fit(X_train_standard,y_train)  <span class="comment"># 用标准化后的训练集训练模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算模型在标准化测试集上的准确率</span></span><br><span class="line">knn_classifier.score(X_test_standard, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="8-KNN-回归代码实现"><a href="#8-KNN-回归代码实现" class="headerlink" title="8 KNN 回归代码实现"></a>8 KNN 回归代码实现</h2><p><strong>用于回归任务</strong>：进行“<strong>平均</strong>”。将这 k 个邻居的数值（例如房价、分数）取平均值或中位数，作为新数据点的预测值。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250715210501.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor  <span class="comment"># 导入K近邻回归器</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler  <span class="comment"># 导入标准化工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split  <span class="comment"># 导入数据集划分工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston  <span class="comment"># 导入波士顿房价数据集</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载波士顿房价数据集，x为特征数据（如房屋面积、周边环境等），y为目标值（房价）</span></span><br><span class="line">boston = load_boston()</span><br><span class="line">x = boston.data</span><br><span class="line">y = boston.target</span><br><span class="line">x.shape, y.shape  <span class="comment"># 查看特征数据和目标值的维度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 按7:3划分训练集和测试集，random_state固定随机种子确保结果可复现</span></span><br><span class="line">x_train ,x_test, y_train, y_test = train_test_split(x, y ,train_size = <span class="number">0.7</span>, random_state=<span class="number">233</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化K近邻回归模型：近邻数=5，权重方式为距离加权（距离越近影响越大），p=2表示欧氏距离</span></span><br><span class="line">knn_reg = KNeighborsRegressor(n_neighbors=<span class="number">5</span>, weights=<span class="string">&#x27;distance&#x27;</span>, p=<span class="number">2</span>)</span><br><span class="line">standardScaler = StandardScaler()  <span class="comment"># 初始化标准化处理器</span></span><br><span class="line"></span><br><span class="line">standardScaler.fit(x_train)  <span class="comment"># 基于训练集计算标准化所需的均值和标准差</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用训练集的标准化规则转换训练集和测试集（避免数据泄露）</span></span><br><span class="line">x_train = standardScaler.transform(x_train)</span><br><span class="line">x_test = standardScaler.transform(x_test)</span><br><span class="line"></span><br><span class="line">knn_reg.fit(x_train, y_train)  <span class="comment"># 用标准化后的训练集训练回归模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># knn_reg.score(x_test, y_test)  # 可用于评估模型在测试集上的表现（R²评分）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对新数据点预测：</span></span><br><span class="line"><span class="comment"># reshape(1,-1)将一维数据转为二维数组（1个样本，特征数自动匹配），符合模型输入格式</span></span><br><span class="line">predict_y = knn_reg.predict(data_new.reshape(<span class="number">1</span>,-<span class="number">1</span>))  <span class="comment"># 输出新数据点的预测值</span></span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://VernalScenery.github.io">Scenery</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://vernalscenery.github.io/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/">https://vernalscenery.github.io/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://VernalScenery.github.io" target="_blank">春和景明的记事本</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/./img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/07/10/%E5%9B%BE%E5%BD%A2%E5%AD%A6/Games101/01_GAMES101%E6%80%BB%E7%BB%93%E7%89%88_%E7%A9%BA%E9%97%B4%E5%8F%98%E6%8D%A2/" title="01_GAMES101总结版_空间变换"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">01_GAMES101总结版_空间变换</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/04/%E5%9B%BE%E5%BD%A2%E5%AD%A6/Games101/07_GAMES101_%E5%87%A0%E4%BD%95(Geometry)/" title="07_GAMES101_几何(Geometry)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">07_GAMES101_几何(Geometry)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">01_机器学习_感知机</div></div></a></div><div><a href="/2025/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E8%A7%88%E3%80%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="00_机器学习_概览、环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-03</div><div class="title">00_机器学习_概览、环境搭建</div></div></a></div><div><a href="/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="02_机器学习_线性回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="title">02_机器学习_线性回归</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="04_机器学习_基础概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">04_机器学习_基础概念</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="03_机器学习_逻辑回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">03_机器学习_逻辑回归</div></div></a></div><div><a href="/2025/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/07_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(MLP)/" title="07_机器学习_多层感知机(MLP)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-23</div><div class="title">07_机器学习_多层感知机(MLP)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Scenery</div><div class="author-info__description">今天不想跑，所以才去跑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chjm0121" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1595718686@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">1 核心思想</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E7%AE%97%E6%B3%95%E5%A6%82%E4%BD%95%E5%B7%A5%E4%BD%9C%EF%BC%9F%EF%BC%88%E7%9B%B4%E8%A7%82%E6%AF%94%E5%96%BB%EF%BC%89"><span class="toc-text">1.1 算法如何工作？（直观比喻）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-KNN-%E7%9A%84%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4"><span class="toc-text">1.2 KNN 的算法步骤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-KNN-%E7%AE%97%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">1.3 KNN 算法的优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%85%B3%E9%94%AE%E8%A6%81%E7%B4%A0%E7%9A%84%E8%AF%A6%E7%BB%86%E8%AF%B4%E6%98%8E"><span class="toc-text">2 关键要素的详细说明</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-k-%E5%80%BC%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-text">2.1 k 值的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F-Distance-Metric"><span class="toc-text">2.2 距离度量 (Distance Metric)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%BD%92%E4%B8%80%E5%8C%96-Normalization"><span class="toc-text">2.3 归一化 (Normalization)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3 代码实现</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%88%92%E5%88%86%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-text">4 划分数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BB%B7"><span class="toc-text">5 模型评价</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text">6 超参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E7%89%B9%E5%BE%81%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">7 特征归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-%E4%B8%A4%E8%80%85%E6%96%B9%E6%B3%95"><span class="toc-text">7.1 两者方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">7.2 代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E4%BD%BF%E7%94%A8%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">7.3 使用归一化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-KNN-%E5%9B%9E%E5%BD%92%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">8 KNN 回归代码实现</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/13_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" title="13_机器学习_概率图模型">13_机器学习_概率图模型</a><time datetime="2025-07-29T00:29:02.000Z" title="发表于 2025-07-29 08:29:02">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/11_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%81%9A%E7%B1%BB/" title="11_机器学习_聚类">11_机器学习_聚类</a><time datetime="2025-07-28T00:19:53.000Z" title="发表于 2025-07-28 08:19:53">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维">12_机器学习_降维</a><time datetime="2025-07-27T20:48:42.000Z" title="发表于 2025-07-28 04:48:42">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="10_机器学习_集成学习">10_机器学习_集成学习</a><time datetime="2025-07-26T01:58:53.000Z" title="发表于 2025-07-26 09:58:53">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="09_机器学习_朴素贝叶斯">09_机器学习_朴素贝叶斯</a><time datetime="2025-07-25T03:08:47.000Z" title="发表于 2025-07-25 11:08:47">2025-07-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./img/1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Scenery</div><div class="footer_custom_text"><div>波澜不惊</div><div class="footer-div"><img class="footer-icon" src="./img/备案图标.png"><a class="footer-a" target="_blank" rel="noopener" href="http://beian.miit.gov.cn/">皖ICP备2021016944号-1</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>