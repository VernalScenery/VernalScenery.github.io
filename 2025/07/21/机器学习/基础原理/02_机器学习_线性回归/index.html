<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>02_机器学习_线性回归 | 春和景明的记事本</title><meta name="author" content="Scenery"><meta name="copyright" content="Scenery"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="分类和回归分类和回归的本质区别是，分类的输出是离散的，而回归的输出是连续的。  回归大致可以理解为根据数据集 D，拟合出近似的曲线，所以回归也常称为拟合（英文：Fit），像下面右图一样拟合出来是直线的就称为线性回归（Linear Regression）：  比如下面这样的数据集 D，拟合后输入某 $"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://vernalscenery.github.io/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '02_机器学习_线性回归',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-05 14:17:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="春和景明的记事本"><span class="site-name">春和景明的记事本</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">02_机器学习_线性回归</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-20T23:48:55.000Z" title="发表于 2025-07-21 07:48:55">2025-07-21</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T06:17:15.000Z" title="更新于 2025-08-05 14:17:15">2025-08-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>16分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="02_机器学习_线性回归"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="分类和回归"><a href="#分类和回归" class="headerlink" title="分类和回归"></a>分类和回归</h1><p>分类和回归的本质区别是，分类的输出是离散的，而回归的输出是连续的。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721195336.png" alt="image.png"></p>
<p>回归大致可以理解为根据数据集 D，拟合出近似的曲线，所以回归也常称为拟合（英文：Fit），像下面右图一样拟合出来是直线的就称为线性回归（Linear Regression）：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721195438.png" alt="image.png"></p>
<p>比如下面这样的数据集 D，拟合后输入某 $\boldsymbol{x}$ 后，可以输出对应的预测结果：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721195500.png" alt="image.png"></p>
<p>其输出是连续的，所以这是监督式学习中的回归问题。</p>
<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="1-一元线性回归"><a href="#1-一元线性回归" class="headerlink" title="1 一元线性回归"></a>1 一元线性回归</h2><h3 id="1-1-核心目标-：找到最佳拟合直线"><a href="#1-1-核心目标-：找到最佳拟合直线" class="headerlink" title="1.1 核心目标 ：找到最佳拟合直线"></a>1.1 核心目标 ：找到最佳拟合直线</h3><p>线性回归是一种基础且强大的统计与机器学习方法，用于<strong>预测一个数值型</strong>的结果。它的核心任务是<strong>找到一个或多个“自变量”（X）与一个“因变量”（Y）之间的线性关系</strong>。</p>
<ul>
<li>**因变量 (Y)**：我们想要预测的目标。例如：房价、明天的气温、一支股票的价格。</li>
<li>**自变量 (X)**：用来进行预测的因素。例如：房屋的面积、今天的气温、公司的利润。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718084444.png" alt="image.png|845"></p>
<p>线性回归的目标就是找出下面这个“魔法公式”（即线性方程）：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721204515.png" alt="image.png"></p>
<ul>
<li><code>w₀</code> (截距): 是基础值，代表直线的起点。</li>
<li><code>w₁</code> (斜率): 是核心，代表 <code>X</code> 每增加一个单位，<code>Y</code> 会相应变化多少。</li>
<li><code>ϵ</code> (误差项): 代表现实中无法被直线完全解释的随机因素。</li>
</ul>
<p>线性回归要做的，就是通过分析现有的数据，找出最合适的 $w_0$ 和 $w_1$ 值，使得这条直线能够最大程度地贴近真实的数据点。</p>
<h3 id="1-2-贴合度的评价指标：损失函数"><a href="#1-2-贴合度的评价指标：损失函数" class="headerlink" title="1.2 贴合度的评价指标：损失函数"></a>1.2 贴合度的评价指标：损失函数</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718085118.png" alt="image.png|760"></p>
<p>想象你有一堆数据点，一条直线不可能完美穿过所有点。我们的目标是找到一条“妥协”得最好的线，它离所有点的“总体距离”最近。</p>
<p>这就需要一个<span style="color: #9c88ff; font-weight: 550;">评价指标</span></p>
<p><span style="color: #badc58; font-weight: 550;">1、定义“距离”：残差 (Residual)</span></p>
<p>我们首先要量化“距离”。这个距离在统计学中被称为<strong>残差 (Residual)<strong>，它指的就是任何一个数据点的</strong>真实值</strong>和我们画的直线给出的<strong>预测值</strong>之间的<strong>纵向差距</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718090111.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">2、累加并平均“距离”：均方误差 (MSE)</span></p>
<p>我们选择将每个残差进行<strong>平方</strong>，然后把它们全部加起来，最后再<strong>除以样本总数 <code>m</code> 进行算术平均</strong>。这个结果就是**均方误差 (Mean Squared Error, MSE)**。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721210535.png" alt="image.png"></p>
<ul>
<li>我们不能直接把所有残差相加，因为有正有负（点在直线上方是正，在下方是负），它们会相互抵消，无法真实反映总的误差大小。</li>
<li>除以 <code>m</code> 进行平均，可以消除样本数量对损失值大小的影响，便于在不同数据集上比较模型性能。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">3、代入具体模型</span></p>
<p>将一个通用的误差公式，转变成专门针对线性模型的、可以用参数 <code>w₀</code> 和 <code>w₁</code> 来表达的误差公式。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721210602.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">4、确立最终目标（最小化损失函数）</span></p>
<p>用来衡量模型总误差的函数，在机器学习中被称为<strong>损失函数</strong>或**成本函数 (Cost Function)**。</p>
<blockquote>
<p> 机器学习的<strong>训练过程</strong>，本质上就是一个<strong>寻找能让损失函数的值最小化的模型参数</strong>的过程。</p>
</blockquote>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721210616.png" alt="image.png"></p>
<ul>
<li><code>arg min</code> 是“argmin”的缩写，意为“寻找一组 <strong><code>w₀</code> 和 <code>w₁</code></strong> 的值，使得损失函数达到最小值？”（arg 指的是 argument，即参数）。</li>
</ul>
<p>在线性回归中，我们使用的损失函数是“<strong>均方误差</strong>”，而找到让这个损失函数最小化的方法，就是“<strong>最小二乘法</strong>”。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721204917.png" alt="image.png"></p>
<blockquote>
<p> 注意：这里的 <code>k</code> 和 <code>b</code> 分别对应我们上面所说的 <code>w₁</code> 和 <code>w₀</code></p>
</blockquote>
<h3 id="1-3-代码实现"><a href="#1-3-代码实现" class="headerlink" title="1.3 代码实现"></a>1.3 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 1. 数据准备 ---</span></span><br><span class="line"><span class="comment"># 从波士顿房价数据集中提取特征和目标变量</span></span><br><span class="line">x = boston.data[:,<span class="number">5</span>] <span class="comment"># 提取所有样本的第6个特征(房间数量RM)作为x</span></span><br><span class="line">y = boston.target     <span class="comment"># 提取房价作为目标变量y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 2. 数据预处理 ---</span></span><br><span class="line"><span class="comment"># 在该数据集中，房价50.0通常被视为异常值或数据上限，故作移除</span></span><br><span class="line">x = x[y &lt; <span class="number">50</span>]  <span class="comment"># 根据y&lt;50的条件过滤x，移除异常点对应的特征</span></span><br><span class="line">y = y[y &lt; <span class="number">50</span>]  <span class="comment"># 同时过滤y，确保x和y的数据一一对应</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 3. 模型训练 ---</span></span><br><span class="line"><span class="comment"># 从scikit-learn库中导入线性回归类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建线性回归模型的一个实例(对象)</span></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用训练数据(x_train, y_train)来训练(或称“拟合”)模型</span></span><br><span class="line"><span class="comment"># .reshape(-1, 1)将一维数据转换为二维，以满足scikit-learn的输入要求</span></span><br><span class="line">lin_reg.fit(x_train.reshape(-<span class="number">1</span>, <span class="number">1</span>), y_train) </span><br><span class="line"></span><br><span class="line"><span class="comment"># --- 4. 模型预测 ---</span></span><br><span class="line"><span class="comment"># 使用训练好的模型，对测试集x_test进行预测，得到预测值y_predict</span></span><br><span class="line">y_predict = lin_reg.predict(x_test.reshape(-<span class="number">1</span>, <span class="number">1</span>))</span><br></pre></td></tr></table></figure>

<h2 id="2-多元线性回归"><a href="#2-多元线性回归" class="headerlink" title="2 多元线性回归"></a>2 多元线性回归</h2><h3 id="2-1-多元线性回归的模型"><a href="#2-1-多元线性回归的模型" class="headerlink" title="2.1 多元线性回归的模型"></a>2.1 多元线性回归的模型</h3><p>如果说<strong>简单线性回归</strong>是试图用<strong>一维的直线</strong>去描述一个自变量 <code>(x)</code> 和因变量 <code>(y)</code> 的关系。</p>
<p>那么，<strong>多元线性回归</strong>就是试图用一个高维的“平面”或“<strong>超平面</strong>” 去描述两个或更多个自变量 <code>(x1​,x2​,x3​,…) </code> 与一个因变量 <code>(y)</code> 之间的关系。</p>
<ul>
<li>一个自变量时，是去拟合一条<strong>直线</strong></li>
<li>两个自变量时，是去拟合一个<strong>平面</strong></li>
<li>三个自变量及以上，是去拟合一个<strong>超平面</strong></li>
</ul>
<p>对于一个拥有 <code>n</code> 个特征的多元线性回归，其拟合的<strong>超平面</strong>可以表示为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721201509.png" alt="image.png"></p>
<ul>
<li>这里的 <code>w₀</code> 就是截距（Intercept）或偏置（Bias）。</li>
<li><code>w₁</code> 到 <code>wₙ</code> 是 <code>n</code> 个特征对应的<strong>权重（Weights）</strong>。</li>
<li><code>ε</code> 是误差项，在表达模型本身时我们通常省略它。</li>
</ul>
<p>通过如下形式：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721203040.png" alt="image.png"></p>
<blockquote>
<p> 通过引入一个恒为 <code>1</code> 的伪特征 <code>x₀</code> 来对应截距项 <code>w₀</code>，我们成功地将一个<strong>带有偏置项的 <code>n</code> 维线性模型</strong>，转化为了一个<strong>无偏置项（即穿过原点）的 <code>n+1</code> 维线性模型</strong>。</p>
</blockquote>
<p>最终的超平面方程可以极为简洁地写作：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721203058.png" alt="image.png"></p>
<h3 id="2-2-损失函数"><a href="#2-2-损失函数" class="headerlink" title="2.2 损失函数"></a>2.2 损失函数</h3><p>假设我们有 <code>m</code> 个数据样本和 <code>p</code> 个特征。对于线性回归，我们的目标是找到一个最优的权重向量 <code>w</code>，使得模型的预测值与真实值尽可能接近。我们通过最小化一个<strong>损失函数</strong>来达到这个目的。</p>
<p>损失函数 <code>J(w)</code> 计算的是所有样本的<strong>预测误差的平方的平均值</strong>。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721210933.png" alt="image.png"></p>
<p>将线性模型的假设 <code>ŷᵢ = wᵀxᵢ</code> 代入上面的公式，我们得到：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721211001.png" alt="image.png"></p>
<p>首先，误差向量 <code>ε</code> 可以表示为 <code>y - Xw</code>。因此，所有误差的平方和 <code>Σ(yᵢ - ŷᵢ)²</code> 在向量运算中等价于误差向量与其自身的内积 <code>(y - Xw)ᵀ(y - Xw)</code>。</p>
<p>所以，损失函数 <code>J(w)</code> 的矩阵形式为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721211453.png" alt="image.png"></p>
<p>其中，<strong>X</strong>、<strong>y</strong> 和 <strong>w</strong> 的定义如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721211504.png" alt="image.png"></p>
<h3 id="2-3-正规方程"><a href="#2-3-正规方程" class="headerlink" title="2.3 正规方程"></a>2.3 正规方程</h3><p>如何从损失函数的矩阵形式，解出最优的 <code>w</code>，这就需要用到<strong>正规方程</strong>。</p>
<blockquote>
<p> 正规方程是求解线性回归问题的一种“一步到位”的数学公式。它可以通过一次纯粹的矩阵运算，直接计算出能让损失函数最小化的那一组最佳参数（系数）</p>
</blockquote>
<p>它的标准形式如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721211657.png" alt="image.png"></p>
<blockquote>
<p> 注意：他的推导比较麻烦，知道有这个东西就行了</p>
</blockquote>
<p>其实找到损失函数的最优解，就是找到一个损失函数“山谷”的最低点，常用两种方法：</p>
<ul>
<li><strong>梯度下降 (Gradient Descent)</strong> 的方法像是一个<strong>徒步下山的探索者</strong>。他从山坡上任意一点出发，环顾四周，找到最陡的下坡方向，然后走一小步。接着，在新的位置重复这个过程，一步一步地慢慢走到谷底。</li>
<li><strong>正规方程 (Normal Equation)</strong> 的方法像是一个拥有“上帝视角”的数学家。他看一眼整座山谷的地形图（也就是我们的全部数据），然后用微积分直接计算出谷底的精确坐标，根本不需要移动。</li>
</ul>
<table>
<thead>
<tr>
<th>特征</th>
<th>正规方程 (Normal Equation)</th>
<th>梯度下降 (Gradient Descent)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>求解方式</strong></td>
<td>直接求解（解析解）</td>
<td>迭代逼近（数值解）</td>
</tr>
<tr>
<td><strong>需要学习率α</strong></td>
<td><strong>否</strong></td>
<td><strong>是</strong></td>
</tr>
<tr>
<td><strong>需要迭代</strong></td>
<td><strong>否</strong></td>
<td><strong>是</strong></td>
</tr>
<tr>
<td><strong>对特征数量 n 的敏感度</strong></td>
<td><strong>高</strong>，n 很大时非常慢</td>
<td><strong>低</strong>，n 很大时依然高效</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>特征数量较少时（比如 n &lt; 1000）</td>
<td>特征数量巨大时</td>
</tr>
</tbody></table>
<h3 id="2-4-代码实现"><a href="#2-4-代码实现" class="headerlink" title="2.4 代码实现"></a>2.4 代码实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 从scikit-learn库中导入内置的波士顿房价数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">boston = datasets.load_boston()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 准备特征数据(X)和目标数据(y)</span></span><br><span class="line">x = boston.data  <span class="comment"># 将所有特征数据赋值给x</span></span><br><span class="line">y = boston.target  <span class="comment"># 将目标变量（房价）赋值给y</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 数据预处理：移除房价为50.0的离群点/上限值</span></span><br><span class="line">x = x[y &lt; <span class="number">50</span>]  <span class="comment"># 过滤特征X</span></span><br><span class="line">y = y[y &lt; <span class="number">50</span>]  <span class="comment"># 同时过滤目标y，确保数据对齐</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 分割数据集为训练集和测试集</span></span><br><span class="line"><span class="comment"># test_size=0.3表示测试集占30%，random_state=0确保每次分割结果都一样</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=<span class="number">0.3</span>, random_state=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 使用训练集数据来训练（拟合）线性回归模型</span></span><br><span class="line">lin_reg.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 6. 使用测试集评估模型性能，返回R²分数（决定系数）</span></span><br><span class="line">lin_reg.score(x_test, y_test)</span><br></pre></td></tr></table></figure>

<h2 id="3-多项式回归"><a href="#3-多项式回归" class="headerlink" title="3 多项式回归"></a>3 多项式回归</h2><h3 id="3-1-核心思想"><a href="#3-1-核心思想" class="headerlink" title="3.1 核心思想"></a>3.1 核心思想</h3><p><strong>多项式回归 (Polynomial Regression)</strong> 是一种通过构建<strong>曲线</strong>来拟合数据点的方法。当数据点的分布趋势明显不是一条直线，而是呈现出某种弯曲的形态时，多项式回归就非常有用。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718100922.png" alt="image.png|794"></p>
<p><strong>多项式回归的本质</strong>，通过变量代换，把一个非线性的数据关系，巧妙地转化成一个可以用线性回归方法来解决的问题。</p>
<p>为了产生讲解需要的数据集，我们在函数 $y=x^2$ 的基础上，增加了正态分布的噪音 $\epsilon$，</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721214549.png" alt="image.png"></p>
<p>然后利用联合分布 $P(X,Y)$ 产生了如下的一些点：</p>
<blockquote>
<p> 联合分布描述的是两个或多个随机变量在同一时间点，各自取特定值的概率。</p>
</blockquote>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721215029.png" alt="image.png"></p>
<p>我们希望用如下假设空间进行拟合：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721215049.png" alt="image.png"></p>
<p>所以需要将上述 xy 坐标系下的点通过 $z=x^2$ 转换到 $zy$ 坐标系下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721215116.png" alt="image.png"></p>
<p>最后<strong>在 $zy$ 坐标系下通过最小二乘法</strong>的矩阵算法完成线性回归，<strong>转换回 $xy$ 坐标系下</strong>得到抛物线回归：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721215203.png" alt="image.png"></p>
<h3 id="3-2-代码实现"><a href="#3-2-代码实现" class="headerlink" title="3.2 代码实现"></a>3.2 代码实现</h3><figure class="highlight python"><figcaption><span>fold</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入所需的库和模块  </span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split  </span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification  </span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression  </span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># --- 1. 数据准备 ---# 设置随机种子以保证每次运行结果都一样  </span></span><br><span class="line">np.random.seed(<span class="number">0</span>)  </span><br><span class="line"><span class="comment"># 生成200个样本，每个样本2个特征(x₁, x₂)，数据服从标准正态分布  </span></span><br><span class="line">X = np.random.normal(<span class="number">0</span>, <span class="number">1</span>, size=(<span class="number">200</span>, <span class="number">2</span>))  </span><br><span class="line"><span class="comment"># 根据一个圆形的边界 (x₁² + x₂² &lt; 2) 来生成目标类别y  </span></span><br><span class="line"><span class="comment"># 如果点在圆内，y=1；如果在圆外或圆上，y=0  </span></span><br><span class="line"><span class="comment"># 这创建了一个用直线无法分开，但用曲线可以分开的数据集  </span></span><br><span class="line">y = np.array((X[:,<span class="number">0</span>]**<span class="number">2</span>) + (X[:,<span class="number">1</span>]**<span class="number">2</span>) &lt; <span class="number">2</span>, dtype=<span class="string">&#x27;int&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># 将数据集分割为训练集和测试集 (70%训练，30%测试)  </span></span><br><span class="line"><span class="comment"># stratify=y确保分割后训练集和测试集中的类别比例与原始数据相同  </span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">233</span>, stratify=y)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># --- 2. 数据可视化 ---# 将训练数据 x_train 中的两个类别分开，以便用不同颜色绘制  </span></span><br><span class="line"><span class="comment"># Class 1: y_train == 1 的点 (圆圈内)  </span></span><br><span class="line">class_1 = x_train[y_train==<span class="number">1</span>]  </span><br><span class="line"><span class="comment"># Class 0: y_train == 0 的点 (圆圈外)  </span></span><br><span class="line">class_0 = x_train[y_train==<span class="number">0</span>]  </span><br><span class="line"><span class="comment"># 创建一个图形  </span></span><br><span class="line"><span class="comment"># 绘制 Class 1 的点  </span></span><br><span class="line">plt.scatter(class_1[:, <span class="number">0</span>], class_1[:, <span class="number">1</span>], color=<span class="string">&#x27;red&#x27;</span>, marker=<span class="string">&#x27;o&#x27;</span>, label=<span class="string">&#x27;Class 1 (圆圈内)&#x27;</span>)  </span><br><span class="line"><span class="comment"># 绘制 Class 0 的点  </span></span><br><span class="line">plt.scatter(class_0[:, <span class="number">0</span>], class_0[:, <span class="number">1</span>], color=<span class="string">&#x27;blue&#x27;</span>, marker=<span class="string">&#x27;x&#x27;</span>, label=<span class="string">&#x27;Class 0 (圆圈外)&#x27;</span>)  </span><br><span class="line">plt.show() <span class="comment"># 显示图形</span></span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250721220603.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 3. 特征工程 ---# 创建一个用于生成二次多项式特征的转换器  </span></span><br><span class="line">poly = PolynomialFeatures(degree=<span class="number">2</span>)  </span><br><span class="line"><span class="comment"># 在训练数据上拟合(fit)转换器，让它“学习”如何从(x₁, x₂)生成多项式特征  </span></span><br><span class="line">poly.fit(x_train)  </span><br><span class="line"><span class="comment"># 使用拟合好的转换器，分别对训练集和测试集进行转换  </span></span><br><span class="line">z_train = poly.transform(x_train)  <span class="comment"># 转换后的训练集  </span></span><br><span class="line">z_test = poly.transform(x_test) <span class="comment"># 转换后的测试集</span></span><br></pre></td></tr></table></figure>

<blockquote>
<p> <strong>注意</strong>：这里无法可视化，因为我们无法直接“看到”六维空间里的数据分布</p>
</blockquote>
<p><code>PolynomialFeatures(degree=2)</code> 函数生成了以下所有 2 阶及以下的多项式组合（不包括常数项 1，因为模型会自己处理截距）：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718172700.png" alt="image.png"></p>
<p>变量替换，将非线性转换为线性：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718172926.png" alt="image.png"></p>
<ul>
<li>令 $z_1 = x_1$</li>
<li>令 $z_2 = x_2$</li>
<li>令 $z_3 = x_1^2$</li>
<li>令 $z_4 = x_2^2$</li>
<li>令 $z_5 = x_1x_2$</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718172942.png" alt="image.png"></p>
<p>然后再处理这个线性模型即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># --- 3. 模型训练 ---# 创建一个逻辑回归分类器实例  </span></span><br><span class="line">clf = LogisticRegression()  </span><br><span class="line"><span class="comment"># 使用转换后的、包含多项式特征的训练数据(x2)来训练逻辑回归模型  </span></span><br><span class="line">clf.fit(z_train, y_train)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># --- 4. 模型评估 ---# 计算模型在训练集上的准确率  </span></span><br><span class="line">clf.score(z_train, y_train)  </span><br><span class="line"><span class="comment"># 在转换后的测试集(x2t)上评估模型的泛化能力，这是更重要的指标  </span></span><br><span class="line">clf.score(z_test, y_test)  </span><br><span class="line">  </span><br><span class="line"><span class="comment"># --- 5. 查看模型参数 ---# 查看模型最终学习到的系数(β)。  </span></span><br><span class="line"><span class="comment"># 这个系数数组中的每一个值，都对应一个多项式特征(1, x₁, x₂, x₁², x₁x₂, x₂²)的权重。  </span></span><br><span class="line">clf.coef_</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718172242.png" alt="image.png"></p>
<p><code>clf.coef_</code> 中六个数字的含义分别是：</p>
<table>
<thead>
<tr>
<th>系数值 (从 <code>clf.coef_</code> 输出)</th>
<th>对应的特征</th>
<th>含义</th>
</tr>
</thead>
<tbody><tr>
<td><strong>-0.000814</strong></td>
<td>常数项 <code>1</code></td>
<td>这是模型为常数项学习到的系数¹</td>
</tr>
<tr>
<td><strong>0.078524</strong></td>
<td>$x_1$​</td>
<td>原始特征 $x_1$​ 的系数 (β1​)</td>
</tr>
<tr>
<td><strong>-0.271674</strong></td>
<td>$x_2$​</td>
<td>原始特征 $x_2$​ 的系数 (β2​)</td>
</tr>
<tr>
<td><strong>-2.861…</strong></td>
<td>$x_1^2$​</td>
<td>特征 $x_1^2$​ 的系数 (β3​)</td>
</tr>
<tr>
<td><strong>0.065163</strong></td>
<td>$x_1​x_2$​</td>
<td>交互特征 $x_1​x_2$​ 的系数 (β4​)</td>
</tr>
<tr>
<td><strong>-2.644352</strong></td>
<td>$x_2^2$​</td>
<td>特征 $x_2^2$​ 的系数 (β5​)</td>
</tr>
</tbody></table>
<p>这个模型最终学到的<strong>决策边界</strong> <code>z=0</code>，其方程近似为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718172552.png" alt="image.png|255"></p>
<p>这是一个<strong>椭圆</strong>的方程！由于两个系数 <code>-2.86</code> 和 <code>-2.64</code> 非常接近，所以这个椭圆非常接近一个<strong>正圆</strong>。</p>
<h1 id="回归模型评估指标"><a href="#回归模型评估指标" class="headerlink" title="回归模型评估指标"></a>回归模型评估指标</h1><table>
<thead>
<tr>
<th>指标</th>
<th>衡量什么</th>
<th>单位</th>
<th>核心特点</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MSE</strong></td>
<td>误差的平方的均值</td>
<td><code>y</code> 的单位的平方</td>
<td>对大误差敏感，单位不直观</td>
</tr>
<tr>
<td><strong>RMSE</strong></td>
<td>误差的均值（放大版）</td>
<td>与 <code>y</code> 的单位相同</td>
<td>单位直观，对大误差敏感</td>
</tr>
<tr>
<td><strong>MAE</strong></td>
<td>误差的绝对值的均值</td>
<td>与 <code>y</code> 的单位相同</td>
<td>单位直观，对离群点鲁棒</td>
</tr>
<tr>
<td><strong>R-squared</strong></td>
<td>模型对数据方差的解释程度</td>
<td><strong>无单位</strong>（百分比）</td>
<td>衡量“拟合优度”，值越高越好</td>
</tr>
</tbody></table>
<ul>
<li><strong><code>mean_squared_error</code> (MSE)</strong>: 计算<strong>均方误差</strong>。</li>
<li><strong><code>mean_absolute_error</code> (MAE)</strong>: 计算<strong>平均绝对误差</strong>。</li>
<li><strong><code>r2_score</code> (R² Score)</strong>: 计算 <strong>R-squared</strong>，衡量模型的拟合优度。</li>
</ul>
<h3 id="0-1-MSE-RMSE-MAE"><a href="#0-1-MSE-RMSE-MAE" class="headerlink" title="0.1 MSE/RMSE/MAE"></a>0.1 MSE/RMSE/MAE</h3><p>前三个没啥好说的，见下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718145738.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error  </span><br><span class="line">mean_squared_error(y_real, y_predict)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718145743.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mean_squared_error(y_real, y_predict, squared=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718145748.png" alt="image.png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error  </span><br><span class="line">mean_absolute_error(y_real, y_predict)</span><br></pre></td></tr></table></figure>

<h3 id="0-2-R-方-R-squared"><a href="#0-2-R-方-R-squared" class="headerlink" title="0.2 R 方 (R-squared)"></a>0.2 R 方 (R-squared)</h3><p><code>R²</code> 是一个与上述三个指标完全不同维度的指标。MSE/RMSE/MAE 衡量的是<strong>绝对误差的大小</strong>，而 <code>R²</code> 衡量的是<strong>模型对数据变异性的解释程度</strong>。</p>
<p><strong>核心问题</strong>: “我的模型解释了目标变量 <code>y</code> 总变动的百分之多少？”</p>
<p><strong>公式</strong>:</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718145851.png" alt="image.png"></p>
<ul>
<li><code>SSE</code> (Sum of Squared Errors): 模型<strong>未能解释</strong>的误差部分（残差平方和）。</li>
<li><code>SST</code> (Total Sum of Squares): 数据<strong>总的</strong>变异量（可以理解为，如果我们用 <code>y</code> 的平均值 <code>ȳ</code> 这个最简单的模型去预测，产生的总误差）。</li>
</ul>
<p><strong>解读</strong>:</p>
<ul>
<li><code>R²</code> 的取值范围通常在 <code>[0, 1]</code> 之间。</li>
<li><code>R²</code> <strong>越接近 1</strong>，表示模型的解释能力越强。例如 <code>R²=0.85</code> 意味着模型能够解释目标变量 85% 的方差（变动），只有 15% 的变动是模型无法解释的。</li>
<li><code>R²</code> <strong>越接近 0</strong>，表示模型解释能力很差，基本跟用平均值去预测差不多。</li>
<li><code>R²</code> <strong>可能是负数</strong>：如果 <code>R²</code> 为负，说明你的模型还不如直接用平均值进行预测，这是一个极差的模型。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">注意事项:</span> <code>R²</code> 有一个“缺陷”，即当你不断向模型中添加新的特征时，<code>R²</code> 的值永远不会下降，只会上升或不变，即使新特征毫无用处。</p>
<ul>
<li><strong>为什么会这样？</strong> 想象一下，你在考试，每多给你一本参考书（即使是无关的闲书），你的分数也不可能变差。最坏的情况是你完全不用这本烂书，分数不变；最好的情况是你碰巧从书里找到了一个能用上的知识点，分数就略有提高。 模型也是一样，每加入一个新特征，模型在拟合训练数据时总能或多或少地利用这个新特征中的一些随机信息，来使残差平方和 (SSE) 稍微降低一点点，从而让 <code>R²</code> 上升。</li>
<li><strong>这会导致什么问题？</strong> 这会严重误导我们。我们可能会错误地认为，一个包含大量特征的复杂模型，仅仅因为它有更高的 <code>R²</code>，就一定是一个更好的模型。但这很可能只是<strong>过拟合 (Overfitting)</strong> 的假象。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score  </span><br><span class="line">r2_score(y_real, y_predict)</span><br><span class="line"><span class="comment"># 模型自带的评价方法就是R方</span></span><br><span class="line">model.score(x_test, y_test)</span><br></pre></td></tr></table></figure>

<h3 id="0-3-调整-R-方"><a href="#0-3-调整-R-方" class="headerlink" title="0.3 调整 R 方"></a>0.3 调整 R 方</h3><p><strong>调整 R 方</strong>就是为了修正普通 <code>R²</code> 的上述缺陷而设计的。</p>
<p><strong>核心思想</strong>： 在计算 <code>R²</code> 的基础上，对模型的复杂度（即特征的数量）施加一个“惩罚 (Penalty)”。</p>
<ul>
<li>当你向模型中添加一个<strong>有用的</strong>新特征时，它对模型解释能力的提升会<strong>大于</strong>因增加复杂度而受到的惩罚，所以调整 R 方会<strong>上升</strong>。</li>
<li>当你向模型中添加一个<strong>无用的</strong>新特征（比如一个完全随机的数列）时，它对模型解释能力的提升微乎其微，<strong>不足以抵消</strong>增加复杂度所带来的惩罚，所以调整 R 方会<strong>下降</strong>。</li>
</ul>
<p><strong>公式</strong>:</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250718151023.png" alt="image.png"></p>
<ul>
<li><code>R²</code>: 就是普通的决定系数。</li>
<li><code>m</code>: 数据样本的总数。</li>
<li><code>p</code>: 模型中<strong>特征（自变量）的数量</strong>。</li>
</ul>
<p><strong>惩罚项是如何运作的？</strong> 关键就在于这个惩罚因子：m−p−1m−1​</p>
<ul>
<li>当你增加一个特征时，<code>p</code> 的值会增加。</li>
<li>这会导致分母 <code>(m - p - 1)</code> 变小。</li>
<li>从而使得整个惩罚因子变大。</li>
<li>因为我们是用 <code>(1 - R²)</code> 去乘以这个变大的惩罚因子，所以被减去的部分就变多了，最终导致 <code>R²adj</code> 的值<strong>下降</strong>（或者因为 <code>R²</code> 的微弱上升而被抵消）。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">如何使用和解释调整 R 方？</span></p>
<ol>
<li><strong>值的大小</strong>：调整 R 方的值永远<strong>小于或等于</strong>普通 R 方。</li>
<li><strong>模型比较</strong>：调整 R 方是<strong>比较含有不同数量特征的模型</strong>的绝佳工具。在多个模型中，我们通常倾向于选择<strong>调整 R 方更高</strong>的那个，因为它代表了模型在“解释能力”和“简约性”之间取得了更好的平衡。</li>
<li><strong>判断特征有效性</strong>：当你在模型中加入一个新特征后：<ul>
<li>如果 <code>R²</code> 显著上升，同时 <code>R²adj</code> 也上升，说明这个新特征是<strong>有用的</strong>。</li>
<li>如果 <code>R²</code> 上升（或者基本不变），但 <code>R²adj</code> 反而<strong>下降</strong>了，这就是一个强烈的信号，说明你刚刚加入的这个特征是<strong>无用的“垃圾”特征</strong>，它只会增加模型的复杂度而不能带来实质性的好处，应该考虑将其移除。</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://VernalScenery.github.io">Scenery</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://vernalscenery.github.io/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">https://vernalscenery.github.io/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://VernalScenery.github.io" target="_blank">春和景明的记事本</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/./img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/05_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%86%B5(Entropy)/" title="05_机器学习_熵(Entropy)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">05_机器学习_熵(Entropy)</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">01_机器学习_感知机</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/" title="00_机器学习_KNN算法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-04</div><div class="title">00_机器学习_KNN算法</div></div></a></div><div><a href="/2025/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E8%A7%88%E3%80%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="00_机器学习_概览、环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-03</div><div class="title">00_机器学习_概览、环境搭建</div></div></a></div><div><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">01_机器学习_感知机</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="04_机器学习_基础概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">04_机器学习_基础概念</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="03_机器学习_逻辑回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">03_机器学习_逻辑回归</div></div></a></div><div><a href="/2025/07/23/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/07_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA(MLP)/" title="07_机器学习_多层感知机(MLP)"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-23</div><div class="title">07_机器学习_多层感知机(MLP)</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Scenery</div><div class="author-info__description">今天不想跑，所以才去跑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">13</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chjm0121" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1595718686@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E5%92%8C%E5%9B%9E%E5%BD%92"><span class="toc-text">分类和回归</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%80%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">1 一元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E6%A0%B8%E5%BF%83%E7%9B%AE%E6%A0%87-%EF%BC%9A%E6%89%BE%E5%88%B0%E6%9C%80%E4%BD%B3%E6%8B%9F%E5%90%88%E7%9B%B4%E7%BA%BF"><span class="toc-text">1.1 核心目标 ：找到最佳拟合直线</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E8%B4%B4%E5%90%88%E5%BA%A6%E7%9A%84%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87%EF%BC%9A%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">1.2 贴合度的评价指标：损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">1.3 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-text">2 多元线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%A4%9A%E5%85%83%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.1 多元线性回归的模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">2.2 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B"><span class="toc-text">2.3 正规方程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">2.4 代码实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%9B%9E%E5%BD%92"><span class="toc-text">3 多项式回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">3.1 核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">3.2 代码实现</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="toc-text">回归模型评估指标</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-1-MSE-RMSE-MAE"><span class="toc-text">0.1 MSE&#x2F;RMSE&#x2F;MAE</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-2-R-%E6%96%B9-R-squared"><span class="toc-text">0.2 R 方 (R-squared)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#0-3-%E8%B0%83%E6%95%B4-R-%E6%96%B9"><span class="toc-text">0.3 调整 R 方</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/05/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/02_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_Numpy/" title="02_数据分析_Numpy">02_数据分析_Numpy</a><time datetime="2025-08-04T21:11:38.000Z" title="发表于 2025-08-05 05:11:38">2025-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/08/05/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/01_%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90_python%E8%AF%AD%E6%B3%95/" title="01_数据分析_python语法">01_数据分析_python语法</a><time datetime="2025-08-04T18:44:45.000Z" title="发表于 2025-08-05 02:44:45">2025-08-05</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/13_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" title="13_机器学习_概率图模型">13_机器学习_概率图模型</a><time datetime="2025-07-29T00:29:02.000Z" title="发表于 2025-07-29 08:29:02">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/11_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%81%9A%E7%B1%BB/" title="11_机器学习_聚类">11_机器学习_聚类</a><time datetime="2025-07-28T00:19:53.000Z" title="发表于 2025-07-28 08:19:53">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维">12_机器学习_降维</a><time datetime="2025-07-27T20:48:42.000Z" title="发表于 2025-07-28 04:48:42">2025-07-28</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./img/1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Scenery</div><div class="footer_custom_text"><div>波澜不惊</div><div class="footer-div"><img class="footer-icon" src="./img/备案图标.png"><a class="footer-a" target="_blank" rel="noopener" href="http://beian.miit.gov.cn/">皖ICP备2021016944号-1</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>