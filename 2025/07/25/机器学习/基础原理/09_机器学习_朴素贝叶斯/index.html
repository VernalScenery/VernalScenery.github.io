<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>09_机器学习_朴素贝叶斯 | 春和景明的记事本</title><meta name="author" content="Scenery"><meta name="copyright" content="Scenery"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="概率论的基本概念1 样本空间和事件集合是数学中最基础的一个定义，样本空间是特殊的集合，事件是样本空间的子集，如下图所示：  由于样本空间、事件本质上都是集合，所以它们不仅完全具备集合的特性，还可以进行集合运算。 对于抛掷骰子的样本空间 $S&amp;#x3D;{1,2,3,4,5,6}$ 而言，可以有如下一些事件"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://vernalscenery.github.io/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.13.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.1/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '09_机器学习_朴素贝叶斯',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-05 14:17:15'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/style.css"><meta name="generator" content="Hexo 5.4.2"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/./img/1.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/./img/1.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="春和景明的记事本"><span class="site-name">春和景明的记事本</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 归类</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">09_机器学习_朴素贝叶斯</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-07-25T03:08:47.000Z" title="发表于 2025-07-25 11:08:47">2025-07-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-08-05T06:17:15.000Z" title="更新于 2025-08-05 14:17:15">2025-08-05</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>20分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="09_机器学习_朴素贝叶斯"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="概率论的基本概念"><a href="#概率论的基本概念" class="headerlink" title="概率论的基本概念"></a>概率论的基本概念</h1><h2 id="1-样本空间和事件"><a href="#1-样本空间和事件" class="headerlink" title="1 样本空间和事件"></a>1 样本空间和事件</h2><p><strong>集合</strong>是数学中最基础的一个定义，<strong>样本空间</strong>是特殊的集合，<strong>事件</strong>是样本空间的子集，如下图所示：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111002.png" alt="image.png"></p>
<p>由于样本空间、事件本质上都是集合，所以它们不仅完全具备集合的特性，还<strong>可以进行集合运算</strong>。</p>
<p>对于<strong>抛掷骰子的样本空间</strong> $S={1,2,3,4,5,6}$ 而言，可以有如下一些<strong>事件</strong></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111400.png" alt="image.png"></p>
<p>表展示了上述<strong>事件之间的某些关系、运算</strong>，以及对应的概率论解释：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111225.png" alt="image.png|599"></p>
<h2 id="2-概率的性质"><a href="#2-概率的性质" class="headerlink" title="2 概率的性质"></a>2 概率的性质</h2><blockquote>
<p> 其实就是类比集合的计算</p>
</blockquote>
<h3 id="2-1-概率的公理化定义"><a href="#2-1-概率的公理化定义" class="headerlink" title="2.1 概率的公理化定义"></a>2.1 概率的公理化定义</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111453.png" alt="image.png"></p>
<h3 id="2-2-有限可加性"><a href="#2-2-有限可加性" class="headerlink" title="2.2 有限可加性"></a>2.2 有限可加性</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111609.png" alt="image.png"></p>
<h3 id="2-3-概率的单调性"><a href="#2-3-概率的单调性" class="headerlink" title="2.3 概率的单调性"></a>2.3 概率的单调性</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111621.png" alt="image.png"></p>
<p>如下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111658.png" alt="image.png"></p>
<h3 id="2-4-逆事件的概率"><a href="#2-4-逆事件的概率" class="headerlink" title="2.4 逆事件的概率"></a>2.4 逆事件的概率</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111721.png" alt="image.png"></p>
<h3 id="2-5-概率的容斥定理"><a href="#2-5-概率的容斥定理" class="headerlink" title="2.5 概率的容斥定理"></a>2.5 概率的容斥定理</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111728.png" alt="image.png"></p>
<p>如下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111750.png" alt="image.png"></p>
<p>推广到三个事件 A、B 和 C 的情况，如下图所示：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111832.png" alt="image.png"></p>
<p>结合上图，可以直观地得到并理解容斥定理的推广：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111846.png" alt="image.png"></p>
<p>进而可以推广到 n 个事件 $A_1、A_2、\cdots、A_n$ 的情况，其计算过程仍然遵循 “ 包容 - 排斥 “ 的交替模式：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725111902.png" alt="image.png"></p>
<h2 id="3-条件概率"><a href="#3-条件概率" class="headerlink" title="3 条件概率"></a>3 条件概率</h2><h3 id="3-1-严格定义"><a href="#3-1-严格定义" class="headerlink" title="3.1 严格定义"></a>3.1 严格定义</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725112030.png" alt="image.png"></p>
<p>条件概率实际上是**将原样本空间缩小为了事件 $B$**，因为除了一个 $P(B)$</p>
<p>如下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725112252.png" alt="image.png"></p>
<p>两者最核心的区别在于有着<strong>不同的样本空间</strong>：</p>
<ul>
<li>$P(A)$ 的样本空间为 S，这是一个全局的空间，包含所有可能性</li>
<li>$P(A|B)$ 的样本空间为 B，相对 S 而言这是一个局部空间，排除了一些可能性</li>
</ul>
<p>$P(A)$ 也可以被视为一种特殊的条件概率，因为 $P(A)=\frac{P(A)}{P(S)}=P(A|S)$。</p>
<h3 id="3-2-乘法公式"><a href="#3-2-乘法公式" class="headerlink" title="3.2 乘法公式"></a>3.2 乘法公式</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725112602.png" alt="image.png"></p>
<p>上述乘法公式看似复杂，实际上描述了一个分步完成的过程：n 个事件同时发生的概率，等于这 n 个事件依次发生概率的乘积。这与计数方法中介绍的乘法原理类似，只是该公式运用于概率计算：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725112705.png" alt="image.png"></p>
<h3 id="3-3-全概率公式"><a href="#3-3-全概率公式" class="headerlink" title="3.3 全概率公式"></a>3.3 全概率公式</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725112744.png" alt="image.png"></p>
<p>上述定义的分割指的是互不重叠、完全覆盖样本空间 S 上的一组事件。</p>
<p>如下图中的 $A_1$、$A_2$、$A_3$ 和 $A_4$ 就是样本空间 S 的一个分割。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725112805.png" alt="image.png|373"></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725141546.png" alt="image.png"></p>
<p>全概率公式表明，若要计算某事件 B 的发生概率，我们需要考虑所有可以导致其发生的条件。具体来说，B 的发生概率，等于所有条件的发生概率，与 B 在这些条件下发生概率的乘积之和：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725141618.png" alt="image.png"></p>
<p>如下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725142024.png" alt="image.png"></p>
<p>故可得</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725142105.png" alt="image.png|549"></p>
<h2 id="4-贝叶斯公式"><a href="#4-贝叶斯公式" class="headerlink" title="4 贝叶斯公式"></a>4 贝叶斯公式</h2><h3 id="4-1-严格定义"><a href="#4-1-严格定义" class="headerlink" title="4.1 严格定义"></a>4.1 严格定义</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725142214.png" alt="image.png"></p>
<p>贝叶斯公式是概率论中的著名公式，它深刻揭示了人类认知的两大特质：</p>
<ul>
<li><strong>基本比率谬误 (Base Rate Fallacy)</strong>: 人们在做决策时，常常会过分关注于具体、生动的“刻板印象”，而忽略了更重要的“基本比率”（即某件事发生的普遍概率），从而导致认知偏差。</li>
<li><strong>认知迭代 (Cognitive Iteration)</strong>: 贝叶斯公式提供了一个通过新信息不断修正、更新已有知识的理性框架，描述了我们学习和适应的思维过程。</li>
</ul>
<h3 id="4-2-一个案例"><a href="#4-2-一个案例" class="headerlink" title="4.2 一个案例"></a>4.2 一个案例</h3><p>当你见到图 2 中的女士时，你认为她更可能是<strong>大学教授</strong>，还是<strong>公司职员</strong>？</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725143318.png" alt="image.png|302"></p>
<p><span style="color: #badc58; font-weight: 550;">1、基本比率谬误：</span></p>
<p>单从气质上看更可能是大学教授，但是这种刻板印象却忽略了更重要的“基本比率”，即大学教授比公司员工少的多。</p>
<p>由于在刻板印象中，大学教授更容易有着“知性、文雅的外表”。因此设定 $\displaystyle\frac{1}{2}$ 的大学教授具有这一特征，同时设定仅 $\displaystyle\frac{1}{8}$ 的公司职员具有这一特征。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725143744.png" alt="image.png"></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725143720.png" alt="image.png"></p>
<p>具体解释如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725144443.png" alt="image.png|638"></p>
<p><span style="color: #badc58; font-weight: 550;">2、认知迭代</span></p>
<p>可以换个角度来看这个案例，一开始面对“某人是否为大学教授”的问题时，我们能依据的只有大学教授在总人口中的概率 $\displaystyle P(A_1)=\frac{|A_1|}{|S|}=\frac{1}{21}$。</p>
<p>但看到图 2 后，新的信息 $B=\text{“知性、文雅的外表”}$ 出现了，我们的认知立即重构，如图 5 所示。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725144808.png" alt="image.png"></p>
<p>故有：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725144925.png" alt="image.png"></p>
<ul>
<li><strong>先验概率</strong> $P(A_1)$：这是大学教授在总人群中的概率</li>
<li><strong>证据强度</strong> $\displaystyle \frac{P(B|A_1)}{P(B)}$：这是大学教授中“知性外表”与总体中“知性外表”的概率之比<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145133.png" alt="image.png"></li>
<li><strong>后验概率</strong> $P(A_1|B)$：这是在“知性外表”条件下，某人是大学教授的概率。</li>
</ul>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145206.png" alt="image.png"></p>
<h2 id="5-事件的独立性"><a href="#5-事件的独立性" class="headerlink" title="5 事件的独立性"></a>5 事件的独立性</h2><h3 id="5-1-独立的定义"><a href="#5-1-独立的定义" class="headerlink" title="5.1 独立的定义"></a>5.1 独立的定义</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145244.png" alt="image.png"></p>
<p>上述定义可通过条件概率来解释。当 $P(B)=P(B|A)$ 时，意味着事件 $A$ 的发生对事件 $B$ 的概率没有影响，换句话说，$A$ 和 $B$ 之间没有关系。</p>
<p>结合上乘法公式，就可推导出独立的定义：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145415.png" alt="image.png"></p>
<h3 id="5-2-独立的价值"><a href="#5-2-独立的价值" class="headerlink" title="5.2 独立的价值"></a>5.2 独立的价值</h3><p>在概率论中，独立性的一个显著价值在于简化计算。以下表格展示了独立性对运算过程的影响：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145652.png" alt="image.png"></p>
<h3 id="5-3-三个事件的独立性"><a href="#5-3-三个事件的独立性" class="headerlink" title="5.3 三个事件的独立性"></a>5.3 三个事件的独立性</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145916.png" alt="image.png"></p>
<p>关于定义 1 ，需要强调的是，事件 A、B 和 C 的两两独立性不能推出它们的相互独立性。</p>
<p>一个通俗的例子是图 1 中展示的波罗梅奥环（Borromean Rings）。仔细观察可以发现，其中任意两环没有交叉、相互分离，因此可称之为“两两独立”；然而，这三个环却彼此交叉、无法分开，因此并不是“相互独立”的。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145949.png" alt="image.png"></p>
<h3 id="5-4-多个事件的独立性"><a href="#5-4-多个事件的独立性" class="headerlink" title="5.4 多个事件的独立性"></a>5.4 多个事件的独立性</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725145846.png" alt="image.png"></p>
<h3 id="5-5-条件独立"><a href="#5-5-条件独立" class="headerlink" title="5.5 条件独立"></a>5.5 条件独立</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725150031.png" alt="image.png"></p>
<p>理解条件独立（定义 1 ）可借助图 1 所示的例子：两台服务器（标记为 1 号和 2 号）连接在同一个路由器上。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725150206.png" alt="image.png"></p>
<p>定义如下事件：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725150229.png" alt="image.png"></p>
<p>当路由器正常时，两台服务器的工作状态取决于自身，不会相互影响。即在 $RG$ 条件下，$S_1$ 和 $S_2$ 条件独立：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725150316.png" alt="image.png"></p>
<p>而当路由器异常时（响应缓慢、数据丢失等），通常会同时影响到两台服务器，使它们的故障事件不再独立。</p>
<p>也就是说，在 $RB$ 条件下，$S_1$ 和 $S_2$ 不具备条件独立性，即：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725150332.png" alt="image.png"></p>
<h1 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h1><h2 id="1-生成模型的工作原理"><a href="#1-生成模型的工作原理" class="headerlink" title="1 生成模型的工作原理"></a>1 生成模型的工作原理</h2><table>
<thead>
<tr>
<th>特性</th>
<th>判别模型 (Discriminative Models)</th>
<th>生成模型 (Generative Models)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>学习目标</strong></td>
<td>直接学习决策函数或条件概率 $P(y∣x)$。</td>
<td>学习数据的<strong>联合概率分布</strong> $P(x,y)$，也即 $P(x\cap y)$，再通过<strong>贝叶斯公式</strong>求出 $P(y∣x)$</td>
</tr>
<tr>
<td><strong>解决问题</strong></td>
<td>“给定输入 x，它属于类别 y 的概率是多少？”</td>
<td>“类别 y 的数据特征 x 是什么样子的？”</td>
</tr>
<tr>
<td><strong>核心任务</strong></td>
<td>寻找类别间的<strong>决策边界</strong>（Decision Boundary）</td>
<td>学习每个类别数据的<strong>分布模型</strong>（Data Distribution）</td>
</tr>
<tr>
<td><strong>数据流向</strong></td>
<td><strong>单向的</strong>：从特征 x 到标签 y</td>
<td><strong>双向的</strong>：可从标签 y 生成对应的特征 x</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td><strong>分类精度高</strong>：目标明确，直击分类任务，通常需要的数据量更少，计算也相对简单。</td>
<td><strong>信息量更丰富</strong>：能反映数据本身的分布，可以用来<strong>生成新数据</strong>，天然适合处理缺失值和异常点检测。</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td><strong>无法生成数据</strong>：因为它没有学习数据本身的样子。</td>
<td><strong>学习更复杂</strong>：需要学习更多信息，当数据量不足时可能学得不好，分类精度有时会低于判别模型。</td>
</tr>
<tr>
<td><strong>典型算法</strong></td>
<td>- 逻辑回归 (Logistic Regression)  <br>- 支持向量机 (SVM)  <br>- 传统的神经网络/多层感知机 (NN/MLP)  <br>- 条件随机场 (CRF)</td>
<td>- 朴素贝叶斯 (Naive Bayes)  <br>- 高斯混合模型 (GMM)  <br>- 隐马尔可夫模型 (HMM)  <br>- 生成对抗网络 (GANs)  <br>- 变分自编码器 (VAEs)</td>
</tr>
</tbody></table>
<p>生成模型的根本目标是学习数据的<strong>联合概率分布</strong> $P(x,y)$</p>
<p>但直接学习 <code>P(x, y)</code> 可能很困难，因此，生成模型采用了一种更巧妙的“迂回”策略。根据概率论中的链式法则，联合概率可以被分解为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725155307.png" alt="image.png"></p>
<p>它告诉我们，要得到联合概率 <code>P(x, y)</code>，我们只需要学习两样东西：</p>
<ul>
<li><strong>P(y)：类别先验概率 (Class Prior Probability)</strong><ul>
<li><strong>含义</strong>：在所有数据中，某个类别 <code>y</code> 本身出现的概率是多少？</li>
<li><strong>如何求</strong>：这很简单，直接在训练数据里统计频率即可。例如，在垃圾邮件分类中，<code>P(y=垃圾邮件) = (垃圾邮件的总数) / (邮件总数)</code>。</li>
</ul>
</li>
<li><strong>P(x∣y)：类条件概率 (Class-Conditional Probability)</strong><ul>
<li><strong>含义</strong>：<strong>在已知类别为 <code>y</code> 的条件下</strong>，特征 <code>x</code> 会呈现出什么样的分布？</li>
<li>这里假设分类为<strong>二元分类</strong>，分布特征为<strong>正态分布</strong>，如下图：<br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725155742.png" alt="|718"></li>
<li>红色的正态分布曲线，描述的就是“<strong>如果这封邮件是垃圾邮件</strong>，那么它的特征 <code>x</code> 的概率分布是怎样的”。这即是 <code>P(x | y=垃圾邮件)</code></li>
<li>蓝色的正态分布曲线，描述的就是“<strong>如果这封邮件是正常邮件</strong>，那么它的特征 <code>x</code> 的概率分布是怎样的”。这即是 <code>P(x | y=正常邮件)</code>。</li>
</ul>
</li>
</ul>
<p>就上图而言，可以将这两个分布的交点作为边界（这里有很多细节，我们后面再讨论），这是因为当某点出现在决策边界左侧时，根据正、负类的正态分布，该点为正类的概率更大；同理在右侧的话，为负类的概率更大。比如下图中的 $\color{green}{\blacktriangle}$ 就应该判定为正类 $\color{red}{\bullet}$：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725155829.png" alt="image.png"></p>
<p>这种先根据数据集生成<strong>每个类别</strong>（如正类/负类）的<strong>分布</strong>，再得到决策边界的模型就称为<strong>生成模型</strong>（Generative model）：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725155842.png" alt="image.png"></p>
<ul>
<li><strong>类别</strong>可以是任意多个，不限于正负二元。</li>
<li>分布可以分为很多种，如：二项分布，高斯分布（正态分布）、类分布等，这对应了不同的生成模型算法（如高斯朴素贝叶斯、伯努利朴素贝叶斯等）。</li>
</ul>
<p><span style="color: #badc58; font-weight: 550;">总结：</span>判别模型和生成模型的区别如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725151636.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">下面通过一个二元分类的例子来理解一下：</span></p>
<p>贝叶斯分类器（Bayes classifier）这个生成模型，首先根据数据集 $D$ 成了<strong>正</strong>、<strong>负</strong>类的分布：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725152846.png" alt="image.png|588"></p>
<blockquote>
<p> 即求出他们在条件样本空间内的概率</p>
</blockquote>
<p>然后利用贝叶斯公式，判断某点在哪个分布的概率更高，那么该点就属于哪个类别。比如下面的不等式就说明 $(1,1)$ 点属于负类：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725153020.png" alt="image.png|631"></p>
<ul>
<li>$P(Y = +1) = 6/10$ </li>
<li>$P(Y= -1) = 4/10$</li>
</ul>
<p>这里最终的<strong>不等式比大小</strong>，就对应下图的两个交点比大小</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725162547.png" alt="image.png"></p>
<h2 id="2-朴素贝叶斯：条件独立假设"><a href="#2-朴素贝叶斯：条件独立假设" class="headerlink" title="2 朴素贝叶斯：条件独立假设"></a>2 朴素贝叶斯：条件独立假设</h2><p>上一节的数据集中只有 2 个特征，如果有 3 个特征，那么正、负类分布各有 $2^3=8$ 项需要填写</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725154536.png" alt="image.png"></p>
<p>如果有 10 个特征呢，正、负类的分布各有 $2^{10}$ 项需要填写，显然这个数据量太大了，所以必须引入<strong>条件独立假设</strong>。</p>
<p>以垃圾邮件分类数据集为例，它有两个特征“便宜”和“买”。在正常邮件中，或者在垃圾邮件中，如果这两个关键字毫无关系，那么就称为<strong>条件独立</strong>（Conditional Independence）。用代数表示即为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725154426.png" alt="image.png|369"></p>
<p>该定义可以推广到 n 个特征：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725154437.png" alt="image.png|521"></p>
<p>有了条件独立假设后，有 3 个特征的话，只需要知道下面 $2\times 3=6$ 个概率，就可以推出正类的分布：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725154656.png" alt="image.png"></p>
<p>所以，有了条件独立假设后，贝叶斯分类器需要的数据量大大减少：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725154729.png" alt="image.png"></p>
<p>加入了条件独立假设的贝叶斯分类器又称为<strong>天真贝叶斯</strong>，更常用的名字是<strong>朴素贝叶斯</strong>。</p>
<p>有了条件独立假设之后，实际上限制了正、负类分布的取值，降低了模型复杂度，从而减小了过拟合的可能性：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725160135.png" alt="image.png|583"></p>
<h2 id="3-朴素贝叶斯的问题：拉普拉斯平滑"><a href="#3-朴素贝叶斯的问题：拉普拉斯平滑" class="headerlink" title="3 朴素贝叶斯的问题：拉普拉斯平滑"></a>3 朴素贝叶斯的问题：拉普拉斯平滑</h2><p>因为朴素贝叶斯做了条件独立的假设，故有如下公式：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725160427.png" alt="image.png"></p>
<p>但如果这两项，在数据集中，一个概率为 0，那么不论另外一个概率多大，二者相乘都是 0</p>
<p>如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725161202.png" alt="image.png"></p>
<p>计算其<strong>负类</strong>的分布，可得：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725161225.png" alt="image.png|450"></p>
<p>整理为表格有：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725160736.png" alt="image.png|417"></p>
<p>要解决也很简单，在数据集中给<strong>每一种可能性</strong>分别加上 1 个数据，这样不太会影响概率大小的相对关系</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725161026.png" alt="image.png|521"></p>
<p>这里算出的结果为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725161344.png" alt="image.png"></p>
<p>这种做法称为拉普拉斯平滑（Laplacian smoothing）。</p>
<p>上面是在数据集中给<strong>每一种可能性</strong>分别加上 1 个数据，在实际问题中，我们也可以根据具体情况加入<strong>先验知识</strong>，这样在数据较少的时候可以提高泛化能力</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725161640.png" alt="image.png"></p>
<p><span style="color: #badc58; font-weight: 550;">总结</span>：<strong>拉普拉斯平滑</strong>（Laplacian smoothing），就是在原有数据的基础上增加<strong>平滑项</strong>，这样做的目的有两点：</p>
<ul>
<li>解决概率为 0 的问题</li>
<li>加入先验知识，这样在数据较少的时候可以提高泛化能力</li>
</ul>
<h2 id="4-朴素贝叶斯家族"><a href="#4-朴素贝叶斯家族" class="headerlink" title="4 朴素贝叶斯家族"></a>4 朴素贝叶斯家族</h2><p>朴素贝叶斯的核心思想是统一的：先为每个类别的数据建立一个<strong>概率分布模型</strong>，然后通过贝叶斯定理来<strong>决策</strong>。</p>
<p>然而，现实世界中的数据特征千差万别，有的像身高体重一样是连续的数字，有的像词语一样是出现的次数，还有的像“是/否”一样的二元选项。为了最好地拟合不同类型的数据，“朴素贝叶斯家族”的成员们选择了不同的<strong>特征分布假设</strong>。这个假设是它们彼此之间最核心的区别。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725163840.png" alt="image.png"></p>
<h3 id="4-1-高斯朴素贝叶斯-Gaussian-NB"><a href="#4-1-高斯朴素贝叶斯-Gaussian-NB" class="headerlink" title="4.1 高斯朴素贝叶斯 (Gaussian NB)"></a>4.1 高斯朴素贝叶斯 (Gaussian NB)</h3><ul>
<li><p><strong>特征类型</strong>: <strong>连续型 (Continuous)</strong></p>
<ul>
<li>它处理的是像身高、体重、温度、收入这样可以取任意数值的连续性特征。</li>
</ul>
</li>
<li><p><strong>特征分布假设</strong>: <strong>高斯分布 (Normal Distribution)</strong></p>
<ul>
<li>高斯朴 - 素贝叶斯做出了一个核心假设：每个类别下的每个连续特征都服从正态分布（即高斯分布，形状像一个钟形曲线）。</li>
<li>模型在训练时，会为每个类别下的每个特征计算出其均值和方差，从而确定这个“钟形曲线”的具体形状和位置。</li>
</ul>
</li>
<li><p><strong>典型例子</strong>:</p>
<ul>
<li>根据一个人的<strong>身高、体重、年龄</strong>数据来判断其性别。</li>
<li>根据房屋的<strong>面积、房间数量、楼层</strong>来判断其价格区间。</li>
</ul>
</li>
</ul>
<p>比如“变色鸢尾”有 50 个，“维吉尼亚鸢尾”有 20 个，这里只考虑花瓣长度这一个特征</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725165616.png" alt="image.png"></p>
<p>根据数据集分别算出两类的 $\mu$ 和 $\sigma$：</p>
<ul>
<li>变色鸢尾的均值 ≈ 4.26, 标准差 ≈ 0.47</li>
<li>维吉尼亚鸢尾的均值 ≈ 5.66, 标准差 ≈ 0.62</li>
</ul>
<p>然后据此画出各自的分布，显然这两类的分布是高斯分布（正态分布）</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725165551.png" alt="image.png"></p>
<p>因为两类概率不同，所以还需要给分布乘上各自类别的概率：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725165726.png" alt="image.png"></p>
<p>结果如下图：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725165713.png" alt="image.png|560"></p>
<p>以“花瓣长度” 5.1 为决策边界，其分类效果如下:</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725170010.png" alt="image.png"></p>
<p>前面都只考虑了“花瓣长度”，若把“花瓣宽度”也考虑进来：</p>
<p>根据两类鸢尾花的<strong>二维高斯分布</strong>，最终得到决策边界如下：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725170152.png" alt="image.png"></p>
<p>因为这里用到了“天真”（naive）的条件独立假设，所以称为<strong>高斯朴素贝叶斯</strong>（Gaussian Naive Bayes）。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725170253.png" alt="image.png"></p>
<h3 id="4-2-多项式朴素贝叶斯-Multinomial-NB"><a href="#4-2-多项式朴素贝叶斯-Multinomial-NB" class="headerlink" title="4.2 多项式朴素贝叶斯 (Multinomial NB)"></a>4.2 多项式朴素贝叶斯 (Multinomial NB)</h3><ul>
<li><p><strong>特征类型</strong>: <strong>离散型 (Discrete)<strong>，特指代表</strong>计数</strong>的特征。</p>
<ul>
<li>它最常用于处理那些可以被看作“出现次数”的特征。</li>
</ul>
</li>
<li><p><strong>特征分布假设</strong>: <strong>多项式分布 (Multinomial Distribution)</strong></p>
<ul>
<li>这个模型假设特征是由一个多项式分布生成的。简单来说，它关心的是一个类别下，各个特征（比如单词）出现的次数或频率。</li>
</ul>
</li>
<li><p><strong>典型例子</strong>:</p>
<ul>
<li><strong>文本分类</strong>（最经典的应用）：判断一封邮件是否为垃圾邮件。这里的特征就是邮件正文中<strong>每个单词出现的次数</strong>（词频）。例如，“免费”出现了 3 次，“中奖”出现了 2 次。</li>
</ul>
</li>
</ul>
<h3 id="4-3-伯努利朴素贝叶斯-Bernoulli-NB"><a href="#4-3-伯努利朴素贝叶斯-Bernoulli-NB" class="headerlink" title="4.3 伯努利朴素贝叶斯 (Bernoulli NB)"></a>4.3 伯努利朴素贝叶斯 (Bernoulli NB)</h3><ul>
<li><p><strong>特征类型</strong>: <strong>离散型 (二值)</strong></p>
<ul>
<li>它处理的是<strong>布尔类型</strong>或<strong>二元类型</strong>的特征，即特征的取值只有两种可能（如 0 或 1，是或否，出现或不出现）。</li>
</ul>
</li>
<li><p><strong>特征分布假设</strong>: <strong>二项分布 (Binomial Distribution)</strong></p>
<ul>
<li><em>（注：更精确地说是伯努利分布，它是二项分布 n=1 时的特例）</em></li>
<li>它不关心特征出现的次数，只关心<strong>特征是否出现</strong>。</li>
</ul>
</li>
<li><p><strong>典型例子</strong>:</p>
<ul>
<li><strong>文本分类</strong>（另一种视角）：判断一封邮件是否为垃圾邮件。这里的特征不再是每个词的词频，而是<strong>某个关键词是否在这封邮件中出现过</strong>（出现计为 1，未出现计为 0）。例如，不管“中奖”这个词出现几次，只要出现了，对应的特征值就是 1。</li>
</ul>
</li>
</ul>
<h3 id="4-4-分类朴素贝叶斯-Categorical-NB"><a href="#4-4-分类朴素贝叶斯-Categorical-NB" class="headerlink" title="4.4 分类朴素贝叶斯 (Categorical NB)"></a>4.4 分类朴素贝叶斯 (Categorical NB)</h3><ul>
<li><strong>特征类型</strong>: <strong>离散型 (多类别)</strong><ul>
<li>当你的离散特征有<strong>两个以上</strong>的类别，且这些类别没有大小之分时，就使用这个模型。</li>
</ul>
</li>
<li><strong>特征分布假设</strong>: <strong>类分布 (Categorical Distribution)</strong><ul>
<li>它假设特征服从一个有多个类别的分类分布。</li>
</ul>
</li>
<li><strong>典型例子</strong>:<ul>
<li>根据天气情况决定是否出门。特征可以是 <code>天气=&#123;晴天, 阴天, 雨天&#125;</code>，<code>风力=&#123;强, 中, 弱&#125;</code>。这些特征都是多类别的离散值。</li>
</ul>
</li>
</ul>
<h3 id="4-5-补集朴素贝叶斯-Complement-NB"><a href="#4-5-补集朴素贝叶斯-Complement-NB" class="headerlink" title="4.5 补集朴素贝叶斯 (Complement NB)"></a>4.5 补集朴素贝叶斯 (Complement NB)</h3><ul>
<li><strong>特征类型</strong>: <strong>离散型</strong> (与多项式 NB 类似)。</li>
<li><strong>特征分布假设</strong>: <strong>补集分布</strong><ul>
<li>这是对多项式朴素贝叶斯的一种改进，它特别适合处理<strong>数据不平衡</strong>问题（即某个类别的样本数量远少于其他类别）。</li>
<li>它的核心思想很巧妙：在计算一个样本属于类别 A 的概率时，它不去匹配这个样本和类别 A 的特征，而是去计算这个样本<strong>不匹配其他所有类别（即 A 的补集）</strong> 的程度。在类别不平衡的情况下，这种“反向证明”的方式往往更稳定、效果更好。</li>
</ul>
</li>
</ul>
<h2 id="5-贝叶斯方法家族"><a href="#5-贝叶斯方法家族" class="headerlink" title="5 贝叶斯方法家族"></a>5 贝叶斯方法家族</h2><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174832.png" alt="image.png"></p>
<h2 id="6-朴素贝叶斯的优缺点及适用条件"><a href="#6-朴素贝叶斯的优缺点及适用条件" class="headerlink" title="6 朴素贝叶斯的优缺点及适用条件"></a>6 朴素贝叶斯的优缺点及适用条件</h2><table>
<thead>
<tr>
<th>特性</th>
<th>详细描述</th>
</tr>
</thead>
<tbody><tr>
<td><strong>优点 (Advantages)</strong></td>
<td><strong>1. 算法简单高效</strong>：模型和训练过程都非常简单，易于实现，学习和预测的效率都很高，计算开销小。  <br>  <br><strong>2. 对小规模数据表现好</strong>：由于其生成模型的特性和较强的先验假设，即使在训练数据量不大的情况下，依然能获得不错的分类效果。  <br>  <br><strong>3. 性能稳定</strong>：算法的健壮性较好，对于缺失数据和噪声数据不敏感。  <br>  <br><strong>4. 天然支持多分类</strong>：直接支持多分类任务，无需像 SVM 等模型一样做特殊转换。</td>
</tr>
<tr>
<td><strong>缺点 (Disadvantages)</strong></td>
<td><strong>1. 核心假设过于“朴素”</strong>：其“<strong>特征条件独立性假设</strong>”在现实世界中基本不成立。例如，在文本分类中，“便宜”和“甩卖”这两个词显然是相关的，但模型会假设它们相互独立，这会影响最终的概率估算精度。  <br>  <br><strong>2. 对输入数据的表达形式敏感</strong>：不同的特征处理方式（如计数或布尔值）会对应不同的朴素贝叶斯变体（如多项式或伯努利），选择不当会影响效果。  <br>  <br><strong>3. 零概率问题</strong>：如果某个特征值在训练集中从未与某个类别一同出现过，那么其条件概率会计算为 0，这将导致整个后验概率为 0，使得分类器失效。这个问题需要通过<strong>平滑技术</strong>（如拉普拉斯平滑）来解决。</td>
</tr>
<tr>
<td><strong>适用条件 (Application Conditions)</strong></td>
<td><strong>1. 最适场景：文本分类</strong>：在垃圾邮件过滤、新闻主题分类、情感分析等领域，尽管特征（单词）之间有关联，但朴素贝叶斯的效果依然非常出色且高效。  <br>  <br><strong>2. 特征之间关联性较弱的情况</strong>：当数据的特征确实比较独立时，朴素贝叶斯能够发挥出最佳性能。  <br>  <br><strong>3. 需要一个快速、简单的基线模型时</strong>：在项目初期，可以用朴素贝叶斯快速搭建一个 baseline model，来评估问题难度和数据质量。  <br>  <br><strong>4. 不适用场景：特征之间有强交互关系的任务</strong>：当特征之间的复杂组合是决定分类结果的关键时，朴素贝叶斯会因为其“朴素”假设而表现不佳。例如，在金融风控中，“收入”和“负债”两个特征的交互关系就非常重要。</td>
</tr>
</tbody></table>
<h1 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h1><h2 id="1-参数估计的三大家族"><a href="#1-参数估计的三大家族" class="headerlink" title="1 参数估计的三大家族"></a>1 参数估计的三大家族</h2><p>参数估计就是利用我们能观测到的有限的样本数据，去推断出描述整个群体特征的、我们所不知道的模型的参数值。</p>
<p>参数估计的<span style="color: #badc58; font-weight: 550;">中心问题</span>：在给定数据 $X$ 的情况下，如何求解模型的参数 $w$？这引出了求解后验概率 $p(w∣X)$ 的三种主要思想流派：</p>
<ol>
<li>**最大似然估计 (Maximum Likelihood Estimation, MLE)**：认为世界是客观存在的，模型参数 $w$ 是一个固定但未知的常量。目标是找到一个 $w$ 使得当前这批数据 <strong>X</strong> 出现的可能性（似然）最大。</li>
<li>**最大后验估计 (Maximum A Posteriori Estimation, MAP)**：是贝叶斯思想的体现，认为参数 w 不是固定的，它本身也服从一个分布（先验分布）。目标是结合先验信念和数据证据，找到一个最可能的 $w$。</li>
<li>**贝叶斯估计 (Bayesian Estimation)**：最彻底的贝叶斯方法，它不求一个唯一的最佳参数 w，而是求解参数 w 的完整后验分布 p(w∣X)，并用这个分布进行预测。</li>
</ol>
<h2 id="2-最大似然估计-Maximum-Likelihood-Estimation-MLE"><a href="#2-最大似然估计-Maximum-Likelihood-Estimation-MLE" class="headerlink" title="2 最大似然估计 (Maximum Likelihood Estimation, MLE)"></a>2 最大似然估计 (Maximum Likelihood Estimation, MLE)</h2><p><span style="color: #badc58; font-weight: 550;">核心思想：</span> 最大化**似然函数 $p(X∣w)$**。即，寻找能让观测数据出现概率最大的那一组参数 w。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725171451.png" alt="image.png|264"></p>
<p><span style="color: #badc58; font-weight: 550;">关键点总结:</span></p>
<ul>
<li><strong>本质</strong>: 它是众多机器学习任务中<strong>损失函数的来源</strong>。最小化损失函数，往往等价于最大化某个概率模型的似然函数。</li>
<li><strong>与最小二乘法关系</strong>: 经典的<strong>最小二乘法</strong>等价于假设数据噪声服从<strong>高斯分布</strong>的<strong>最大似然估计</strong>。</li>
<li><strong>应用广泛</strong>:<ul>
<li><strong>决策树</strong>的生成过程（如用信息增益选择划分）可以看作是在使用 MLE 选择概念模型。</li>
<li><strong>神经网络</strong>中常用的<strong>交叉熵损失函数 (Cross-Entropy Loss)</strong> 本质上也是在进行最大似然估计。</li>
</ul>
</li>
</ul>
<h2 id="3-最大后验估计-Maximum-A-Posteriori-Estimation-MAP"><a href="#3-最大后验估计-Maximum-A-Posteriori-Estimation-MAP" class="headerlink" title="3 最大后验估计 (Maximum A Posteriori Estimation, MAP)"></a>3 最大后验估计 (Maximum A Posteriori Estimation, MAP)</h2><h3 id="3-1-核心思想"><a href="#3-1-核心思想" class="headerlink" title="3.1 核心思想"></a>3.1 核心思想</h3><p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725171708.png" alt="image.png|700"></p>
<ul>
<li><strong>似然 × 先验</strong>，在数学上就等于联合分布，即 <code>似然 × 先验 = 联合概率</code><br><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725172200.png" alt="image.png"></li>
<li><strong>先验 p(w)<strong>：是我们对参数的</strong>初始信念</strong>，来自于过去的知识和经验。</li>
<li><strong>似然 p(X∣w)<strong>：是新观测到的</strong>数据证据</strong>对我们各种信念的支持程度。</li>
<li><strong>似然 × 先验</strong>: 是一个<strong>权衡过程</strong>，它将我们<strong>固有的信念</strong>和<strong>新来的证据</strong>结合起来，得到一个更全面、更合理的判断。这个乘积的结果正比于我们最终想知道的**后验概率 p(w∣X)**（即在看到新证据后，我们对参数的更新认知）。</li>
</ul>
<p> 为了计算方便，我们通常对这个目标取对数（log），因为对数函数是单调递增的，最大化原函数等价于最大化其对数形式。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173257.png" alt="image.png"></p>
<p>在机器学习中，我们更习惯于将问题表述为<strong>最小化损失函数</strong>。最大化一个值等价于最小化它的相反数，所以上面的公式又等价于：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173309.png" alt="image.png"></p>
<ul>
<li><strong>第一项 <code>(-log p(X|w))</code><strong>：这就是我们通常所说的</strong>损失函数</strong>（Loss Function），比如交叉熵损失或均方误差损失。它衡量了模型在当前数据上的表现。</li>
<li><strong>第二项 <code>(-log p(w))</code><strong>：这就是由</strong>先验分布</strong>带来的<strong>正则化项</strong>（Regularization Term），又称为<strong>先验项</strong></li>
</ul>
<h3 id="3-2-高斯先验-→-L2-正则化"><a href="#3-2-高斯先验-→-L2-正则化" class="headerlink" title="3.2 高斯先验 → L2 正则化"></a>3.2 高斯先验 → L2 正则化</h3><p>假设先验分布为高斯分布：</p>
<p>我们假设模型参数 w 的先验分布是一个均值为 $0$，方差为 $σ^2$ 的高斯分布。这意味着我们相信参数 $w$ 的值很可能在 $0$ 附近，取很大的值或很小的值的概率较低。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174115.png" alt="image.png|563"></p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173627.png" alt="image.png"></p>
<p>推导正则化项 <code>-log p(w)</code>：</p>
<p>对 $p(w)$ <strong>取对数</strong>，并化简：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173709.png" alt="image.png"></p>
<p>再加一个<strong>负号</strong>：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173738.png" alt="image.png"></p>
<p>在最小化优化目标时，常数项 C 不影响最终解，可以被忽略。所以，最小化 <code>-log p(w)</code> 就等价于最小化：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173815.png" alt="image.png"></p>
<p>进一步代换：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725173849.png" alt="image.png"></p>
<p><strong>直观解释</strong>：假设参数服从高斯分布，就是假设参数值应尽可能靠近均值 0。<code>L2</code> 正则化惩罚的是参数的平方和，也能驱使参数值变小，从而防止过拟合。两者在目标上是一致的。</p>
<h3 id="3-3-拉普拉斯先验-→-L1-正则化"><a href="#3-3-拉普拉斯先验-→-L1-正则化" class="headerlink" title="3.3 拉普拉斯先验 → L1 正则化"></a>3.3 拉普拉斯先验 → L1 正则化</h3><p>我们假设模型参数 w 的先验分布是一个均值为 $0$，尺度参数为 $b$ 的拉普拉斯分布。它的形状在 $0$ 点处是一个尖峰，这意味着我们有更强的信念认为参数应该为 $0$。</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174323.png" alt="image.png|526"></p>
<p>其概率密度函数 (PDF) 为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174431.png" alt="image.png"></p>
<p>同样假设<strong>参数独立</strong>，则联合先验为：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174438.png" alt="image.png"></p>
<p>推导正则化项 <code>-log p(w)</code>：</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174453.png" alt="image.png"></p>
<p>与 L1 正则化建立联系</p>
<p><img src="https://picgocloud.oss-cn-shanghai.aliyuncs.com/picgo/20250725174525.png" alt="image.png"></p>
<p><strong>直观解释</strong>：拉普拉斯分布在 $0$ 点处是一个尖锐的峰，这代表了“参数应该为 $0$”的信念非常强。L1 正则化惩罚的是参数的绝对值之和，其数学性质会使得很多参数的最终解恰好变为 $0$，从而实现特征选择和稀疏化。这与拉普拉斯先验的假设也是一致的。</p>
<h3 id="3-4-关键点总结"><a href="#3-4-关键点总结" class="headerlink" title="3.4 关键点总结"></a>3.4 关键点总结</h3><ul>
<li><strong>引入先验分布</strong>: MAP 在 MLE 的基础上，额外引入了对参数 w 的**先验分布 p(w)**。这个先验代表了我们在看到数据之前对参数的已有认知或偏好。</li>
<li><strong>等价于正则化</strong>: 在优化目标中加入<strong>先验项</strong>，等价于在损失函数中加入了**正则化项 (Regularization)**。<ul>
<li><strong>高斯先验</strong> → <strong>惩罚二次项</strong> → <strong>L2 正则化</strong> (权重衰减，使参数平滑)</li>
<li><strong>拉普拉斯先验</strong> → <strong>惩罚绝对值</strong> → <strong>L1 正则化</strong> (使参数稀疏，很多参数变为 0)</li>
</ul>
</li>
<li><strong>泛化效果更好</strong>: 由于正则化的存在（即先验的约束），MAP 能够有效防止模型过拟合，通常比 MLE 有更好的<strong>泛化能力</strong>。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://VernalScenery.github.io">Scenery</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://vernalscenery.github.io/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/">https://vernalscenery.github.io/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://VernalScenery.github.io" target="_blank">春和景明的记事本</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></div><div class="post_share"><div class="social-share" data-image="/./img/1.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="10_机器学习_集成学习"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">10_机器学习_集成学习</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/24/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/08_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/" title="08_机器学习_支持向量机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">08_机器学习_支持向量机</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/07/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_KNN%E7%AE%97%E6%B3%95/" title="00_机器学习_KNN算法"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-04</div><div class="title">00_机器学习_KNN算法</div></div></a></div><div><a href="/2025/07/18/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/01_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%84%9F%E7%9F%A5%E6%9C%BA/" title="01_机器学习_感知机"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-18</div><div class="title">01_机器学习_感知机</div></div></a></div><div><a href="/2025/07/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/00_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E8%A7%88%E3%80%81%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/" title="00_机器学习_概览、环境搭建"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-03</div><div class="title">00_机器学习_概览、环境搭建</div></div></a></div><div><a href="/2025/07/21/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" title="02_机器学习_线性回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-21</div><div class="title">02_机器学习_线性回归</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/04_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/" title="04_机器学习_基础概念"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">04_机器学习_基础概念</div></div></a></div><div><a href="/2025/07/22/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/03_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/" title="03_机器学习_逻辑回归"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-22</div><div class="title">03_机器学习_逻辑回归</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/./img/1.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Scenery</div><div class="author-info__description">今天不想跑，所以才去跑</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">73</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">14</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chjm0121" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="/1595718686@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A6%82%E7%8E%87%E8%AE%BA%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">概率论的基本概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%A0%B7%E6%9C%AC%E7%A9%BA%E9%97%B4%E5%92%8C%E4%BA%8B%E4%BB%B6"><span class="toc-text">1 样本空间和事件</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A6%82%E7%8E%87%E7%9A%84%E6%80%A7%E8%B4%A8"><span class="toc-text">2 概率的性质</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A6%82%E7%8E%87%E7%9A%84%E5%85%AC%E7%90%86%E5%8C%96%E5%AE%9A%E4%B9%89"><span class="toc-text">2.1 概率的公理化定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E6%9C%89%E9%99%90%E5%8F%AF%E5%8A%A0%E6%80%A7"><span class="toc-text">2.2 有限可加性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E6%A6%82%E7%8E%87%E7%9A%84%E5%8D%95%E8%B0%83%E6%80%A7"><span class="toc-text">2.3 概率的单调性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E9%80%86%E4%BA%8B%E4%BB%B6%E7%9A%84%E6%A6%82%E7%8E%87"><span class="toc-text">2.4 逆事件的概率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E6%A6%82%E7%8E%87%E7%9A%84%E5%AE%B9%E6%96%A5%E5%AE%9A%E7%90%86"><span class="toc-text">2.5 概率的容斥定理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87"><span class="toc-text">3 条件概率</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E4%B8%A5%E6%A0%BC%E5%AE%9A%E4%B9%89"><span class="toc-text">3.1 严格定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E4%B9%98%E6%B3%95%E5%85%AC%E5%BC%8F"><span class="toc-text">3.2 乘法公式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%85%A8%E6%A6%82%E7%8E%87%E5%85%AC%E5%BC%8F"><span class="toc-text">3.3 全概率公式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="toc-text">4 贝叶斯公式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E4%B8%A5%E6%A0%BC%E5%AE%9A%E4%B9%89"><span class="toc-text">4.1 严格定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E4%B8%80%E4%B8%AA%E6%A1%88%E4%BE%8B"><span class="toc-text">4.2 一个案例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%BA%8B%E4%BB%B6%E7%9A%84%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="toc-text">5 事件的独立性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E7%8B%AC%E7%AB%8B%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-text">5.1 独立的定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E7%8B%AC%E7%AB%8B%E7%9A%84%E4%BB%B7%E5%80%BC"><span class="toc-text">5.2 独立的价值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%B8%89%E4%B8%AA%E4%BA%8B%E4%BB%B6%E7%9A%84%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="toc-text">5.3 三个事件的独立性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E5%A4%9A%E4%B8%AA%E4%BA%8B%E4%BB%B6%E7%9A%84%E7%8B%AC%E7%AB%8B%E6%80%A7"><span class="toc-text">5.4 多个事件的独立性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B"><span class="toc-text">5.5 条件独立</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-text">1 生成模型的工作原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%9A%E6%9D%A1%E4%BB%B6%E7%8B%AC%E7%AB%8B%E5%81%87%E8%AE%BE"><span class="toc-text">2 朴素贝叶斯：条件独立假设</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E9%97%AE%E9%A2%98%EF%BC%9A%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91"><span class="toc-text">3 朴素贝叶斯的问题：拉普拉斯平滑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%AE%B6%E6%97%8F"><span class="toc-text">4 朴素贝叶斯家族</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E9%AB%98%E6%96%AF%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Gaussian-NB"><span class="toc-text">4.1 高斯朴素贝叶斯 (Gaussian NB)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E5%A4%9A%E9%A1%B9%E5%BC%8F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Multinomial-NB"><span class="toc-text">4.2 多项式朴素贝叶斯 (Multinomial NB)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BC%AF%E5%8A%AA%E5%88%A9%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Bernoulli-NB"><span class="toc-text">4.3 伯努利朴素贝叶斯 (Bernoulli NB)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E5%88%86%E7%B1%BB%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Categorical-NB"><span class="toc-text">4.4 分类朴素贝叶斯 (Categorical NB)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E8%A1%A5%E9%9B%86%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF-Complement-NB"><span class="toc-text">4.5 补集朴素贝叶斯 (Complement NB)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%96%B9%E6%B3%95%E5%AE%B6%E6%97%8F"><span class="toc-text">5 贝叶斯方法家族</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E5%8F%8A%E9%80%82%E7%94%A8%E6%9D%A1%E4%BB%B6"><span class="toc-text">6 朴素贝叶斯的优缺点及适用条件</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1"><span class="toc-text">参数估计</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1%E7%9A%84%E4%B8%89%E5%A4%A7%E5%AE%B6%E6%97%8F"><span class="toc-text">1 参数估计的三大家族</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1-Maximum-Likelihood-Estimation-MLE"><span class="toc-text">2 最大似然估计 (Maximum Likelihood Estimation, MLE)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%9C%80%E5%A4%A7%E5%90%8E%E9%AA%8C%E4%BC%B0%E8%AE%A1-Maximum-A-Posteriori-Estimation-MAP"><span class="toc-text">3 最大后验估计 (Maximum A Posteriori Estimation, MAP)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">3.1 核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%AB%98%E6%96%AF%E5%85%88%E9%AA%8C-%E2%86%92-L2-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">3.2 高斯先验 → L2 正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%85%88%E9%AA%8C-%E2%86%92-L1-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-text">3.3 拉普拉斯先验 → L1 正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%85%B3%E9%94%AE%E7%82%B9%E6%80%BB%E7%BB%93"><span class="toc-text">3.4 关键点总结</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/29/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/13_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/" title="13_机器学习_概率图模型">13_机器学习_概率图模型</a><time datetime="2025-07-29T00:29:02.000Z" title="发表于 2025-07-29 08:29:02">2025-07-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/11_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E8%81%9A%E7%B1%BB/" title="11_机器学习_聚类">11_机器学习_聚类</a><time datetime="2025-07-28T00:19:53.000Z" title="发表于 2025-07-28 08:19:53">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/28/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/12_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%99%8D%E7%BB%B4/" title="12_机器学习_降维">12_机器学习_降维</a><time datetime="2025-07-27T20:48:42.000Z" title="发表于 2025-07-28 04:48:42">2025-07-28</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/26/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/10_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" title="10_机器学习_集成学习">10_机器学习_集成学习</a><time datetime="2025-07-26T01:58:53.000Z" title="发表于 2025-07-26 09:58:53">2025-07-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/07/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E5%9F%BA%E7%A1%80%E5%8E%9F%E7%90%86/09_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0_%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF/" title="09_机器学习_朴素贝叶斯">09_机器学习_朴素贝叶斯</a><time datetime="2025-07-25T03:08:47.000Z" title="发表于 2025-07-25 11:08:47">2025-07-25</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/./img/1.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By Scenery</div><div class="footer_custom_text"><div>波澜不惊</div><div class="footer-div"><img class="footer-icon" src="./img/备案图标.png"><a class="footer-a" target="_blank" rel="noopener" href="http://beian.miit.gov.cn/">皖ICP备2021016944号-1</a></div></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.13.0"></script><script src="/js/main.js?v=4.13.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.33/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@5.2.0/instantpage.min.js" type="module"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js?v=4.13.0"></script></div></div></body></html>